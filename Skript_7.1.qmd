---
title: "Einführung"
author: "Stephanie Geise"
toc: true
number-sections: true
highlight-style: pygments
format:
  html:
    code-fold: false
    code-line-numbers: true
editor_options: 
  chunk_output_type: inline
---

# Einführung in die Analyse von Zusammenhängen zwischen Variablen

In diesem Kapitel schauen wir uns verschiedene statistische Verfahren an, um Zusammenhänge zwischen Variablen zu untersuchen. Statistische Verfahren, die den Zusammenhang zwischen Variablen überprüfen, bieten ein großes Aufklärungspotential,  Muster, Beziehungen und Trends in Daten zu identifizieren und entsprechende Hypothesen zu testen. 

Es gibt verschiedene Arten von Zusammenhängen zwischen Variablen, und je nach Art des Zusammenhangs werden unterschiedliche statistische Verfahren eingesetzt. Die Wahl des jeweils angemessenen Verfahrens hängt von verschiedenen Faktoren ab, vor allem von
1) der Art der untersuchten Variablen und ihres Skalenniveaus (ob es sich also um nominale, ordinale oder metrische Variblen handelt)
2) der Art des (vermuteten) Zusammenhangs (z.B. linear, nicht-linear) sowie 
3) den spezifischen Fragestellungen ab, die wir untersuchen wollen. 
Manchmal kann es sich auch anbieten, verschiedene Verfahren zu kombinieren, um ein umfassendes Verständnis des Zusammenhangs zu erhalten.

Im Fokus dieses Kapitels stehen *lineare Zusammenhänge* zwischen Variablen verschiedenen Skalenniveaus. Ein linearer Zusammenhang liegt vor, wenn eine konstante Veränderung einer Variable mit einer konstanten Veränderung einer anderen Variable einhergeht. 

Ein Beispiel für einen linearen Zusammenhang im Bereich der Mediennutzung könnte der Zusammenhang zwischen der Anzahl der verbrachten Stunden vor dem Fernseher und der Anzahl der Fernsehwerbespots sein, die eine Person pro Tag sieht. Wenn wir annehmen, dass eine Person pro Stunde kontant 10 Werbestpots sieht, hätte sie nach 2 Stunden 20 Werbespots rezipiert, nach 3 Stunden 30 Spots usw. Einen solchen linearen Zusammenhang kann man auch grafisch darstellen: Wenn man Punkte auf einem Streudiagramm zeichnet, dann könnte man eine gerade Linie durch die Punkte legen, und diese Linie würde den Zusammenhang zwischen den Variablen am besten beschreiben.

GRAFIK LINEARER ZUSAMMENHANG

Im Werbespot-Beispiel vermuten wir einen Zusammenhang zwischen zwei kontinuierlichen bzw. metrischen Variablen. Lineare Zusammenhänge zwischen kontinuierlichen bzw. metrischen Variablen werden oft durch *Korrelationskoeffizienten* und *lineare Regression* untersucht.

Häufig gibt es aber auch den Fall, dass man den Zusammenhang zwischen zwei nominalskalierten oder ordinalskalierten Variablen Variablen untersuchen möchte. Zur Erinnerung: Nominalskalierte Variablen haben Werte, die Kategorien spiegeln, ohne dass diese eine natürliche Reihenfolge oder direkte Rangordnunghaben haben (z.B. präferierte Mediengattung, Geschlecht). Dagegen weisen die Werte bei ordinalskalierten Variablen eine hierarchische Ordnung oder Rangfolge zwischen den verschiedenen Kategorien auf, wobei die Abstände zwischen den Kategorien nicht fest definiert und auch nicht unbedingt gleichmäßig sind. Typische Beispiele für ordinalskalierte Variablen sind Noten, Bildungsabschlüsse oder Einkommensklassen.

Auf beide Variablentypen sind Methoden, die auf ordinalen oder metrischen Daten basieren, nicht anwendbar. Daher nutzen wir spezifische Tests für nominalskalierte Variablen, zum Beispiel den *Pearson Chi-Quadrat*-Test. Mit diesem Verfahren starten wir nun...

# Das Überprüfen von Zusammenhängen bei zwei nominalskalierten Variablen

Der *Pearson Chi-Quadrat Test* ist ein statistisches Verfahren, das verwendet wird, um zu überprüfen, ob es einen statistisch signifikanten Zusammenhang zwischen zwei *nominalskalierten* (kategorialen) Variablen gibt. Er testet die Nullhypothese, dass die beiden Variablen uabhängig voneinander sind. Beispiel: Wenn wir untersuchen möchten, ob es einen Zusammenhang zwischen Geschlecht und der Religionszugehörigkeit gibt, könnten wir einen Chi-Quadrat-Test durchführen.

Mit dem Chi-Quadrat-Test wird der Chi-Quadrat-Wert berechnet, indem die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten verglichen werden. In R kann der Pearson Chi-Quadrat Test mit der Funktion chisq.test() durchgeführt werden. Zuerst müssen wir dazu aber erst einmal die benötigen Pakete und Daten laden...

# Laden der Pakete und des Datensatzes
```{r}
install.packages("haven")
install.packages("tidyverse")
install.packages("vcd")
library(tidyverse)
library(haven)
library(vcd)
```

# Laden des Datensatzes
```{r}
daten = haven::read_dta("Datensatz/Allbus_2021.dta")
```

... und wir sollten die Hypothese, die den Zusammenhang zwischen den beiden Variablen annimmt, konkret formulieren: Es gibt einen Zusammenhang zwischen dem Geschlecht der Befragten (sex) und der Konfessionszugehörigkeit (rd01). Die Variable Konfessionszugehörigkeit ist im Allbus-Datensatz mit dem Namen "rd01" benannt; wie benennen sie in "Konfession" um; außerdem filtern wir die für uns relevanten Fälle der Variablen heraus (und ignorieren damit die irrelevanten, z.B. -9=Keine Angabe):

# Umbenennen der Variablen
```{r}
daten <- daten %>%
  rename(Konfession = rd01)%>%
  filter(between(Konfession, 1, 6))%>%
  filter(between(sex, 1, 3))
```

Nun erstellen wir eine Kreuztabelle (auch Kontingenztafel genannt), um die Häufigkeiten der verschiedenen Kombinationen von Kategorien beider Variablen anzuzeigen. Das gibt uns einen ersten Überblick darüber, wie oft bestimmte Kombinationen vorkommen.

# Erstellen und Ausgeben einer Kreuztabelle
```{r}
kreuztabelle <- table(daten$sex, daten$Konfession)
print(kreuztabelle)
```

[Hier können Sie auch noch einmal nachschauen, wie Kreuztabellen erstellt werden] (LINK)

In der Kreuztabelle können wir keine klare Tendenz erkennen, nach der das Geschlecht Einfluss auf die Konfession zu haben scheint: Frauen und Männer verteilen sich ungefähr gleich zwischen den verschiedenen Konfessionen; insgesamt scheinen allerdings etwas mehr Frauen als Männer überhaupt einer Konfession anzugehören.

Mit einem gestapelten Balkendiagramm können wir diese Tendenz auch visualisieren:

# Ein gestapeltes Balkendiagramm zur Visualisierung des Zusammenhangs erstellen
Ein gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) "aufeinandergestapelt", wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den "as.factor"-Befehl nutzen:

# Erstelle das gestapelte Balkendiagramm
```{r}
library(ggplot2)

daten$Konfession <- as.factor(daten$Konfession)
daten$sex <- as.factor(daten$sex)

ggplot(daten, aes(x = Konfession, fill = sex)) +
  geom_bar(position = "fill") +
  labs(title = "Verteilung der Konfessionszugehörigkeit nach Geschlecht",
       x = "Einkommensgruppe", y = "Anteil") +
  scale_fill_manual(values = c("blue", "red", "green")) # Farben anpassen

```
Diese Visualisierung zeigt sehr schön, was wir auch schon in der Kreuztabelle ablesen konnten: Es gibt keinen deutlich sichtbaren Zusammenhang zwischen der Konfessionszugehörigkeit und dem Geschlecht - allenfalls lässt sich vermuten, dass mehr Frauen als Männer überhaupt einer Konfession angehören. Ob sich der Zusammenhang als statistisch bedeutsam erweist, können wir nun mit dem Chi-Quadrat Test prüfen:

# Überprüfung des Zusammenhangs mit dem Chi-Quadrat Test
Der Chi-Quadrat Test ist ein statistisches Verfahren, das verwendet wird, um festzustellen, ob es einen signifikanten Unterschied zwischen den beobachteten und erwarteten Häufigkeiten in einer Kreuztabelle gibt. Da wir schon eine Kreuztabelle erstellt haben, können wir die Funktion chisq.test auf unsere Kreuztabelle anwenden - das geht ganz einfach:

```{r}
chisq.test(kreuztabelle)
```

Schauen wir uns nun den R-Output an: Der *Chi-Quadrat-Wert* (X² oder X-squared) gibt uns an, wie gut die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten übereinstimmen. Ein höherer X²-Wert deutet auf eine größere Abweichung hin.

*df* zeigt die Anzahl der Freiheitsgrade (Degrees of Freedom) des Chi-Quadrat-Tests an. Sie hängt von der Anzahl der Kategorien in den Variablen ab und beeinflusst die Verteilung des Chi-Quadrat-Werts.

Der *p-Wert* gibt die Wahrscheinlichkeit an, den beobachteten Chi-Quadrat-Wert zu erhalten, wenn die Nullhypothese wahr ist (d.h. wenn es keinen Zusammenhang zwischen den Variablen gibt). Der hier beobachtete p-Wert (von 0.01095) ist kleiner als .05 und damit signifikant. Das deutet darauf hin, dass der beobachtete Effekt überzufällig ist - Geschlecht und Konfessionszugehörigkeit hängen also statistisch zusammen.

Allerdings ist der Chi-Quadrat-Test empfindlich gegenüber der Stichprobengröße: Bei sehr großen Stichproben können auch kleine Unterschiede signifikant werden. In solchen Fällen ist es ratsam, neben dem p-Wert auch die Effektstärke zu betrachten. Dazu ermitteln wir nun noch Cramér's V. Das ist ein Maß für den Zusammenhang zwischen zwei kategorialen Variablen. Es reicht von 0 bis 1, wobei 0 keinen Zusammenhang und 1 einen vollständigen Zusammenhang anzeigt.

# Berechnung der Effektstärke Cramér's V
```{r}
library(vcd)

cramers_v <- CramerV(kreuztabelle)
print(cramers_v)
```
Zuerst extrahieren wir den beobachteten Chi-Quadrat-Wert und speichern ihn in einem Datenobjekt "chi_square" ab. Dann berechnen wir die Effektstärke nach der Formel für Cramér's V:

```{r}
# Schritt 1: Extrahieren des beobachteten Chi-Quadrat-Werts
chi_square <- chisq.test(kreuztabelle)$statistic

# Schritt 2: Berechnen der Effektstärke (Cramér's V)
n <- sum(kreuztabelle)  # Gesamtanzahl der Beobachtungen
k <- nrow(kreuztabelle)  # Anzahl der Kategorien in 'sex'
r <- ncol(kreuztabelle)  # Anzahl der Kategorien in 'Konfession'

V <- sqrt(chi_square / (n * min(k-1, r-1)))

# Schritt 3: Ausgabe der beobachteten Chi-Quadrat-Wert und Cramér's V 
cat("Beobachteter Chi-Quadrat-Wert:", chi_square, "\n")
cat("Cramér's V:", V, "\n")
```
Wie wir nun sehen, ist unser Wert für Cramér's V mit 0.047 sehr gering. Das deutet auf einen sehr schwachen Zusammenhang zwischen den betrachteten Variablen hin. Der Zusammenhang ist so schwach, dass er in der praktischen Anwendung wahrscheinlich vernachlässigbar ist. Das deckt sich mit unserer "visuellen Inspektion" der Daten.

Kommen wir nun zur nächsten Analyse-Option: der Analyse von Zusammenhängen zwischen zwei ordinalskalierten Variablen:

# Rangkorrelationen zur Überprüfung von Zusammenhängen zwischen zwei ordinalskalierten Variablen
Im Gegensatz zu nominalskalierten Variablen, bei denen es nur um die Kategorisierung oder Klassifikation von Daten geht (z.B. Geschlecht), haben ordinalskalierte Variablen eine klar definierte Rangordnung zwischen den Kategorien, jedoch nicht unbedingt einen gleichmäßigen Abstand zwischen den Kategorien. Um den Zusammenhang zwischen zwei ordinalskalierten Variablen zu berechnen, nutzt man 
*Rangkorrelationen*. Rangkorrelationen kommen auch dann zum Einsatz, wenn Variablen nicht normalverteilt sind. Sie geben an, ob es eine systematische Tendenz gibt, dass höhere Ränge in einer Variable mit höheren Rängen in der anderen Variable zusammenfallen. 

Rangkorrelationen sind statistische Maße, die verwendet werden, um den Zusammenhang zwischen zwei Variablen zu bewerten, wenn die Daten ordinal oder nicht normalverteilt sind. Zwei häufig verwendete Verfahren bzw. Arten der Rangkorrelation sind *Spearman's Rho* und *Kendall's Tau*. Grundsätzlich sind Spearman's Rho und Kendall's Tau zwei ähnliche Maße für Rangkorrelationen - wenn die Daten ordinal oder kategorial sind, sind sowohl Spearman's Rho als auch Kendall's Tau geeignete Maße. Allerdings weisen sie auch ein paar Unterschiede auf:

Bei *Spearman's Rho* werden die Daten in Ränge umgewandelt, und dann wird der Pearson-Korrelationskoeffizient auf den Rängen berechnet. Spearman's Rho ist besonders robust gegenüber Ausreißern (auch etwas mehr als Kendall's Tau). Spearman's Rho  berücksichtigt die genauen Abstände zwischen den Rängen und kann subtilere Beziehungen zwischen den Variablen erkennen.

Zur Ermittlung von *Kendall's Tau* wird der Grad der Übereinstimmung zwischen den Rängen der beiden Variablen berwertet. Kendall's Tau ist auch wenig anfällig für Ausreißer; das Verfahren ist besonders gut geeignet, wenn die relative Rangreihenfolge der Daten wichtiger ist als die genaue Distanz zwischen den Rängen. 

Beide Werte lassen sich leicht mit Hilfe der Funktion cor.test() berechnen. Denn diese Funktion ermöglicht es, verschiedene Arten von Korrelationskoeffizienten zu berechnen, einschließlich der Rangkorrelation nach Spearman's Rho und Kendall's Tau. Oft ist es ratsam, beide Maße zu berechnen und zu vergleichen, um sicherzustellen, dass die Ergebnisse konsistent sind.

# Die Analyse von Zusammenhängen zwischen zwei ordinalskalierten Variablen
Nun wollen wir das an einem Beispiel untersuchen: Wir testen, ob es einen Zusammenhang zwischen den ordinalskalierten Variablen Schulabsabschluss und der Einkommensgruppe gibt - bei beiden Variablen spiegelt der Wert eine zu Grunde liegende Rangordnung wider (z.B. Bildung: 1=ohne Abschluss; 3=Mittlere Reife; 5=Abitur; Einkommensgruppe: 1=bis unter 200 Euro; 2= 200 bis unter 300 Euro; 25=7.500 bis unter 10.000 Euro).

Um den Zusammenhang zu analysieren, müssen wir zuerst unsere Daten aufbereiten: 

Die Variable Einkommensgruppe ist im Datensatz mit dem Namen "di02a" benannt, die benenen wir in "Einkommensgruppe" um; außerdem filtern wir die für uns relevanten Fälle der Variablen (Einkommensgruppe 1 bis 25, ohne missings, z.B. -9=Keine Angabe; -8=Weiß nicht) heraus. Die Variable Bildungsabschluss ist im Datensatz mit dem Namen "educ" benannt, die benenen wir in "Bildung" um; auch hier filtern wir die für uns relevanten Fälle der Variablen (Bildung 1 bis 5, ohne missings, z.B. -9=Keine Angabe) heraus.  

# Umbenennen der Variablen
```{r}
daten <- daten %>%
  rename(Einkommensgruppe = di02a)%>%
  rename(Bildung = educ)%>%
  filter(between(Einkommensgruppe, 1, 25))%>%
  filter(between(Bildung, 1, 5))
```

Nun nutzen wir die Funktion cor.test() und spezifizieren bei "method" den Korrelationskoeefizienten, zuerst "spearman", dann zum Vergleich auch "kendall":

# Berechnung der Rangkorrelation nach Spearman's Rho und Ausgabe der Werte

```{r}
result_spearman <- cor.test(daten$Einkommensgruppe, daten$Bildung, method = "spearman")
print(result_spearman)
      
```

Der Wert *rho* gibt den berechneten Korrelationskoeffizienten wieder. Der Wert von 0,28 deutet darauf hin, dass es einen positiven Zusammenhang zwischen den beiden Variablen Bildungsabschluss und Einkommensgruppe gibt.

Der sehr kleine *p-Wert* (kleiner als 2.2e-16, was nahezu null ist), zeigt an, dass der beobachtete Zusammenhang zwischen den Variablen auch statistisch signifikant ist.

Der *S-Wert* (3725893693) repräsentiert die Summe der quadrierten Unterschiede zwischen den Rängen der beiden Variablen. Er ist Teil der Berechnung des Spearman's Rho und wird für die Interpretation eigentlich nicht unbedingt benötigt.

Schauen wir uns nun zum Vergleich das Ergebnis mit Kendall's Tau an:

# Berechnung der Rangkorrelation nach Kendall's Tau und Ausgabe der Werte

```{r}
result_kendall <- cor.test(daten$Einkommensgruppe, daten$Bildung, method = "kendall")
print(result_kendall)
```

Schauen wir uns die Ausgabe an: Der z-Wert gibt an, wie viele Standardabweichungen die beobachtete Korrelation von der erwarteten Korrelation entfernt ist. Ein höherer z-Wert deutet darauf hin, dass die beobachtete Korrelation signifikant von Null abweicht. In unserem Fall ist der z-Wert sehr hoch (16.343), was darauf hindeutet, dass die beobachtete Korrelation sehr weit von Null entfernt ist.

Der p-Wert ist die Wahrscheinlichkeit mit der die beobachtete Korrelation zufällig ist. Ein kleiner p-Wert deutet darauf hin, dass die beobachtete Korrelation sehr unwahrscheinlich ist, wenn kein Zusammenhang bestehen würde. Da unser p-Wert extrem klein ist (< 2.2e-16), können wir von einem Zusammenhang ausgehen. Beide Rangkorrelationskoeffizienten liefern uns also ein ähnliches Ergebnis und zeigen einen (positiven) Zusammenhang zwischen dem Bildungsabschluss und der Einkommensgruppe.


# Das Überprüfen von Zusammenhängen zwischen einer kategorialen und einer ordinalen Variable mittels Rangkorrelation nach Spearman's Rho
Häufig kommt es vor, dass wir einen Zusammenhang zwischen Variablen untersuchen möchten, die ein unterschiedliches Skalenniveau aufweisen, etwa wenn der Zusammenhang zwischen Geschlecht (als kategoriale Variable) und der Einkommensgruppe (als ordinale Variable) untersucht werden soll. Dann bietet sich die Anwendung der Rangkorrelation nach *Spearman's Rho* an, denn dieser Korrelationskoeffizient wurde (u.a.) speziell für die Analyse des Zusammenhang zwischen ordinalen und nicht-stetigen Variablen entwickelt.

Betrachten wir dies an einem Beispiel. Wir haben folgende Zusammenhangshypothese aufgestellt: Es gibt einen Zusammenhang zwischen dem Geschlecht der Befragten (sex) und der Einkommensgruppe (di02a = Haushaltsnetto-Einkommen ordinalskaliert abgefragt). 

Um diese Hypothese zu testen, müssen wir zunächst unsere Daten vorbereiten: Die Variable Einkommensgruppe ist im Datensatz mit dem Namen "di02a" benannt, die benennen wir in "Einkommensgruppe" um. Dann filtern wir die für uns relevanten Fälle der Variablen heraus (Einkommensgruppe 1 bis 25, ohne missings, z.B. -9=Keine Angabe; -8=Weiß nicht). Die Variable Geschlecht stellt einen Faktor dar - um keine Fehlermeldung zu bekommen, müssen wir sie dem as.numeric-Befehl als numerische Variable behandeln. Auch bei Geschlecht selektieren wir die Missungs aus (z.B. -9=Keine Angabe):

# Umbenennen und Filtern der Variablen
```{r}
daten_neu <- daten %>%
  filter(between(Einkommensgruppe, 1, 25))%>%
  mutate(sex = as.numeric(sex)) %>%
  filter(between(sex, 1, 3))
```

Wieder können wir nun die Funktion cor.test() nutzen, wenn wir bei "method" den Korrelationskoeefizienten "spearman" spezifizieren.

# Berechnung der Rangkorrelation 
```{r}
result_spearman <- cor.test(daten_neu$sex, daten_neu$Einkommensgruppe, method = "spearman")
print(result_spearman)
      
```

Schauen wir uns das Ergebnis an: Zunächst sehen wir den geschätzten Spearman's Rangkorrelationskoeffizient -0.2975236. Gleichzeitig sehen wir einen sehr kleinen p-Wert (kleiner als 2.2e-16) - das deutet darauf hin, dass der beobachtete Rangkorrelationskoeffizient höchst signifikant ist.

Wir können daher davon ausgehen, dass es einen signifikanten negativen Rangkorrelationszusammenhang zwischen den Variablen "sex" und "Einkommensgruppe" gibt. Der geschätzte Korrelationskoeffizient von -0.2975236 deutet darauf hin, dass höhere Werte in der "Einkommensgruppe" tendenziell mit niedrigeren Werten in der Variable "sex" korrelieren, und umgekehrt. Da "männlich" im Datensatz mit 1 codiert ist (2=weiblih, 3=divers), können wir daraus schließen, dass Männer eher in höheren Einkommensgruppen zu finden sind als Frauen oder diverse Personen.

Mit einem gestapelten Balkendiagramm können wir diese Tendenz auch visualisieren:

# Ein gestapeltes Balkendiagramm zur Visualisierung des Zusammenhangs erstellen
Ein gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) "aufeinandergestapelt", wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den "as.factor"-Befehl nutzen:

# Erstelle das gestapelte Balkendiagramm
```{r}
library(ggplot2)

daten_neu$Einkommensgruppe <- as.factor(daten_neu$Einkommensgruppe)
daten_neu$sex <- as.factor(daten$sex)

ggplot(daten_neu, aes(x = Einkommensgruppe, fill = sex)) +
  geom_bar(position = "fill") +
  labs(title = "Verteilung der Einkommensgruppe nach Geschlecht",
       x = "Einkommensgruppe", y = "Anteil") +
  scale_fill_manual(values = c("blue", "red", "green")) # Farben anpassen
```
Diese Visualisierung zeigt sehr schön, was wir auch schon anhand der Spearman's Rank Correlation ablesen konnten: Es sind mehr Frauen als Männer in den unteren Einkommensgruppen zu finden; es sind mehr Männer als Frauen in höheren Einkommensgruppen zu finden.


# Das Überprüfen von Zusammenhängen bei zwei (intervallskalierten) Variablen

## Korrelation

## Korrelationskoeffizienten: 
Der Pearson-Korrelationskoeffizient misst die Stärke und Richtung des linearen Zusammenhangs zwischen zwei kontinuierlichen Variablen. Er liegt zwischen -1 und 1, wobei -1 einen perfekten negativen linearen Zusammenhang, 1 einen perfekten positiven linearen Zusammenhang und 0 keinen linearen Zusammenhang anzeigt. Der Pearson-Korrelationskoeffizient wird verwendet, wenn man den linearen Zusammenhang zwischen zwei metrischen Variablen untersuchen möchte.


