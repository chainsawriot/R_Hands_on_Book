[
  {
    "objectID": "Skript_7.3.html",
    "href": "Skript_7.3.html",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "",
    "text": "Das Testen der Maschine, Bild generiert von Midjourney\nIn diesem Teilkapitel gehen wir (wie angekündigt) näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Im folgenden Teilkapitel lernen wir dann die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren."
  },
  {
    "objectID": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Notebook gehen wir (wie angekündigt) zunächst näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Dann lernen wir die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren.\n\n1.1.1 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ESS8_vier_laender-Datensatzes, den wir entsprechend einlesen:\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\")\n#install.packages(\"lmtest\")\n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n#install.packages(\"sandwich\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\nlibrary(lmtest)\n\nWarning: Paket 'lmtest' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: zoo\n\n\nWarning: Paket 'zoo' wurde unter R Version 4.3.1 erstellt\n\n\n\nAttache Paket: 'zoo'\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\nWarning: Paket 'sandwich' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n1.1.2 Data Management\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die Internetnutzung (netustm); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Rezeptionszeit von politischen Nachrichten (nwspol) sowie das Geschlecht der Befragten (gndr) an. Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl um.\nDann setzen wir den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifizieren wir wieder, auf welche Variablen sich der Befehl beziehen soll). Das modifizierte Datenset weisen wir einem neuen Datenobjekt zu: daten_mod2\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(internetnutzung = netustm,\n         alter = agea,\n         politische_Nachrichtenrezeption = nwspol,\n         gender = gndr) %&gt;% \n  drop_na(c(internetnutzung, alter, politische_Nachrichtenrezeption, gender)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 158 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\n\n1.1.3 Erinnerung: Einfache lineare Regression mit Alter als UV und Internetnutzung als AV mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nsummary(model) # klassischer Output mit relevanten Kennzahlen\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-211.39  -97.38  -44.70   53.71  487.73 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 279.2690    45.2122   6.177 0.0000000149 ***\nalter        -1.9585     0.9218  -2.125       0.0361 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 150 on 98 degrees of freedom\nMultiple R-squared:  0.04404,   Adjusted R-squared:  0.03428 \nF-statistic: 4.514 on 1 and 98 DF,  p-value: 0.03613\n\n#summary(lm.beta(model)) # klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Internetnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um “den Estimate-Wert” in Messeinheiten (hier: -4.296 Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit dem Teil A dieses Skripts, nämlich der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern Sie sich, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Sie haben Recht: Eigentlich würden wir erst die Voraussetzungen prüfen, dann das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen.\n\n\n1.1.4 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)\n\n\n1.1.5 TEIL A: Prüfung der Voraussetzungen einer Regressionsanalyse\n\n1.1.5.1 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt.\n\n\n1.1.5.2 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß – unabhängig wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde.\n\ncheck_heteroscedasticity(model)\n\nOK: Error variance appears to be homoscedastic (p = 0.329).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). In diesem Fall liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\n\n\n1.1.5.3 Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei höheren Werten erkennbar ist, weil wir einen leicht nach rechts geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\n\n\n1.1.5.4 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 279.2690    44.7423  6.2417 0.00000001109 ***\nalter        -1.9585     0.8609 -2.2749       0.02509 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!\n\n\n1.1.5.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.294).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0,588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!\n\n\n1.1.5.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.\n\n\n1.1.5.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, kann ich wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n1.1.5.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr coole Funktion, die mir eine visuelle Inspektion meiner Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion kann ich mir dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\n\ncheck_model(model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n\n1.1.6 TEIL B: Die multiple lineare Regression\n\n1.1.6.1 Anwendungsbereich der multiplen linearen Regression\nDie multiple lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen metrischen Variablen besteht. Die multiple lineare Regressionsanalyse hat das Ziel, eine abhängige Variable (y) mittels mehrerer unabhängigen Variablen (x1, x2, …) zu erklären. (Zur Erinnerung: Für nur eine x-Variable nutzen wir die einfache lineare Regression)\nMit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen den unabhängigen und der einen abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variablen vorhergesagt werden?\nDie multiple Regression entspricht in ihrer Analyslogik also der einfachen linearen Regression - nur dass sie mehr als eine unabhängige Variable berücksichtigt.\n\n\n1.1.6.2 Ziel der Analyse\nMit Hilfe der multiplen Regression wollen wir die Annahme prüfen, dass die Variablen Alter (agea) sowie die Rezeptionszeit von politischen Nachrichten (nwspol) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) haben bzw. diese erklären und vorhersagen können. Alle Variablen sind metrisch und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n1.1.6.3 Modell zum Zusammenhang von Alter, politischer Nachrichtenrezeption und Internetnutzung spezifizieren und anzeigen lassen\nDie Berechnung der multiplen Regression unterscheidet sich nicht stark von der Berechnung der einfachen linearen Regression. In die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) einfach die zusätzliche unabhängige Variable (politische_Nachrichtenrezeption) ein, indem wir sie mit einem + Zeichen anhängen:\n\nmodel_m &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption, data = daten_mod)\nsummary(lm.beta(model_m))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption, \n    data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-203.71  -92.77  -47.73   38.81  457.27 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     261.8715           NA    44.9628   5.824\nalter                            -2.0528      -0.2200     0.9041  -2.271\npolitische_Nachrichtenrezeption   0.2785       0.2186     0.1234   2.256\n                                    Pr(&gt;|t|)    \n(Intercept)                     0.0000000745 ***\nalter                                 0.0254 *  \npolitische_Nachrichtenrezeption       0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147 on 97 degrees of freedom\nMultiple R-squared:  0.09171,   Adjusted R-squared:  0.07299 \nF-statistic: 4.897 on 2 and 97 DF,  p-value: 0.009415\n\n\n\n\n1.1.6.4 Interpretation des Outputs: Was sehen wir in der Regressionstabelle?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängigen Variablen “alter” und “politische_Nachrichtenrezeption” zu erklären.\n\n\n1.1.6.5 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n1.1.6.6 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n1.1.6.7 Standardized\nDiese Estimates sind die standardisierte b-Werte. Weil wir diese über die lm.beta-Funktion standardisiert haben, lassen sich die Koeefizienten auch bei unterschiedlicher Skalierung vergleichen.\n\n\n1.1.6.8 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n1.1.6.9 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n1.1.6.10 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n1.1.6.11 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter und politische Nachrichtenrezeption auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter und die politische Nachrichtenrezeption etwa 20 Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n1.1.6.12 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n1.1.6.13 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n1.1.6.14 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (-4.9169) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &lt; .05 statistisch signifikant. Diese Ergebnisse überraschen uns nicht: Den Einfluss des Alters haben wir ja letzte Woche schon überprüft. Mit der Erweiterung zur multiplen Regression können wir nun zusätzlich sagen, dass die politische Nachrichtenrezeption auch einen Einfluss auf die Internet-Nutzung hat, denn der Wert ist ebenfalls signifikant (p &lt; .05)! Die F-Statistik sagt uns zusätzlich, dass auch unser Gesamtmodell signifikant ist (p-value: 0.00001239, also &lt; .05).\n\n\n1.1.6.15 Erinnerung: Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model_m))\n\n# A tibble: 3 × 6\n  term                         estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;                           &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)                   262.          NA        45.0        5.82 7.45e-8\n2 alter                          -2.05        -0.220     0.904     -2.27 2.54e-2\n3 politische_Nachrichtenrezep…    0.278        0.219     0.123      2.26 2.63e-2\n\n\n\nglance(model_m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0917        0.0730  147.      4.90 0.00941     2  -639. 1287. 1297.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "href": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden",
    "text": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.3 Prüfung der Voraussetzungen",
    "text": "1.3 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "href": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.4 Berechnung und Interpretation einer multiplen Regression",
    "text": "1.4 Berechnung und Interpretation einer multiplen Regression"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Skript_1.1.html",
    "href": "Skript_1.1.html",
    "title": "Einführung in R und RStudio",
    "section": "",
    "text": "Artwork by @allison_horst\nIn diesem Kurs nutzen wir die Programmiersprache R sowie R-Studio als benutzerfreundliche Programmoberfläche. In diesem Kapitel gehen wir die wichtigsten Punkte Schritt für Schritt durch."
  },
  {
    "objectID": "Skript_1.2.html",
    "href": "Skript_1.2.html",
    "title": "R-Studio",
    "section": "",
    "text": "1 Wie funktioniert R-Studio?"
  },
  {
    "objectID": "Skript_1.3.html",
    "href": "Skript_1.3.html",
    "title": "Die Logik von Markdown und Quarto",
    "section": "",
    "text": "Artwork by @allison_horst\nWir können in R-Studio unterschiedliche Arten von Dokumenten nutzen. In diesem Kurs nutzen wir hauptsächlich R Quarto. Im folgenden Kapitel gehen wir auf die wichtigsten Funktionsweisen von Quarto ein."
  },
  {
    "objectID": "Skript_2.1.html",
    "href": "Skript_2.1.html",
    "title": "Der Aufbau von Datensätzen",
    "section": "",
    "text": "Wie soll ich mich hier nur zurecht finden?, Bild generiert von Midjourney\n\n\nSozialwissenschaftlichen Daten sind Informationen, die zur Beschreibung und Erklärung der sozialen Welt verwendet werden. Sie können aus einer Vielzahl von Quellen stammen, zum Beispiel aus Umfragen, Experimenten, Interviews und Inhaltsanalysen. Kommunikationswissenschaftliche Daten können quantitativ oder qualitativ sein. Quantitative Daten sind in Zahlen ausgedrückt und können verwendet werden, um Muster und (kausale) Zusammenhänge zu identifizieren. Qualitative Daten sind in Text oder (Bewegt)Bildern ausgedrückt und können verwendet werden, um die Bedeutung von Ereignissen und Erfahrungen genauer zu verstehen.\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n1 Strukturierte vs. unstrukturierte Daten\nUnter strukturiertern Datensätzen verstehen wir digital zusammengestellte Daten, die bestimmte strukturierende Merkmale aufweise. Häufig lassen sich solche strukturierten Daten als Tabellen darstellen und speichern. Ein konkretes Beispiel wäre eine Sammlung von Zeitungsartikeln. Handelt es sich um einzelne Textdatein, zu deren Inhalt man keine weiteren Informationen besitzt, würde man in der Regel von unstruktukrierten Daten sprechen. Fügt man aber Titel, Publikationsdatum und Quelle hinzu, könnten wir diese Daten als Tabelle abspeichern und von einem strukturierten Datensatz sprechen.\n\n\n2 Data Frames und strukturierte Daten\nNahezu alle strukturierte Daten lassen sich in R als Data Frames speichern. Dies bietet zahlreiche Vorteile, weil sich Data Frames leicht umformen, refaktorieren oder aggregieren lassen. So kann man leicht auszählen, wie viele Artikel zur gleichen Quelle gehören, zu welchem Zeitpunkt wie viele Artikel veröffentlicht wurden, oder (bei einer Befragung) wie viele Stunden die StudienteilnehmerInnen im Mittel wöchentlich fernsehen. Häufig lassen sich zunächst unstrukturierte Daten in ein strukturiertes Format überführen, wenn man relevante Metadaten nutzt, beispielsweise Dateinahmen.\n\n\n3 Beobachtungen und Variablen\nDer Aufbau von Tabellen (bzw. Data Frames) folgt dabei in der Regel einer logischen Struktur. Die Zeilen geben die Anazhl der Beobachtungen (oder Fälle) an, während die Spalten einzelne Variablen (oder Merkmale) repräsentieren.\n\n\n\n\n\n\n\n\nEinheit\nBeobachtung (oder Fall)\nVariable (oder Merkmal)\n\n\n\n\nTabellen-Strukturmerkmal\nZeile\nSpalte\n\n\nBeispiele\n\nRespondent in einer Befragung\nZeitungsartikel in einer Inhaltsanalyse\nVersuchsdurchlauf in einem Experiment\n\n\nAntwort auf eine Frage in einem Fragebogen\nQuelle oder Titel eines Zeitungsartikels in einer Inhaltsanalyse\nMesswert in einem Experiment\n\n\n\n\n\n\n4 Zusammenfassung\nIm nächsten Kapitel werden wir uns eingehender mit der Behandlung von tabellenstrukturierten Daten in unterschiedlichen Formaten beschäftigen."
  },
  {
    "objectID": "Skript_2.2.html",
    "href": "Skript_2.2.html",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "Wie bekomme ich meine Daten?, Bild generiert von Midjourney\n\n\nUm in R mit größeren Datensätzen arbeiten zu können, müssen diese zunächst eingelesen werden. Um Daten in R einzulesen (d.h. um sie in ein R-Objekt im Arbeitsspeicher ihres Rechners zu überführen), gibt es verschiedene Möglichkeiten. Die gebräuchlichste Methode ist die Verwendung von Funktionen. Zum Glück bietet R eine Reihe nützlicher Funktionen für ganz unterschiedliche Dateiformate, z. B. CSV-, Excel-, SPSS- und STATA-Dateien.\n\n1 Data Management\nZunächst laden wir die Pakete, die für den Import von Dateien in den o.g. Formaten nützlich sind.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(readr, readxl, haven)\n\noptions(scipen = 999) \n\n\n\n2 CSV-Dateien\nCSV-Dateien sind eine sehr einfache Art von Daten, die in R eingelesen werden können. Sie sind als Textdateien gespeichert, wobei die Tabellenspalten durch Kommas (oder manchmal Semikolons oder Tab-Zeichen) getrennt sind. Um eine CSV-Datei in R einzulesen, können Sie die Funktion read_csv (bzw. hier read_csv2) aus dem Paket readr verwenden.\nDieser Code importiert die Datei geschlechterverteilung.csv in das R-Environment. Die Datei geschlechterverteilung.csv muss im gleichen Verzeichnis wie der R-Code gespeichert sein.\n\ngeschlechterverteilung_csv &lt;- read_csv2(\"geschlechterverteilung.csv\")\n\nDie Funktion read_csv2 liefert hier eine Menge zusätzlicher Informationen, etwa dazu, welche Datentypen für die eingelesenen Variablen gewählt wurden. Es wurde hier deshalb nicht read_csv sondern read_csv2 gewählt, weil letztere das Semikolon (;) als Trennzeichen und das Komma (,) als Dezimalzeichen verwendet. Hingegen geht read_csv vom Komma als Trennzeichen und dem Punkt als Dezimalzeichen aus, wie im angelsächsischem Gebrauch üblich. Liest man also mit der falschen Funktion ein, sind die Daten unter Umständen fehlerhaft.\n\n\n3 Excel-Dateien\nEbenfalls nützlich ist die Möglichkeit, Daten aus Microsoft Excel in R zu imoportiern. Excel-Dateien können mit der Funktion readxl::read_excel() in R eingelesen werden. Diese Funktion unterstützt alle gängigen Excel-Formate, einschließlich XLSX, XLS und CSV.\n\ngeschlechterverteilung_excel &lt;- read_excel(\"geschlechterverteilung.xlsx\")\n\n\n\n4 SPSS-Dateien\nSPSS ist eine statistische Softwareanwendung, die von er Firma IBM (früher SPSS Inc) für Datenmanagement, inferenzstatistische Analysen und (kommerziell) Business Intelligence entwickelt wurde.SPSS-Dateien können mit der Funktion haven::read.spss() in R eingelesen werden. Diese Funktion unterstützt alle gängigen SPSS-Formate, einschließlich SAV und DSP.\nDa der ALLBUS im konkreten Fall im Stata-Format verwendet wird, verwenden wir für SPSS ein anderes Beispiel, nämlich die dritte Welle (in 2016 erhoben) des Global Report on Adult Learning and Education (GALE-3) der UNESCO.\n\ngale3_spss &lt;- read_spss(\"https://uil.unesco.org/i/doc/adult-education/grale-3/survey-data/grale-3-suquant.sav\")\n\nDas Beispiel zeigts dass wir neben lokal abgespeicherten Datein auch problemlos Web-Adressen als Pfadangabe beim Import von Daten verwenden können, wenn diese direkt auf die relevanten Daten zeigen.\n\n\n5 Stata-Dateien\nBei STATA handelt es sich um eine weitere kommerzielle Statistik-Software. Mittels Stata analysiert man einfache und komplexe Datenmodelle. Die Software gehört neben SPSS zu den bekanntesten Programmen für die professionelle Datenauswertung. STATA-Dateien können mit der Funktion haven::read_dta() in R eingelesen werden. Diese Funktion unterstützt alle gängigen STATA-Formate, einschließlich DTA und DTB.\nWir verwnden als Beispiel für den Import von STATA-Daten hier den ALLBUS, da dieser im STATA-Format vorliegt.\n\nallbus_stata &lt;- read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n\n6 Andere (textbasierte) Formate\nFür andere Datenformate gibt es in der Regel spezielle Funktionen, die in den entsprechenden Paketen enthalten sind. Zum Beispiel kann die Funktion readr::read_tsv() für das Einlesen von TSV-Dateien (die mit einem Tabulator- oder TAB-Zeichen getrennt sind) verwendet werden. Die meisten Funktionen zum Einlesen von (Text)Daten haben eine Reihe von Optionen, mit denen Sie den Importprozess anpassen können. Zu den häufigsten Optionen gehören:\n\n\n\n\n\n\n\nFunktionsargument\nBeschreibung\n\n\n\n\nheader\nGibt an, ob die Datendatei eine Kopfzeile enthält\n\n\nsep\nGibt das Trennzeichen zwischen den Werten an\n\n\nna\nGibt an, wie fehlende Werte codiert werden sollen\n\n\ndec\nGibt das Dezimaltrennzeichen an\n\n\n\n\n\n7 Zusammenfassung\nEs gibt verschiedene Möglichkeiten, Daten in R einzulesen. Die gebräuchlichste Methode ist die Verwendung von Funktionen. Für jedes gängige Datenformat gibt es eine entsprechende Funktion."
  },
  {
    "objectID": "Skript_2.3.html",
    "href": "Skript_2.3.html",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "Wir haben unsere Daten, Bild generiert von Midjourney\n\n\nDie Allgemeine Bevölkerungsumfrage der Sozialwissenschaften (ALLBUS) ist eine standardisierte Befragung der deutschen Bevölkerung, die seit 1980 regelmäßig durch das GESIS Leibniz Institut für Sozialforschung durchgeführt wird. Im ALLBUS werden in der Regel alle zwei Jahre Daten über Einstellungen, Verhaltensweisen und Sozialstruktur der Bevölkerung in der Bundesrepublik Deutschland gesammelt. Dafür wird in persönlichen Interviews jeweils eine repräsentative Stichprobe aus der Bevölkerung Deutschlands befragt (jeweils ca. 2.800 bis 3.500 Befragte).\nAbgefragt werden u.a. Einschätzungen und Einstellungen in den Bereichen:\n- Wirtschaft\n- Umwelt\n- Immigration\n- Politische Einstellungen und Partizipation\n- Mediennutzung\n- Einstellungen zu Ehe, Familie und Partnerschaft\n- Einstellungen zu und Kontakte mit Behörden\n- Freizeitaktivitäten\n- Gesundheit und gesundheitsrelevantes Verhalten\nFolgend arbeiten wir im Rahmen dieses Moduls durchgängig mit dem ALLBUS, speziell mit der Erhebnungswelle des Jahres 2021.\n\n1 Data Management & Einlesen des ALLBUS\nWir beginnen damit, die notwendigen Pakete zu laden, die wir für die ersten Schritte mit den Daten benötigen. Das sind hier die Pakete haven (für das Einlesen der ALLBUS-Daten im Stata-Format) und das Pakete readr (für das Einlesen einiger Vorab vorbereiteter Samples aus dem Gesamtdatensatz), sowie das Paket dplyr, mit dem wir am Schluss einen Beispielhaften Teildatensatz bilden.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(haven, readr, dplyr)\n\noptions(scipen = 999) \n\nNun lesen wir den ALLBUS-Datensatz mittels der Stata-Importfunktion read_dta aus dem Paket haven ein.\n\ndaten &lt;- read_dta(\"Datensatz/Allbus_2021.dta\")\n\nAls nächstes laden wir zudem noch drei zuvor erstellte Zufallssamples im Umfang von 20, 200 und 500 Zeilen aus dem Gesamtdatensatz. Diese enthalten weiterhin eine deutlich kleinere Anzahl relevanter Variablen und sind daher etwas übersichtlicher als der sehr große Hauptdatensatz.\n\nsample_klein &lt;- read_rds(\"Datensatz/ALLBUS_sample_klein.rds\")\nsample_mittel &lt;- read_rds(\"Datensatz/ALLBUS_sample_mittel.rds\")\nsample_gross &lt;- read_rds(\"Datensatz/ALLBUS_sample_gross.rds\")\n\n\n\n2 Erste Schritte mit dem ALLBUS\nZunächst schauen wir uns die Daten an. Dies geschieht entweder daturch, dass man den Objektnamen verwendet (also im folgende Beispiel einfach sample_klein) oder indem man mit einem Klick in RStudio unter dem Reiter Environment oder mit dem Befehl View() aufruft. Bei diesem Vorgehen zeigt RStudio die Daten an, was i.d.R. den praktischsten Zugang darstellt.\n\nsample_klein\n\n# A tibble: 20 × 4\n   alter geschlecht bildung            fernsehkonsum\n   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;                      &lt;dbl&gt;\n 1    37 MANN       FACHHOCHSCHULREIFE             3\n 2    38 MANN       MITTLERE REIFE                 7\n 3    47 FRAU       VOLKS-,HAUPTSCHULE             6\n 4    66 MANN       HOCHSCHULREIFE                 2\n 5    47 FRAU       HOCHSCHULREIFE                 7\n 6    75 MANN       VOLKS-,HAUPTSCHULE             7\n 7    41 FRAU       MITTLERE REIFE                 4\n 8    18 MANN       NOCH SCHUELER                  7\n 9    91 MANN       &lt;NA&gt;                          NA\n10    56 MANN       HOCHSCHULREIFE                 4\n11    58 MANN       HOCHSCHULREIFE                 7\n12    32 MANN       MITTLERE REIFE                 2\n13    47 MANN       FACHHOCHSCHULREIFE             1\n14    49 MANN       MITTLERE REIFE                 1\n15    23 MANN       MITTLERE REIFE                 0\n16    48 FRAU       MITTLERE REIFE                 0\n17    49 MANN       HOCHSCHULREIFE                 0\n18    36 MANN       FACHHOCHSCHULREIFE             7\n19    70 MANN       HOCHSCHULREIFE                 7\n20    43 FRAU       MITTLERE REIFE                 7\n\n\nDer kleine Beispieldatensatz illustriert den grundlegeenden Aufbau des ALLBUS. Dieser folgt (beim ALLBUS, aber auch den meisten anderen Befragungen) den folgenden Prinzipien\n\njede Befragungswelle ist ein einzeles Data Frame-Objekt (= eine große Tabelle)\ndie Zeilen der Tabelle sind die Beobachtungen (= RespondentInnen)\ndie Spalten der Tabelle sind die Variablen (= Antworten auf Survey-Fragen, oder bei der Möglichkeit zur Mehrfachnennung, die einzelnen Antwortoptionen)\ndie Zelleninhalte sind i.d.R. (Dummy)Zahlenwerte (= etwa “1” für wenig Zustimmung und “5” für hohe Zustimmung)\n\n\n\n3 Einen Überblick über den ALLBUS gewinnen\nIm Beispieldatensatz sind die Werte der Variablen alter, geschlecht und bildung recht leicht nachvollziebar, wobei sie etwas unterschiedliche Datentypen aufweisen, wie man mit Hilfe der Funktion str ermitteln kann.\n\nstr(sample_klein)\n\ntibble [20 × 4] (S3: tbl_df/tbl/data.frame)\n $ alter        : num [1:20] 37 38 47 66 47 75 41 18 91 56 ...\n $ geschlecht   : Factor w/ 4 levels \"KEINE ANGABE\",..: 2 2 3 2 3 2 3 2 2 2 ...\n $ bildung      : Factor w/ 9 levels \"NICHT BESTIMMBAR\",..: 6 5 4 7 7 4 5 9 NA 7 ...\n $ fernsehkonsum: num [1:20] 3 7 6 2 7 7 4 7 NA 4 ...\n\n\nBei der Variable alter handelt es sich schlicht um eine Zahl (num), während geschlecht und bildung sogenannte Faktoren sind. Faktoren nutzt man in R, um wiederholende nicht numerische (typischerweise nominal oder ordinalskalierte) Werte zu speichern. Praktisch jeder Faktor könnte genausogut eine Zeichenkette (chr) sein, aber oftmals sind Faktoren praktischer, weil sie eine festen Reihenfolge haben können (“ranked factors”), die sich bei Bedarf auch in Zahlen umwandel lassen. Im konkreten Beispiel ist das Geschlecht ein ungerankter Faktor, der Bildungsgrad hat hingegen einen Rang. Der Fernsehkonsum ist schließlich eine Likert-skalierte Variable, die wir hier und auch an anderer Stelle als metrische Variable behandeln (und dafür den R-Datentypen numeric verwenden), auch wenn das strikt genommen nicht immer zulässig ist – zumindest dann nicht, wenn man nur auf Grundlage eines einzelnen Items misst und eine 5- oder 7-Punkt Skala verwendet (vgl etwa hier).\nWie sehen die anderen Samples aus? Wir sehen uns im nächsten Schritt das große Zufallssample (500 Fälle) an.\n\nsample_gross\n\n# A tibble: 500 × 29\n   alter geschlecht fernsehkonsum politisches_interesse links_rechts_einordnung\n   &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt; &lt;fct&gt;                                   &lt;dbl&gt;\n 1    50 FRAU                   7 MITTEL                                      5\n 2    77 MANN                   7 STARK                                       3\n 3    64 FRAU                   7 &lt;NA&gt;                                        5\n 4    53 MANN                  NA UEBERHAUPT NICHT                            8\n 5    29 MANN                   0 SEHR STARK                                  2\n 6    44 FRAU                   3 MITTEL                                      7\n 7    79 MANN                   5 MITTEL                                      4\n 8    32 MANN                   7 MITTEL                                      5\n 9    36 FRAU                   0 STARK                                       6\n10    66 MANN                   7 STARK                                       3\n# ℹ 490 more rows\n# ℹ 24 more variables: wahlabsicht_partei &lt;fct&gt;,\n#   zufriedenheit_demokratie &lt;fct&gt;, entwicklung_kriminalitaet &lt;fct&gt;,\n#   social_media_nachrichtenquelle &lt;dbl&gt;, glaubwuerdigkeit_oer_tv &lt;fct&gt;,\n#   glaubwuerdigkeit_privat_tv &lt;fct&gt;, glaubwuerdigkeit_zeitungen &lt;fct&gt;,\n#   glaubwuerdigkeit_social_media &lt;fct&gt;, vertrauen_mitmenschen &lt;dbl&gt;,\n#   vertrauen_gesundheitswesen &lt;dbl&gt;, …\n\n\nDa wir es jetzt mit einer größeren Zahl an Beobachtungen and Variablen zu tun haben kann es nützlich sein, sich einen Überblick zu verschaffen.\nZunächst lassen wir uns die Variablennamen (also die Spalten) ausgeben. Dies geschieht mit der Funktion colnames.\n\ncolnames(sample_gross)\n\n [1] \"alter\"                              \"geschlecht\"                        \n [3] \"fernsehkonsum\"                      \"politisches_interesse\"             \n [5] \"links_rechts_einordnung\"            \"wahlabsicht_partei\"                \n [7] \"zufriedenheit_demokratie\"           \"entwicklung_kriminalitaet\"         \n [9] \"social_media_nachrichtenquelle\"     \"glaubwuerdigkeit_oer_tv\"           \n[11] \"glaubwuerdigkeit_privat_tv\"         \"glaubwuerdigkeit_zeitungen\"        \n[13] \"glaubwuerdigkeit_social_media\"      \"vertrauen_mitmenschen\"             \n[15] \"vertrauen_gesundheitswesen\"         \"vertrauen_bundesverfassungsgericht\"\n[17] \"vertrauen_bundestag\"                \"vertrauen_stadt_gemeindeverwaltung\"\n[19] \"vertrauen_katholische_kirche\"       \"vertrauen_evangelische_kirche\"     \n[21] \"vertrauen_justiz\"                   \"vertrauen_fernsehen\"               \n[23] \"vertrauen_zeitungswesen\"            \"vertrauen_hochschulen\"             \n[25] \"vertrauen_bundesregierung\"          \"vertrauen_polizei\"                 \n[27] \"vertrauen_parteien\"                 \"vertrauen_eu_kommission\"           \n[29] \"vertrauen_eu_parlament\"            \n\n\nEine etwas detailliertere Beschreibung erhalten wir durch die Funktion str. Diese liefert uns auch die Dimensionen (Anzahl der Zeilen und Spalten) des Data Frames, sowie die Variablen, deren Datentyp und die ersten zehn Ausprägungen.\n\nstr(sample_gross)\n\ntibble [500 × 29] (S3: tbl_df/tbl/data.frame)\n $ alter                             : num [1:500] 50 77 64 53 29 44 79 32 36 66 ...\n $ geschlecht                        : Factor w/ 4 levels \"KEINE ANGABE\",..: 3 2 3 2 2 3 2 2 3 2 ...\n $ fernsehkonsum                     : num [1:500] 7 7 7 NA 0 3 5 7 0 7 ...\n $ politisches_interesse             : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 4 NA 7 3 5 5 5 4 4 ...\n $ links_rechts_einordnung           : num [1:500] 5 3 5 8 2 7 4 5 6 3 ...\n $ wahlabsicht_partei                : Factor w/ 13 levels \"NICHT WAHLBERECHTIGT\",..: NA 9 NA 13 10 NA 11 6 NA 9 ...\n $ zufriedenheit_demokratie          : Factor w/ 10 levels \"DATENFEHLER: MFN\",..: 5 6 7 8 6 5 8 NA NA 6 ...\n $ entwicklung_kriminalitaet         : Factor w/ 8 levels \"DATENFEHLER: MFN\",..: 6 5 4 4 6 5 5 5 NA 6 ...\n $ social_media_nachrichtenquelle    : num [1:500] 0 7 0 7 7 6 0 7 NA 0 ...\n $ glaubwuerdigkeit_oer_tv           : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 4 4 5 5 4 NA 5 5 5 5 ...\n $ glaubwuerdigkeit_privat_tv        : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 NA 5 5 5 NA 6 5 6 NA ...\n $ glaubwuerdigkeit_zeitungen        : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 4 5 5 5 NA 7 5 4 4 ...\n $ glaubwuerdigkeit_social_media     : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 6 5 NA 5 6 4 NA 6 6 NA ...\n $ vertrauen_mitmenschen             : num [1:500] 3 3 3 2 3 3 2 NA 3 3 ...\n $ vertrauen_gesundheitswesen        : num [1:500] 6 6 5 7 6 5 5 NA 5 5 ...\n $ vertrauen_bundesverfassungsgericht: num [1:500] 6 6 4 1 6 6 2 NA 5 5 ...\n $ vertrauen_bundestag               : num [1:500] 5 5 5 1 2 7 1 NA 5 4 ...\n $ vertrauen_stadt_gemeindeverwaltung: num [1:500] 5 5 5 1 5 6 4 NA 5 3 ...\n $ vertrauen_katholische_kirche      : num [1:500] 2 1 1 1 2 5 1 NA 2 1 ...\n $ vertrauen_evangelische_kirche     : num [1:500] 3 3 1 1 4 2 1 NA 2 2 ...\n $ vertrauen_justiz                  : num [1:500] 5 3 3 4 4 1 3 NA 4 4 ...\n $ vertrauen_fernsehen               : num [1:500] 4 6 3 3 3 6 1 NA 4 4 ...\n $ vertrauen_zeitungswesen           : num [1:500] 4 6 3 3 3 6 1 NA 4 4 ...\n $ vertrauen_hochschulen             : num [1:500] 5 6 5 1 6 7 3 NA 5 5 ...\n $ vertrauen_bundesregierung         : num [1:500] 5 5 5 1 3 7 1 NA 5 4 ...\n $ vertrauen_polizei                 : num [1:500] 5 5 4 5 4 7 3 NA 5 4 ...\n $ vertrauen_parteien                : num [1:500] 4 4 3 1 3 4 1 NA 2 3 ...\n $ vertrauen_eu_kommission           : num [1:500] 5 5 3 1 3 5 3 NA 4 4 ...\n $ vertrauen_eu_parlament            : num [1:500] 5 5 3 1 3 6 3 NA 4 4 ...\n\n\nEine alternative (aber etwas ordentlichere) Ansicht erhält man mit dem Befehl glimpse aus dem Paket tibble (im tidyverse enthalten).\n\ntibble::glimpse(sample_gross)\n\nRows: 500\nColumns: 29\n$ alter                              &lt;dbl&gt; 50, 77, 64, 53, 29, 44, 79, 32, 36,…\n$ geschlecht                         &lt;fct&gt; FRAU, MANN, FRAU, MANN, MANN, FRAU,…\n$ fernsehkonsum                      &lt;dbl&gt; 7, 7, 7, NA, 0, 3, 5, 7, 0, 7, 7, 7…\n$ politisches_interesse              &lt;fct&gt; MITTEL, STARK, NA, UEBERHAUPT NICHT…\n$ links_rechts_einordnung            &lt;dbl&gt; 5, 3, 5, 8, 2, 7, 4, 5, 6, 3, 4, 7,…\n$ wahlabsicht_partei                 &lt;fct&gt; NA, DIE GRUENEN, NA, WUERDE NICHT W…\n$ zufriedenheit_demokratie           &lt;fct&gt; SEHR ZUFRIEDEN, ZIEMLICH ZUFRIEDEN,…\n$ entwicklung_kriminalitaet          &lt;fct&gt; IST GLEICH GEBLIEBEN, HAT ETWAS ZUG…\n$ social_media_nachrichtenquelle     &lt;dbl&gt; 0, 7, 0, 7, 7, 6, 0, 7, NA, 0, 0, N…\n$ glaubwuerdigkeit_oer_tv            &lt;fct&gt; SEHR GLAUBWUERDIG, SEHR GLAUBWUERDI…\n$ glaubwuerdigkeit_privat_tv         &lt;fct&gt; EHER GLAUBWUERDIG, NA, EHER GLAUBWU…\n$ glaubwuerdigkeit_zeitungen         &lt;fct&gt; EHER GLAUBWUERDIG, SEHR GLAUBWUERDI…\n$ glaubwuerdigkeit_social_media      &lt;fct&gt; EHER N. GLAUBWUERDIG, EHER GLAUBWUE…\n$ vertrauen_mitmenschen              &lt;dbl&gt; 3, 3, 3, 2, 3, 3, 2, NA, 3, 3, 1, 3…\n$ vertrauen_gesundheitswesen         &lt;dbl&gt; 6, 6, 5, 7, 6, 5, 5, NA, 5, 5, 3, N…\n$ vertrauen_bundesverfassungsgericht &lt;dbl&gt; 6, 6, 4, 1, 6, 6, 2, NA, 5, 5, 7, N…\n$ vertrauen_bundestag                &lt;dbl&gt; 5, 5, 5, 1, 2, 7, 1, NA, 5, 4, 4, N…\n$ vertrauen_stadt_gemeindeverwaltung &lt;dbl&gt; 5, 5, 5, 1, 5, 6, 4, NA, 5, 3, 5, N…\n$ vertrauen_katholische_kirche       &lt;dbl&gt; 2, 1, 1, 1, 2, 5, 1, NA, 2, 1, 1, N…\n$ vertrauen_evangelische_kirche      &lt;dbl&gt; 3, 3, 1, 1, 4, 2, 1, NA, 2, 2, 4, N…\n$ vertrauen_justiz                   &lt;dbl&gt; 5, 3, 3, 4, 4, 1, 3, NA, 4, 4, 5, N…\n$ vertrauen_fernsehen                &lt;dbl&gt; 4, 6, 3, 3, 3, 6, 1, NA, 4, 4, 5, N…\n$ vertrauen_zeitungswesen            &lt;dbl&gt; 4, 6, 3, 3, 3, 6, 1, NA, 4, 4, 6, N…\n$ vertrauen_hochschulen              &lt;dbl&gt; 5, 6, 5, 1, 6, 7, 3, NA, 5, 5, 6, N…\n$ vertrauen_bundesregierung          &lt;dbl&gt; 5, 5, 5, 1, 3, 7, 1, NA, 5, 4, 2, N…\n$ vertrauen_polizei                  &lt;dbl&gt; 5, 5, 4, 5, 4, 7, 3, NA, 5, 4, 6, N…\n$ vertrauen_parteien                 &lt;dbl&gt; 4, 4, 3, 1, 3, 4, 1, NA, 2, 3, 2, N…\n$ vertrauen_eu_kommission            &lt;dbl&gt; 5, 5, 3, 1, 3, 5, 3, NA, 4, 4, 2, N…\n$ vertrauen_eu_parlament             &lt;dbl&gt; 5, 5, 3, 1, 3, 6, 3, NA, 4, 4, 2, N…\n\n\nDie hier verwendete Syntax PAKETNAME::FUNKTION ist vielleicht zunächst etwas irritierend. Mit ihr rufen wir ein Paket auf, welches wir nicht vorher geladen haben. Das ist mitunter nützlich und kommt hier zur Anwendung, weil wir das Paket tibble hier ansonsten nicht benutzen.\nSchließlich lassen sich mit dem Befehle summary auch noch Eckwerte wie die Ausprägung von Faktorenstufen (bei Faktoren) oder Lageparameter (bei metrischen Variable) ermitteln.\n\nsummary(sample_gross)\n\n     alter              geschlecht  fernsehkonsum        politisches_interesse\n Min.   :18.00   KEINE ANGABE:  0   Min.   :0.000   MITTEL          :231      \n 1st Qu.:38.00   MANN        :245   1st Qu.:4.000   STARK           :138      \n Median :56.00   FRAU        :254   Median :7.000   WENIG           : 58      \n Mean   :53.78   DIVERS      :  1   Mean   :5.305   SEHR STARK      : 53      \n 3rd Qu.:68.00                      3rd Qu.:7.000   UEBERHAUPT NICHT: 17      \n Max.   :93.00                      Max.   :7.000   (Other)         :  0      \n NA's   :3                          NA's   :8       NA's            :  3      \n links_rechts_einordnung   wahlabsicht_partei       zufriedenheit_demokratie\n Min.   : 1.000          CDU-CSU    : 97      ZIEMLICH ZUFRIEDEN:151        \n 1st Qu.: 4.000          DIE GRUENEN: 89      ETWAS ZUFRIEDEN   : 63        \n Median : 5.000          SPD        : 51      ETWAS UNZUFRIEDEN : 46        \n Mean   : 4.935          FDP        : 50      SEHR ZUFRIEDEN    : 32        \n 3rd Qu.: 6.000          AFD        : 33      ZIEML. UNZUFRIEDEN: 25        \n Max.   :10.000          (Other)    : 56      (Other)           :  3        \n NA's   :23              NA's       :124      NA's              :180        \n        entwicklung_kriminalitaet social_media_nachrichtenquelle\n HAT ETWAS ZUGENOMMEN:168         Min.   :0.000                 \n HAT STARK ZUGENOMMEN:163         1st Qu.:0.000                 \n IST GLEICH GEBLIEBEN:118         Median :1.000                 \n HAT ETWAS ABGENOMMEN: 34         Mean   :2.921                 \n HAT STARK ABGENOMMEN:  3         3rd Qu.:7.000                 \n (Other)             :  0         Max.   :7.000                 \n NA's                : 14         NA's   :45                    \n         glaubwuerdigkeit_oer_tv        glaubwuerdigkeit_privat_tv\n EHER GLAUBWUERDIG   :233        EHER GLAUBWUERDIG   :232         \n SEHR GLAUBWUERDIG   :177        EHER N. GLAUBWUERDIG:154         \n EHER N. GLAUBWUERDIG: 62        SEHR GLAUBWUERDIG   : 40         \n GAR NICHT GLAUBWUERD: 13        GAR NICHT GLAUBWUERD: 23         \n DATENFEHLER: MFN    :  0        DATENFEHLER: MFN    :  0         \n (Other)             :  0        (Other)             :  0         \n NA's                : 15        NA's                : 51         \n        glaubwuerdigkeit_zeitungen      glaubwuerdigkeit_social_media\n EHER GLAUBWUERDIG   :273          EHER N. GLAUBWUERDIG:230          \n SEHR GLAUBWUERDIG   :124          GAR NICHT GLAUBWUERD:109          \n EHER N. GLAUBWUERDIG: 53          EHER GLAUBWUERDIG   : 74          \n GAR NICHT GLAUBWUERD: 11          SEHR GLAUBWUERDIG   : 11          \n DATENFEHLER: MFN    :  0          DATENFEHLER: MFN    :  0          \n (Other)             :  0          (Other)             :  0          \n NA's                : 39          NA's                : 76          \n vertrauen_mitmenschen vertrauen_gesundheitswesen\n Min.   :1.000         Min.   :1.000             \n 1st Qu.:2.000         1st Qu.:4.000             \n Median :2.000         Median :5.000             \n Mean   :2.225         Mean   :4.975             \n 3rd Qu.:3.000         3rd Qu.:6.000             \n Max.   :3.000         Max.   :7.000             \n NA's   :15            NA's   :175               \n vertrauen_bundesverfassungsgericht vertrauen_bundestag\n Min.   :1.000                      Min.   :1.000      \n 1st Qu.:4.000                      1st Qu.:3.000      \n Median :6.000                      Median :4.000      \n Mean   :5.212                      Mean   :4.076      \n 3rd Qu.:7.000                      3rd Qu.:5.000      \n Max.   :7.000                      Max.   :7.000      \n NA's   :179                        NA's   :183        \n vertrauen_stadt_gemeindeverwaltung vertrauen_katholische_kirche\n Min.   :1.000                      Min.   :1.000               \n 1st Qu.:4.000                      1st Qu.:1.000               \n Median :5.000                      Median :2.000               \n Mean   :4.495                      Mean   :2.259               \n 3rd Qu.:5.000                      3rd Qu.:3.000               \n Max.   :7.000                      Max.   :7.000               \n NA's   :177                        NA's   :179                 \n vertrauen_evangelische_kirche vertrauen_justiz vertrauen_fernsehen\n Min.   :1.000                 Min.   :1.000    Min.   :1.000      \n 1st Qu.:1.000                 1st Qu.:4.000    1st Qu.:3.000      \n Median :3.000                 Median :5.000    Median :4.000      \n Mean   :3.016                 Mean   :4.567    Mean   :3.642      \n 3rd Qu.:4.000                 3rd Qu.:6.000    3rd Qu.:5.000      \n Max.   :7.000                 Max.   :7.000    Max.   :7.000      \n NA's   :182                   NA's   :179      NA's   :176        \n vertrauen_zeitungswesen vertrauen_hochschulen vertrauen_bundesregierung\n Min.   :1.000           Min.   :1.000         Min.   :1.00             \n 1st Qu.:3.000           1st Qu.:5.000         1st Qu.:3.00             \n Median :4.000           Median :5.000         Median :4.00             \n Mean   :4.006           Mean   :5.176         Mean   :4.08             \n 3rd Qu.:5.000           3rd Qu.:6.000         3rd Qu.:5.00             \n Max.   :7.000           Max.   :7.000         Max.   :7.00             \n NA's   :177             NA's   :177           NA's   :177              \n vertrauen_polizei vertrauen_parteien vertrauen_eu_kommission\n Min.   :1.000     Min.   :1.000      Min.   :1.000          \n 1st Qu.:4.000     1st Qu.:2.000      1st Qu.:2.000          \n Median :5.000     Median :3.000      Median :4.000          \n Mean   :4.994     Mean   :3.205      Mean   :3.516          \n 3rd Qu.:6.000     3rd Qu.:4.000      3rd Qu.:5.000          \n Max.   :7.000     Max.   :6.000      Max.   :7.000          \n NA's   :177       NA's   :178        NA's   :178            \n vertrauen_eu_parlament\n Min.   :1.000         \n 1st Qu.:2.000         \n Median :4.000         \n Mean   :3.575         \n 3rd Qu.:5.000         \n Max.   :7.000         \n NA's   :178           \n\n\nNun schauen wir uns den ALLBUS selbst – also den Gesamtdatensatz – genauer an.\n\ndaten\n\n# A tibble: 5,342 × 544\n   za_nr           doi   version respid substudy mode    splt21  eastwest german\n   &lt;dbl+lbl&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+&gt;\n 1 5280 [ALLBUS 2… http… 2.0.0 …      1 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 2 5280 [ALLBUS 2… http… 2.0.0 …      2 1 [SIMU… 4 [MAI… 1 [SPL… 1 [ALTE… 1 [JA]\n 3 5280 [ALLBUS 2… http… 2.0.0 …      3 1 [SIMU… 4 [MAI… 1 [SPL… 1 [ALTE… 1 [JA]\n 4 5280 [ALLBUS 2… http… 2.0.0 …      4 1 [SIMU… 4 [MAI… 2 [SPL… 2 [NEUE… 1 [JA]\n 5 5280 [ALLBUS 2… http… 2.0.0 …      5 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 6 5280 [ALLBUS 2… http… 2.0.0 …      6 1 [SIMU… 3 [CAW… 2 [SPL… 1 [ALTE… 1 [JA]\n 7 5280 [ALLBUS 2… http… 2.0.0 …      7 2 [SEQU… 3 [CAW… 3 [SPL… 1 [ALTE… 1 [JA]\n 8 5280 [ALLBUS 2… http… 2.0.0 …      8 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 9 5280 [ALLBUS 2… http… 2.0.0 …      9 2 [SEQU… 3 [CAW… 3 [SPL… 2 [NEUE… 1 [JA]\n10 5280 [ALLBUS 2… http… 2.0.0 …     10 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n# ℹ 5,332 more rows\n# ℹ 535 more variables: ep01 &lt;dbl+lbl&gt;, ep03 &lt;dbl+lbl&gt;, ep04 &lt;dbl+lbl&gt;,\n#   ep06 &lt;dbl+lbl&gt;, lm01 &lt;dbl+lbl&gt;, lm02 &lt;dbl+lbl&gt;, lm19 &lt;dbl+lbl&gt;,\n#   lm20 &lt;dbl+lbl&gt;, lm21 &lt;dbl+lbl&gt;, lm22 &lt;dbl+lbl&gt;, lm14 &lt;dbl+lbl&gt;,\n#   xr19 &lt;dbl+lbl&gt;, xr20 &lt;dbl+lbl&gt;, lm27 &lt;dbl+lbl&gt;, lm28 &lt;dbl+lbl&gt;,\n#   lm29 &lt;dbl+lbl&gt;, lm30 &lt;dbl+lbl&gt;, lm31 &lt;dbl+lbl&gt;, lm32 &lt;dbl+lbl&gt;,\n#   lm33 &lt;dbl+lbl&gt;, lm34 &lt;dbl+lbl&gt;, lm35 &lt;dbl+lbl&gt;, lm36 &lt;dbl+lbl&gt;, …\n\n\nEs wird schnell klar, das weniger die Anzahl der Beobachtungen als vielmehr die Anzahle der Variablen (544) eine Herausforderung darstellt, zumal diese eher kryptische Namen wie hp06 haben. Wie sich also zurechfinden?\n\n\n4 Die Variablen des ALLBUS\nZum Glück lassen sich die sog. Labels, also die sprechenden Beschriftungen die sowohl Fragen and auch Antwortoptionen im ALLBUS-Stata-Datensatz haben, mittels R extrahieren (dies geschieht mit dem Paket labelled). Wir haben dies bereits vorbereitend für den ALLBUS gemacht und laden die entsprechenden Tabellen nun nur noch.\n\nvariablen &lt;- read_csv2(\"Datensatz/ALLBUS_2021_variablen.csv\", show_col_types = FALSE)\nvariablen_optionen &lt;- read_csv2(\"Datensatz/ALLBUS_2021_variablen_optionen.csv\", show_col_types = FALSE)\n\nEs lohnt sich, beide Objekte mittels View() oder durch eine Klick ob die beiden Objekte variablen und variablen_optionen in RStudio anzuschauen. Interessant sind die Felder variable (der Name der Variable im ALLBUS) und label (eine sprechende Beschreibung).\nSuchen wir etwa nach hp06 finden wie die Label-Beschreibung “EPIDEMIE: STAAT DARF KRANKE ISOLIEREN”, die schon deutlich besser interpretierbar ist als hp06. Eine genaue Dokumentation und (vor allem wichtig) der genaue Fragetext findet sich in den Dokumenten ZA5280_fb_CAWI.pdf (Fragebogen) und ZA5280_cdb.pdf (Variablenreport). Beide sind wie der ALLBUS selbst abgelegt im Ordner Datensatz, werden aber ausserhalb von RStudio geöffnet.\nDer Fragebogen reicht normalerweise aus, um sich einen Überblick zu verschaffen, aber der Variablenreport ist dann nützlich, wenn man den Zusammenhang zwischen einem Dummywert (bspw. “4”) und dessen Bedeutung in Verbindung mit einer bestimmten Variable herausfinden möchte. Die Fragen lauter bei hp06\nUnd was denken Sie über folgende Maßnahmen: Sollte in Deutschland in Zeiten schwerer Epidemien der Staat das Recht haben, Folgendes zu tun?\nNachweislich infizierte Personen isolieren\nUnd der Dummy-Wert “4” steht bei dieser Frage für die Antwort Auf keinen Fall.\nWenn man die drei kleinen Zufallssamples mit dem Hauptdatensatz vergleicht, fällt schnell auf, dass die Samples ausschließlich (echte) Zahlenfelder für (vor allem) ordinale Likert-skalierte Variablen enthalten. Bei diesen bedeutet ein höherer Wert i.d.R. mehr Zustimmung oder eine ausgeprägtere Verhaltensausprägung gegenüber geringeren Werten. Es gibt aber auch Fälle in denen diese sog. Polarität der Variablenwerte umgedreht ist und geringe Werte “stärker” sind als hohe, order solche, in denen wir es mit nominalen Skalen zu tun haben, die Zahlen also in keinerlei logischem Zusammenhang stehen.\nWas heißt das konkret? Zur Sicherheit sollte man im Rahmen einer eigenen Analyse in den Hauptdatensatz und in die Dokumentatuin schauen um absolut sicher zu sein, dass man keine unzulässigen Umformungen oder Berechnungen vornimmt (und etwa den Mittelwert einer nominalskalierten Variable bestimmt), oder die Ergebnisse misinterpretiert (etwa wenn die Polarität einer Variablen umgedreht codiert wurde). Es gilt immer: know your data.\nZunächst ist es aber vollkommen legetim, um die Variablenliste oder das Befragungsdokument nach interessanten Variablen zu durckforsten. Wir können in der View()-Ansicht des Objekts variable nach Begriffen suchen, die in den Labels vorkommen. Beispielsweise finden wir mit einer Suche nach dem Begriff ‘medien’ die Variablen lm35 (Nutzung von sozialen Medien als Nachrichtenquelle) und lm39 (Glaubwürdigkeit sozialer Medien mit Blick auf Kriminalität), die uns vielleicht interessieren.\n\n\n5 Bildung eines Teilsamples\nEin Schritt, der praktisch für alle Studienprojekte im Verlauf des Semesters relevant sein wird, ist die Bildung eines Teildatensatzes, welcher die Variablen (und ggf. Fälle) enthält, die für Ihre Analyse relevant sind.\nTechnisch gesehen ist das gar nicht unbedingt notwendig – wir können jeder Zeit Berechnungen am Gesamtdatensatz anstellen. Aber oft ist ein Teildatensatz übersichtlicher und ermöglicht ein besseres Verständnis der Daten.\nWie bildet man ein solches Teilsample? Entscheidend ist hier die Funktion select.\n\nfernsehkonsum &lt;- daten %&gt;% \n  select(age, sex, lm02)\n\nWir extrahieren hier mittels select drei Variablen, nämlich Alter (age), Geschlecht (sex) und den Fernsehkonsum in Minuten (lm02).\nDas Ergegnis ist ein Datensatz, der weiterhin alle 5.342 Fälle, aber nur drei (statt 544) Variablen enthält.\n\nfernsehkonsum\n\n# A tibble: 5,342 × 3\n   age       sex       lm02              \n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;         \n 1 54        2 [FRAU]  210               \n 2 53        1 [MANN]   90               \n 3 89        2 [FRAU]  135               \n 4 79        1 [MANN]   60               \n 5 62        2 [FRAU]  180               \n 6 23        1 [MANN]   45               \n 7 31        2 [FRAU]   30               \n 8 57        2 [FRAU]   -9 [KEINE ANGABE]\n 9 68        1 [MANN]  180               \n10 51        2 [FRAU]  180               \n# ℹ 5,332 more rows\n\n\nEine gewisse Komplikation ist allerdings die Tatsache, dass in diesem Ausschnitt die Variablen sex noch eine Dummy-Zahl ist (1 = männlich, 2 = weiblich, 3 = divers) und zudem die Variable lm02 negative Werte enthät. Diese zeigen keinen negativen Fernsehkonsum an, sondern werden für Spezialwerte verwendet (“keine Angabe”, “durch Filterbedingung weggefallen”). Als Faustregel gilt: Negative Werte im ALLBUS sollten praktisch immer durch NAs ersetzt werden. Das ist unbedingt von “0” als Wert zu unterscheiden. Mit einer “echten” Null kann ebenso wie mit “echten” Negtivwerten gerechnet werden, dies führt aber zu substantiellen Verzerrungen, wenn es sich um Dummy-Werte handelt.\nDer folgenden Codeblock bereinigt die Daten zum Fernsehkomsum. Dazu benennte er die drei Variablen znnächst in transparentere Were um. Anschlißend werden negative Werte in NAs umgewandelt (hier mit der Funktion replace_with_na_all aus dem Paket naniar). Dann wird das Geschlecht faktorisiert, was die Zahlen durch das Label (also MANN / FRAU / DIVERS) ersetzt. Und schließlich werden die Labels und Attribute entfernt, die nun nicht mehr benötigt werden.\n\nfernsehkonsum_bereinigt &lt;- fernsehkonsum %&gt;% \n  rename(alter = age,\n         geschlecht = sex,\n         fernsehkonsum_minuten = lm02) %&gt;% \n  naniar::replace_with_na_all(condition = ~.x &lt; 0) %&gt;% \n  mutate(geschlecht = as_factor(geschlecht)) %&gt;% \n  labelled::remove_labels() %&gt;%\n  labelled::remove_attributes(\"format.stata\")\n\n\nfernsehkonsum_bereinigt\n\n# A tibble: 5,342 × 3\n   alter geschlecht fernsehkonsum_minuten\n   &lt;dbl&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1    54 FRAU                         210\n 2    53 MANN                          90\n 3    89 FRAU                         135\n 4    79 MANN                          60\n 5    62 FRAU                         180\n 6    23 MANN                          45\n 7    31 FRAU                          30\n 8    57 FRAU                          NA\n 9    68 MANN                         180\n10    51 FRAU                         180\n# ℹ 5,332 more rows\n\n\n\n\n6 Zusammenfassung\nWir sind jetzt in einer guten Position, um mit der praktischen Arbeit am ALLBUS zu beginnnen, also der Analyse und Interpretation konkreter Daten."
  },
  {
    "objectID": "Skript_3.2.html",
    "href": "Skript_3.2.html",
    "title": "Datentypen und -strukturen",
    "section": "",
    "text": "Artwork by @allision_horst\nIn unserem alltäglichen Leben haben wir ständig mit unterschiedlichsten Formen von Daten zu tun, meistens ohne uns darüber groß Gedanken zu machen. Beispielsweise haben wir es bei dem Versenden einer Textnachricht mit einer Reihe von unterschiedlichen Formen an Daten zu tun, die Nachricht an sich als auch den/die Empfänger:in - in Form von Text - als auch den Zeitpunkt - in Form von Zahlen bzw. eines Datums. Wenn wir mit diesen Daten arbeiten möchten, benötigen wir etwas Wissen darüber wie verschiedene Datenformate funktionieren."
  },
  {
    "objectID": "Skript_3.3.html",
    "href": "Skript_3.3.html",
    "title": "Selektion, Manipulation und Transformation",
    "section": "",
    "text": "Bild von Gerd Altmann auf Pixabay\nWenn wir mit Daten arbeiten liegen diese in den seltesten Fällen in der von uns benötigten Form vor. Entsprechend wichtig ist es für uns die Fähigkeit zu besitzen Daten in die für uns gewünschte Form zu bringen. Damit beschäftigen wir uns in diesem Abschnitt."
  },
  {
    "objectID": "Skript_3.4.html",
    "href": "Skript_3.4.html",
    "title": "Tabellen und Grafiken in R",
    "section": "",
    "text": "Überall Tabellen!, Bild generiert von Midjourney\nDieses Notebook illustriert verschiedene Möglichkeiten dafür, wie auf Grundlage der ALLBUS-Daten informative Tabellen und Plots erstellt werden können. Tabellen und Datenvisualisierungen (“Plots”) stellen zwei sehr effektive Möglichkeiten dar, sich schnell einen Überblick über komplexe Daten zu verschaffen. Dabei lassen sich oftmals Zusammenhänge zwischen zwei oder mehr Variablen erahnen, die dann später inferenzstatistisch auf die Solidität ihres Zusammenhangs hin überprpüft werden können."
  },
  {
    "objectID": "Skript_4.1.html",
    "href": "Skript_4.1.html",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Die Skyline als Barplot, Bild generiert von Midjourney\nZunächst ein paar Begriffserläuterungen: Bei der quantitativen Datenerhebung misst man die Ausprägung eines bestimmten Merkmals. Ein Merkmal kann zum Beispiel das Alter einer Person sein. Die Person ist dann der Merkmalsträger. Die Merkmalsausprägung bzw. der Messwert ist die konkrete Altersangabe, z.B. 23 Jahre. Die Menge aller Merkmalsträger, über die man durch die Untersuchung zu Erkenntnissen kommen will, heißt Grundgesamtheit. Oft kann man aber gar nicht oder nur sehr schwer die ganze Grundgesamtheit heranziehen und muss sich mit einer Teilmenge begnügen: Der Stichprobe. Die Daten sind erhoben, die Arbeitsumgebung ist eingerichtet, wir haben die Datensätze geladen und wissen, wie wir sie bearbeiten und visualisieren können. Nun wird es Zeit, sich einen ersten Eindruck von den statistischen Eigenschaften unserer Daten zu verschaffen. Von diesen Eigenschaften hängt ab, welche Methoden wir auf sie anwenden können und somit, welche Fragen wir mit ihnen überhaupt beantworten können. Dabei interessieren uns zunächst zwei Aspekte, erstens: Welche Methoden passen zur Beschaffenheit der Daten und zweitens: Welche inhaltlichen Eigenschaften können wir darauf aufbauend in den Daten erkennen? Ersteres ist abhängig vom Skalenniveau, für letzteres schauen wir uns einige der wichtigsten Maßzahlen (Parameter) an."
  },
  {
    "objectID": "Skript_4.2.html",
    "href": "Skript_4.2.html",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "",
    "text": "Überall Daten, Bild generiert von Midjourney\n\n\nLaden wir zunächst wieder unsere Daten: Da wir uns drei verschiedene Skalenniveaus ansehen wollen, verwenden wir Daten, die jeweils ein solches repräsentieren. Wir laden daher Daten zu Geschlecht (“sex”), Vertrauen in die Bundesregierung (“pt12”) und Netto-Einkommen (“di01a”).\nIm Folgenden werden wir aus dem Allbus-2021-Datensatz ein paar Beispiele herausgreifen, um die Berechnung und Visualisierung von Häufigkeiten und Parametern zu demonstrieren.\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n1 Data Management\nDazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, ggplot2, haven, dplyr)  \ntheme_set(theme_classic()) \n\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\") \n\n\nallbus_df &lt;- daten %&gt;% \n  select(\"sex\", \"pt12\", \"di01a\") %&gt;%          \n  mutate(across(c(\"pt12\", \"di01a\"), ~ as.numeric(.))) %&gt;%                                     \n  mutate(across(c(\"pt12\", \"di01a\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%          \n  na.omit()   \n\ncolnames(allbus_df) &lt;- c(\"Geschlecht\", \"VertrauenBR\", \"Einkommen\") \n\n\n\n2 Der Modus\nDer Modus, auch Modalwert genannt, gibt an, welche Ausprägung eines gemessenen Merkmals am häufigsten vorkommt.\nWir müssen einfach nur zählen, wie oft jede Merkmalsausprägung vorkommt. Diejenige mit dem höchsten Wert (bzw. der größten absoluten Häufigkeit) ist der Modus. In R gibt es keine vorgefertigte Funktion, die diesen Parameter berechnet. Das folgende Code-Beispiel zeigt eine mögliche Lösung speziell für unseren “allbus.df$Geschlecht”-DataFrame.\n\nget_mode = function(vector){\n\n  # Häufigkeitstabelle erstellen:\n  frequencies = table(vector) \n  \n  # Höhe der größten Häufigkeit ermitteln:\n  max_freq = max(frequencies)  \n  \n  # Teiltabelle erstellen, die nur die Spalten mit der höchsten Häufigkeit enthält:\n  where_max =frequencies == max_freq \n  \n  # Namen der verbliebenen Spalten (= Modus) ermitteln:\n  modus = names(frequencies[where_max]) \n  return(modus)\n}\n#Ausgabe des Modus:\ncat(\"Der Modus lautet \", get_mode(allbus_df$Geschlecht), \".\")\n\nDer Modus lautet  1 .\n\n\n\n\n3 Der Median\nFür mindestens ordinal skalierte Messwerte empfielt sich neben dem Modus zusätzlich der Median. Einen der Größe nach aufsteigend sortierten Datensatz teilt der Median genau in der Mitte, es liegen also genauso viele Elemente links wie rechts davon.\nFür den Fall, dass die Anzahl an Elementen im Datensatz \\(n\\) ungerade ist, entspricht der Median dem Messwert, der genau in der Mitte liegt:\n\\[\nx_{Med} = x_{\\frac{n + 1}{2}}\n\\]\nIst \\(n\\) gerade, kann jedes der beiden Elemente, die in der Mitte liegen, als Median verwendet werden. Es ist aber eher üblich, beide zu addieren und dann durch zwei zu teilen:\n\\[\nx_{Med} = \\frac{1}{2} (x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})\n\\]\nR stellt eine Funktion zur Berechnung des Medians bereit. Wir schauen uns als Beispiel das Vertrauen der Allbus-Befragten in die Bundesregierung an: Im Gegensatz zum Geschlecht können wir hier eine Rangfolge festlegen, jedoch nicht die Abstände dazwischen exakt messen. Wir haben es folglich mit ordinalen Daten zu tun.\nWir verschaffen uns zunächst wieder einen Überblick mit der table-Funktion und sehen sieben verschiedene Werte.\n\ntable(allbus_df$VertrauenBR)\n\n\n  1   2   3   4   5   6   7 \n 68  79 120 170 176 155  32 \n\n\nDie Daten sind bereits von “gar nicht” zu “sehr hoch” sortiert. Wir wenden die median-Funktion an und erhalten “4” als Ausgabe:\n\nmedian_vertrauen = median(allbus_df$VertrauenBR)\n\nWenn man sich die Daten als Säulendiagramm bzw. Barplot ausgeben lässt und den Median einzeichnet (im folgenden Codebeispiel die rote gestrichelte Linie), kann man erahnen, dass der Median die sieben Klassen so teilt, dass beidseitig gleich viele abgegebene Stimmen liegen:\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = median_vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1)\n\n\n\n\n\n\n4 Quantile\nEin Quantil legt fest, wie viele Werte über bzw. unter einer bestimmten Grenze liegen und teilt den Datensatz damit in zwei Teile. Den bekanntesten Spezialfall haben wir mit dem Median bereits kennengelernt. Die Grenze lag in dem Fall genau in der Mitte, es liegen also 50% unterhalb der Grenze und 50% darüber. Bei einem 31%-Quantil würden hingegen 31% der Werte unter der Grenze liegen und 69% darüber. Wichtige Quantile sind die sogenannten Quartile, zu denen das 25%-Quantil, der Median und das 75%-Quantil zusammengefasst werden. Sie teilen die Gesamtmenge an Messwererten in vier gleich große Teile.\nDas folgende R-Beispiel gibt die drei Quartile des Vertrauens-Datensatzes aus:\n\nquartile = quantile(allbus_df$VertrauenBR, probs = c(0.25, 0.5, 0.75))\nquartile\n\n25% 50% 75% \n  3   4   5 \n\n\nDas folgende Codebeispiel zeichnet neben dem Median (rot) auch das 25%-Quantil (blau) und das 75%-Quantil (grün) in das Säulendiagramm ein:\n\nquantile25 = quartile[\"25%\"]  #quantile(allbus_df$VertrauenBR, probs=c(0.25)) \nquantile75 = quartile[\"75%\"]\n\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = quantile25, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quantile75, color = \"green\", linetype = \"dashed\", linewidth = 1)\n\n\n\n\n\n\n5 Der Mittelwert\nDer Begriff “Mittelwert” ist etwas ungenau, da es mehrere verschiedene Mittelwerte gibt. Oft ist damit das arithmetische Mittel gemeint. Es lässt sich nur bei mindestens kardinal skalierten Daten anwenden und bezieht die Gewichte der jeweiligen Merkmalsausprägungen mit ein.\nDas arithmetische Mittel erhält man, indem man alle Messwerte addiert und durch die Gesamtzahl an Messwerten teilt:\n\\[\n\\bar{x} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} = \\frac{1}{n}  \\sum_{i=1}^{n} {x_{i}}\n\\]\nSchauen wir uns das am Beispiel des Netto-Einkommens der Befragten im Allbus-Datensatz an: Wir können dazu die bereits vorhandene Funktion mean nutzen:\n\nmean_einkommen = mean(allbus_df$Einkommen)\nmean_einkommen\n\n[1] 2442.545\n\n\nVergleichen wir das mit dem Median, fällt auf, dass zwischen beiden Lageparametern über 200 Euro Differenz bestehen:\n\nmedian_einkommen = median(allbus_df$Einkommen)\nmedian_einkommen\n\n[1] 2200\n\n\nDer Modus liegt noch weiter weg:\n\nmodus_einkommen = get_mode(allbus_df$Einkommen)\nmodus_einkommen\n\n[1] \"3000\"\n\n\nDas liegt daran, dass die Einkommensdaten kontinuierlich sind und es keinen homogenen An- und Abstieg der Häufigkeitsverteilung gibt. Der Einkommenswert, den am meisten Personen exakt gleich angegeben haben, ist deshalb wenig aussagekräftig und der Modus macht nur Sinn, nachdem man die Daten in Form von Einkommensklassen diskretisiert hat.\n\n\n\n\n\n\n\n6 Geometrisches Mittel\nNicht unerwähnt bleiben sollte das geometrische Mittel, das bei der Berechnung des Mittelwerts von prozentualen Veränderungen angewendet wird. Dabei werden die einzelnen Messwerte multipliziert und die n-te Wurzel aus dem Ergebnis gezogen, wobei n die Gesamtzahl an Messwerten ist:\n\\[\nx_{Geom} = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}\n\\]\nIn R gibt es dafür keine eigenständige Funktion, man kann aber die Gleichung umstellen und mit Hilfe einiger anderer eingebauter Funktionen eine simple Alternative erstellen, indem man einen kleinen Trick mit der Exponentialfunktion und dem natürlichen Logarithmus anwendet:\n\\[\\begin{aligned}\nx_{Geom}  & = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}} \\\\\n          & = e^{ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}} \\\\\n          & = e^{\\frac{1}{n}ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})} \\\\\n          & = e^{\\frac{1}{n} \\sum_{i=1}^{n} ln({x_{i})}  }\n\\end{aligned}\\]\nAuch, wenn die resultierende Formel wenig ansprechend aussieht, kann man bei genauerem Hinsehen das versteckte arithmetische Mittel erkennen und den ganzen Ausdruck in folgenden R-Code umsetzen:\n\ngeom_mean = function(vector){\n  exp(mean(log(vector)))\n}\n\n\nmin(allbus_df$Einkommen)\n\n[1] -41\n\ngeom_einkommen &lt;- geom_mean(allbus_df$Einkommen + 42) \ngeom_einkommen\n\n[1] 1994.776"
  },
  {
    "objectID": "Skript_4.2.html#berechnung",
    "href": "Skript_4.2.html#berechnung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung",
    "href": "Skript_4.2.html#visualisierung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation",
    "href": "Skript_4.2.html#interpretation",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-1",
    "href": "Skript_4.2.html#berechnung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-1",
    "href": "Skript_4.2.html#visualisierung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-1",
    "href": "Skript_4.2.html#interpretation-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-2",
    "href": "Skript_4.2.html#berechnung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-2",
    "href": "Skript_4.2.html#visualisierung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-2",
    "href": "Skript_4.2.html#interpretation-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html",
    "href": "Skript_4.3.html",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "",
    "text": "Wie groß sind die Kreise?, Bild generiert von Midjourney\n\n\nIm letzten Abschnitt haben wir uns angesehen, mit welchen Maßen sich etwas darüber sagen lässt, um welche Punkte sich die Masse der Messwerte in unserem Datensatz konzentriert. Jetzt wollen wir wissen, wie stark die Messwerte streuen, also wie stark die Messwerte vom Lageparameter abweichen.\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\n\n1 Data Management\nDazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, dplyr)  \ntheme_set(theme_classic())\n\nIm Anschluss laden wir die benötigten Daten und bereiten diese für die spätere Analyse vor.\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\") \n\n\nallbus_df &lt;- daten %&gt;% \n  dplyr::select(sex, pt12, di01a) %&gt;%          \n  mutate(across(c(\"pt12\", \"di01a\"), ~ as.numeric(.))) %&gt;%                                     \n  mutate(across(c(\"pt12\", \"di01a\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%          \n  na.omit()  \n\ncolnames(allbus_df) = c(\"Geschlecht\", \"VertrauenBR\", \"Einkommen\") \n\n\n\n2 Die Spannweite\nDie Spannweite (auch Range genannt) gibt den Abstand zwischen der kleinsten und der größten Merkmalsausprägung an. Sie ist das einfachste Streuungsmaß, zugleich aber nur wenig aussagekräftig.\n\\[\nSP = x_{max} - x_{min}  \n\\] Mit der range-Funktion kann man sich in R die beiden Extremwerte als Vektor ausgeben lassen:\n\nrange(allbus_df$Einkommen)\n\n[1]   -41 14100\n\n\nAlternativ kann man auch einfach Das Maximum und das Minimum ermitteln und die Differenz berechnen:\n\neinkommen_max = max(allbus_df$Einkommen) - min(allbus_df$Einkommen)\neinkommen_max\n\n[1] 14141\n\n\n\n\n3 Quantilsabstände\nDieses Maß gibt die Differenz zweier Quantile an. Insbesondere der Quartilsabstand (75%-Quantil - 25%-Quantil) ist hier von Bedeutung.\nDer Quaantilsabstand berechnet sich für das obere Quantil \\(Q_{o}\\) und das untere Quantil \\(Q_{u}\\) folgendermaßen:\n\\[\nQA = Q_{o} - Q_{u}\n\\]\nFür den Quartilsabstand gibt es eine Funktion in R:\n\n# Berechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range):\nIQR(allbus_df$VertrauenBR)\n\n[1] 2\n\n\n\nBerechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range)\n\nUm den mittleren Quartilsabstand zu ermitteln, kann man das Ergebnis noch durch 2 teilen.\n\n\n4 Varianz und Standardabweichung\nEin nützlicheres Maß dafür, wie die einzelnen Merkmalsausprägungen um den Mittelwert verteilt sind - vorausgesetzt, man hat es mit kardinal skalierten Daten zu tun, kann die Varianz sein.\nEs wird unterschieden zwischen der Varianz der Grundgesamtheit und der Stichprobenvarianz. Bei ersterer berechnet man für jede einzelne Merkmalsausprägung ihre Abweichung vom Mittelwert, quadriert die Ergebnisse und summiert diese. Anschließend teilt man durch die Größe der Grundgesamtheit:\n\\[\nVar = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nDa man wie gesagt meist nur auf eine Stichprobe zurückgreifen kann, kann man den Mittelwert der Grundgesamtheit durch den Mittelwert der Stichprobe nur schätzen. Um die damit verbundene Verzerrung auszugleichen, kann es sinnvoll sein die Summe der quadrierten Abweichungen nicht durch \\(n\\), sondern durch \\((n-1)\\) zu teilen. So ergibt sich folgende Formel, wobei \\(n\\) hier die Stichprobengröße darstellt:\n\\[\nVar = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nIn R bekommen wir die Varianz mit der var-Funktion, hier am Beispiel des Netto-Einkommens:\n\nvar(allbus_df$Einkommen)\n\n[1] 2590358\n\n\nWie wir sehen, ist das Ergebnis sehr groß und auf den ersten Blick nicht leicht interpretierbar. Nützlicher ist da die Standardabweichung, die wir einfach dadurch erhalten, dass wir die Quadratwurzel der Varianz ziehen:\n\\[\nSD = \\sqrt{Var} = s =  \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] R bietet zur Berechnung der Standardabweichung die sd-Funktion an.\n\nsd(allbus_df$Einkommen)\n\n[1] 1609.459\n\n\n\n\n5 Zusammenfassungen ausgeben lassen\nWenn man nicht jeden Parameter einzeln abfragen will und kardinal skalierte Daten hat, kann man sich die wichtigsten Lageparameter auch als Zusammenfassung ausgeben lassen:\n\nsummary(allbus_df$Einkommen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    -41    1500    2200    2443    3000   14100 \n\n\n\n\n6 Visualisierung der Streuungsparameter\nIn folgendem Codebeispiel lassen wir uns das Histogramm aus dem letzten Abschnitt ausgeben, allerdings reduziert auf Median (blau, durchgezogene Linie) und das arithmetische Mittel (rot, durchgezogene Linie). Wir ergänzen diese Lageparameter um den Minimal- und den Maximalwert (schwarz, gestrichelt), das untere und obere Quartil (blau, gestrichelt) und die Standardabweichung, hier beiderseits vom Mittelwert aufgetragen(rot, gestrichelt).\n\n\n\n\n\nEs ist sehr deutlich zu erkennen, dass die Spannweite (die Distanz zwischen den beiden schwarzen gestrichelten Linien) wenig hilfreich ist, wenn man erfahren will, wo besonders viele Messwerte liegen. Der Interquartilsabstand scheint hier sehr viel aussagekräftiger zu sein, während die Standardabweichung unterhalb des Mittelwerts mehr Messwerte einzuschließen scheint als oberhalb davon. Dieses Ungleichgewicht wird im Abschnitt über Verteilungen noch eine Rolle spielen."
  },
  {
    "objectID": "Skript_4.3.html#berechnung",
    "href": "Skript_4.3.html#berechnung",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung\n\\[\nSP = x_{max} - x_{min}  \n\\] Mit der range-Funktion kann man sich in R die beiden Extremwerte als Vektor ausgeben lassen:\n\nrange(allbus_df$Einkommen)\n\n[1]   -41 14100\n\n\nAlternativ kann man auch einfach Das Maximum und das Minimum ermitteln und die Differenz berechnen:\n\neinkommen_max = max(allbus_df$Einkommen) - min(allbus_df$Einkommen)\neinkommen_max\n\n[1] 14141"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung",
    "href": "Skript_4.3.html#visualisierung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation",
    "href": "Skript_4.3.html#interpretation",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-1",
    "href": "Skript_4.3.html#berechnung-1",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung\nDer Quaantilsabstand berechnet sich für das obere Quantil \\(Q_{o}\\) und das untere Quantil \\(Q_{u}\\) folgendermaßen:\n\\[\nQA = Q_{o} - Q_{u}\n\\]\nFür den Quartilsabstand gibt es eine Funktion in R:\n\n# Berechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range):\nIQR(allbus_df$VertrauenBR)\n\n[1] 2\n\n\n\nBerechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range)\n\nUm den mittleren Quartilsabstand zu ermitteln, kann man das Ergebnis noch durch 2 teilen."
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-1",
    "href": "Skript_4.3.html#visualisierung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-1",
    "href": "Skript_4.3.html#interpretation-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-2",
    "href": "Skript_4.3.html#berechnung-2",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung\nEs wird unterschieden zwischen der Varianz der Grundgesamtheit und der Stichprobenvarianz. Bei ersterer berechnet man für jede einzelne Merkmalsausprägung ihre Abweichung vom Mittelwert, quadriert die Ergebnisse und summiert diese. Anschließend teilt man durch die Größe der Grundgesamtheit:\n\\[\nVar = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\] Da man wie gesagt meist nur auf eine Stichprobe zurückgreifen kann, kann man den Mittelwert der Grundgesamtheit durch den Mittelwert der Stichprobe nur schätzen. Um die damit verbundene Verzerrung auszugleichen, kann es sinnvoll sein die Summe der quadrierten Abweichungen nicht durch \\(n\\), sondern durch \\((n-1)\\) zu teilen. So ergibt sich folgende Formel, wobei \\(n\\) hier die Stichprobengröße darstellt:\n\\[\nVar = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nIn R bekommen wir die Varianz mit der var-Funktion, hier am Beispiel des Netto-Einkommens:\n\nvar(allbus_df$Einkommen)\n\n[1] 2593715\n\n\nWie wir sehen, ist das Ergebnis sehr groß und auf den ersten Blick nicht leicht interpretierbar. Nützlicher ist da die Standardabweichung, die wir einfach dadurch erhalten, dass wir die Quadratwurzel der Varianz ziehen:\n\\[\nSD = \\sqrt{Var} = s =  \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] R bietet zur Berechnung der Standardabweichung die sd-Funktion an.\n\nsd(allbus_df$Einkommen)\n\n[1] 1610.501"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-2",
    "href": "Skript_4.3.html#visualisierung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-2",
    "href": "Skript_4.3.html#interpretation-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html",
    "href": "Skript_4.4.html",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "",
    "text": "Wie ist die Stadt verteilt?, Bild generiert von Midjourney\nWir haben jetzt eine Menge verschiedener Methoden kennengelernt, mit denen wir uns einen Eindruck davon machen können, wie unsere Daten verteilt sind. Um den Begriff der Verteilung an sich haben wir dagegen bis hierher einen Bogen gemacht. Dabei spielt die Art der Verteilung für viele weiterführende Anwendungen eine große Rolle. Manche statistische Verfahren setzen spezifische Verteilungen voraus.\nStatistische Verteilungen stellen eine Verbindung her zwischen den empirisch gewonnenen Daten und auf Wahrscheinlichkeit basierenden Aussagen, die wir daraus ableiten wollen. Dabei steht die Frage im Zentrum, mit welcher Wahrscheinlichkeit ein bestimmtes Zufallsereignis eintritt bzw. wie wahrscheinlich es ist, dass eine zufällig gezogene Stichprobe so ausfällt, wie es beobachtet wurde. Können wir annehmen, dass ein Merkmal auf eine bestimmte Weise verteilt ist, können wir eine Prognose abgeben, in welchem Rahmen eine Stichprobe liegen wird. Interessant wird es insbesondere dann, wenn sich nachher zeigt, dass die theoretische Schätzung falsch war und völlig andere Ergebnisse herauskommen. Denn das bedeutet, dass unsere Daten von Faktoren beeinflusst werden, die wir nicht mit eingerechnet haben.\nWir hätten also gerne eine Funktion, die es uns ermöglicht, alle denkbaren Stichproben auf einen Wahrscheinlichkeitswert zwischen 0 und 1 abzubilden. Wenn abzählbar viele Ereignisse eintreten können (z.B. beim Würfeln oder Münzwurf), spricht man von Wahrscheinlichkeitsfunktionen. Bei stetigen Verteilungen (z.B. wenn Zeiträume betrachtet werden) verwendet man den Begriff Dichtefunktion. In beiden Fällen summieren sich alle Funktionswerte zu 1 auf. Eine kumulierte Wahrscheinlichkeits- bzw. Dichtefunktion nennt man Verteilungsfunktion. D.h. bei der Verteilungsfunktion werden die Funktionswerte der Wahrscheinlichkeits summiert bzw. die Dichtefunktion integriert.\nSchauen wir uns als Beispiel das Histogramm der Allbus-Einkommensverteilung aus den vorherigen Abschnitten an."
  },
  {
    "objectID": "Skript_4.4.html#berechnung",
    "href": "Skript_4.4.html#berechnung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung\nDie Dichtefunktion der Normalverteilung wird folgendermaßen berechnet:\n\\[\nf(x,\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\] Dabei ist \\(\\mu\\) der Mittelwert (arithm. Mittel, Median, oder Modus) und \\(\\sigma\\) die Standardabweichung.\nBei einer Normalverteilung mit einem Mittelwert von null und einer Varianz von eins spricht man von einer Standardnormalverteilung:\n\\[\n\\phi(z) = f(z, 0, 1) = \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{- \\frac{z^2}{2}}\n\\]\nNormal- aber nicht standardnormal verteilte Werte lassen sich leicht standardisieren, indem man den Mittelwert von ihnen abzieht und das Ergebnis durch die Standardabweichung teilt:\n\\[\nz = \\frac{Messwert - Mittelwert}{Standardabweichung} = \\frac{x - \\mu}{\\sigma}\n\\] \\(\\mu\\) kann dabei das arithmetische Mittel, der Median oder der Modus sein.\nDer Vollständigkeit halber sei hier auch noch die Formel der Verteilungsfunktion aufgeschrieben:\n\\[\nF(x, \\mu, \\sigma) = \\frac{1}{2} \\left[1 + \\text{erf} \\left (\\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right]\n\\] \\(erf\\) steht für die Gauß’sche Fehlerfunktion.\nDa R umfangreiche Funktionalitäten bereitstellt, um diese Verteilungen zu berechnen, ist es an dieser Stelle nicht notwendig, sich diese Formel einzuprägen oder irgendwas damit per Hand zu rechnen. R hat eine ganze Reihe an Verteilungen implementiert und stellt zu jeder davon u.a. vier Funktionen bereit:\n\ndie Wahrscheinlichkeits-/Dichtefunktion, beginnend mit dem Buchstaben d,\ndie Verteilungsfunktion, beginnend mit p,\nQuantile, beginnend mit q sowie\nZufallszahlen auf Basis der jeweiligen Verteilung, beginnend mit r."
  },
  {
    "objectID": "Skript_4.4.html#visualisierung",
    "href": "Skript_4.4.html#visualisierung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung\nDie folgenden vier Codebeispiele ergeben Visualisierungen der Normalverteilung bei unterschiedlichen Stichprobengrößen von \\(n = 10\\) bis \\(n = 10000\\). Dabei wird jeweils eine Zufallsstichprobe gezogen und die Verteilung der “Messwerte” als Histogramm ausgegeben. Je größer die Stichprobe, desto stärker sollte sich dabei die Verteilung der Zufallswerte der Dichtefunktion der Normalverteilung annähern, die hier rot dargestellt ist. Mittelwert und Standardabweichung sind so gewählt, dass die Standardnormalverteilung herauskommt. Wiederholen Sie die einzelnen Beispiele mehrmals, dann sollte jedesmal eine andere Stichproben-Verteilung herauskommen:\n\n#Zufalls-Stichprobe mit 10 Messwerten:\n\nrandom.data = rnorm(10, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 10 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\nrandom.data = rnorm(100, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 100 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\n\n\n\n\nrandom.data = rnorm(1000, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 1000 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\n\n\n\n\nrandom.data = rnorm(10000, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 10000 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")"
  },
  {
    "objectID": "Skript_4.4.html#interpretation",
    "href": "Skript_4.4.html#interpretation",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation\nAus der Visualisierung der unterschiedlichen Stichprobengrößen sollte ersichtlich geworden sein, warum ausreichend große Stichproben in der Statistik so wichtig sind. Erst ab einer ausreichend großen Anzahl Messwerten kann überhaupt sicher gesagt werden, ob die Daten einer bestimmten Verteilung folgen.\nWenn man weiß, dass ein bestimmtes Merkmal normalverteilt ist, kann man das nutzen, um mit relativ wenig Aufwand Berechnungen anzustellen. Angenommen, für unsere Einkommensdaten träfe das auch zu, dann könnte man z.B. mithilfe der Normalverteilung ausrechnen, wieviel Prozent der Population theoretisch über maximal 1500 Euro Netto-Einkommen im Monat verfügen. Dazu verwenden wir die Verteilungsfunktion der Normalverteilung, da wir an einem kumulierten Wert interessiert sind, nämlich allen möglichen Einkommenswerten bis maximal 1500 Euro. Wir übergeben der Funktion Mittelwert und Standardabweichung unserer Daten sowie die besagte Einkommensobergrenze:\n\nincome_mean = mean(allbus_df$Einkommen) \nincome_sd = sd(allbus_df$Einkommen)\nincome_median = median(allbus_df$Einkommen)\n# Die Verteilungsfunktion der Normalverteilung: \"p\" + \"norm\": \npnorm(1500, mean=income_mean, sd=income_sd) \n\n[1] 0.2786057\n\n\nWir bekommen als Ergebnis ca. 0.278. Das vergleichen wir mit dem 27,8%-Quantil unserer Messwerte:\n\nquantile(allbus_df$Einkommen, probs = c(0.278))\n\n27.8% \n 1500 \n\n\nZur Erinnerung: Das 27,8%-Quantil teilt die untersten 27,8% vom Rest der Messwerte. Interessanterweise liegt die Grenze genau bei 1500 Euro, was exakt der Vorhersage entspricht. Die Übereinstimmung wird aber deutlich schwächer, wenn wir das mit dem (100-27,8)%-Quantil vergleichen:\n\nq = quantile(allbus_df$Einkommen, probs = c(1 - 0.278))\nq\n\n   72.2% \n2977.378 \n\n\n\npnorm(2977.378, mean=income_mean, sd=income_sd)\n\n[1] 0.6294329\n\n\nUnsere Messwerte ergeben für das 72,2%-Quantil einen Wert von ca. 2977 Euro. Kalkulieren wir für das selbe Quantil bei der Normalverteilung das zu erwartende Einkommen, werden uns dagegen ca. 3393 Euro angegeben:\n\nqnorm(0.722, mean=income_mean, sd=income_sd)\n\n[1] 3393.598\n\n\nDie Datenlage weicht folglich um ca. 416 Euro von der theoretischen Annahme ab und fällt deutlich geringer aus. Ein Umstand, der sich auch grafisch widerspiegelt, wenn wir die Normalverteilung in unser Histogramm einzeichnen: Die blaue Kurve markiert den Median als Mittelwert, die rote das arithmetische Mittel, die gelbe das geometrische Mittel.\n\nggplot(allbus_df, aes(x = Einkommen + 42)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = income_mean, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = income_median, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\nstat_function(fun=dnorm, args=list(mean=income_mean, sd=income_sd), colour=\"red\") +   # Einfügen der Normalcerteilung mit arithm. mittel\nstat_function(fun=dnorm, args=list(mean=income_median, sd=income_sd), colour=\"blue\") +  # Einfügen der NV mit Median\n  stat_function(fun=dnorm, args=list(mean=geom_mean(allbus_df$Einkommen + 42), sd=income_sd), colour=\"yellow\") +\nlabs(x = \"Einkommen\", y = \"Häufigkeit\")\n\n\n\n\nMan sieht darin zweierlei: Zum einen gibt es zwar eine gewisse Übereinstimmung, aber doch auch deutliche Unterschiede zwischen den Messwerten und der Normalverteilung und zum zweiten wirken Median und geometrisches Mittel etwas genauer als das arithmetische Mittel. Hier zeigt sich die stärkere Robustheit des Medians gegenüber Ausreißern.\nEine andere Möglichkeit, die Daten mit visuellen Methoden auf Normalverteilung zu prüfen, ist ein sogenannter Q-Q-Plot. Dabei werden die Quantile der theoretischen Verteilung auf einer Achse aufgetragen und die dazu korrespondierenden Quantile der empirischen Daten auf der anderen. Sind die Daten normalverteilt, sollten die Punkte im Plot alle sehr dicht an einer gemeinsamen Linie liegen. Glücklicherweise gibt es auch für diesen Plot eine Funktion:\n\n#shapiro.test(allbus_messniveau_bsp$di01a)\nqqnorm(allbus_df$Einkommen) # Erstellen des Q-Q-Plots\nqqline(allbus_df$Einkommen) # Einfügen der Linie, auf der die Punkte liegen sollten\n\n\n\n\nWie leicht zu sehen ist, weichen die Daten sehr stark von der Linie ab. Der Bereich unterhalb von ca. 3000 Euro Einkommen scheint zumindest teilweise als normalverteilt interpretierbar zu sein, während für höhere Werte die Ergebnisse massiv abweichen. Allerdings muss auch dazugesagt werden, dass ab 5000 Euro aufwärts die Anzahl an Messwerten deutlich abnimmt.\nEine weitere Möglichkeit, auf Normalverteilung zu testen, sind der Kolmogorov-Smirnov-Test und der Shapiro-Wilk-Test. Diese gehen von der Null-Hypothese aus, dass die Daten normalverteilt sind. Wenn der p-Wert also nahe bei 1 liegt, kann man davon ausgehen, dass die Hypothese bestätigt ist. Wenn der p-Wert gegen 0 geht, deutet das darauf hin, dass die Daten nicht normalverteilt sind. Allerdings sind beide Verfahren nicht unproblematisch, da sie mit zunehmender Stichprobengröße immer anfälliger für Ausreißer werden und die Null-Hypothese eher ablehnen. Der Shapiro-Wilk-Test wird vor allem bei kleinen Stichproben (n &lt; 50) eingesetzt.\nZur Demonstration der R-Funktionalitäten sind die beiden Tests hier aufgeführt, wobei die Größe der Stichprobe eine Ablehnung der Null-Hypothese erwarten lässt. Zunächst der Kolmogorov-Smirnov-Test. Es werden an die Funktion übergeben: Die Stichprobe, die Art der Verteilung, auf die getestet werden soll (hier: die Normalverteilung), der Mittelwert sowie die Standardabweichung der Stichprobe:\n\nks.test(allbus_df$Einkommen, \"pnorm\", mean=mean(allbus_df$Einkommen), sd=sd(allbus_df$Einkommen))\n\nWarning in ks.test.default(allbus_df$Einkommen, \"pnorm\", mean =\nmean(allbus_df$Einkommen), : für den Komogorov-Smirnov-Test sollten keine\nBindungen vorhanden sein\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  allbus_df$Einkommen\nD = 0.14598, p-value = 3.442e-15\nalternative hypothesis: two-sided\n\n\nDie Warnung macht uns darauf aufmerksam, dass manche Messwerte im Datensatz mehrfach vorkommen. Der p-Wert ist extrem klein, es wird also angenommen, dass die Daten nicht normalverteilt sind.\nAnalog dazu der Shapiro-Wilk-Test:\n\nshapiro.test(allbus_df$Einkommen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_df$Einkommen\nW = 0.83606, p-value &lt; 2.2e-16\n\n\nAuch hier ist der p-Wert extrem klein, weshalb auch hier die Annahme abgelehnt wird, dass die Daten normalverteilt sind.\nUm die Problematik dieser beiden Tests zu demonstrieren, wird im folgenden Code-Beispiel eine kleine zufällige Teil-Stichprobe aus dem Einkommens-Datensatz gezogen. Führen Sie es mehrmals aus und schauen Sie sich an, wie stark der p-Wert schwankt:\n\nsample_einkommen = sample(allbus_df$Einkommen, size=20)\nks.test(sample_einkommen, \"pnorm\", mean=mean(allbus_df$Einkommen), sd=sd(allbus_df$Einkommen))\n\nWarning in ks.test.default(sample_einkommen, \"pnorm\", mean =\nmean(allbus_df$Einkommen), : für den Komogorov-Smirnov-Test sollten keine\nBindungen vorhanden sein\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  sample_einkommen\nD = 0.16526, p-value = 0.6456\nalternative hypothesis: two-sided\n\nshapiro.test(sample_einkommen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sample_einkommen\nW = 0.89098, p-value = 0.02803"
  },
  {
    "objectID": "Skript_4.4.html#berechnung-1",
    "href": "Skript_4.4.html#berechnung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-1",
    "href": "Skript_4.4.html#visualisierung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-1",
    "href": "Skript_4.4.html#interpretation-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_5.1.html",
    "href": "Skript_5.1.html",
    "title": "Die Messung von latenten Variablen",
    "section": "",
    "text": "Eine überfüllte & scheinbar unstrukturierte Stadt an der Oberfläche, Bild generiert von Midjourney\n\n\nIn der Kommunikationswissenschaft, beziehungsweise in den Sozialwissenschaften allgemein, bezieht sich der Begriff latente Variable auf eine nicht direkt beobachtbare Eigenschaft oder einen nicht direkt messbaren Faktor, der sich jedoch durch mehrere beobachtbare Variablen (teilweise auch als Indikatoren bezeichnet) manifestiert. Latente Variablen sind theoretische Konstrukte, die nicht direkt gemessen werden können. Als Wissenschaftler:innen gehen wir allerdings davon aus, dass diese Konstrukte existieren und wir somit komplexe gesellschaftliche Phänomene mit der Hilfe von latenten Variablen besser verstehen können.\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\nWenn wir uns jetzt ganz konkret mit dem Vertrauen in gesellschaftliche Institutionen beschäftigen, ist Vertrauen eine latente Variable. Wir können Vertrauen in gesellschaftliche Institutionen nicht direkt beobachten, wir sehen beispielsweise einem Menschen nicht an, ob er oder sie dem Bundesverfassungsgericht als eine von mehreren gesellschaftlichen Institutionen stark oder nicht so stark vertraut. Wenn wir uns als Forscher:innen fragen, inwieweit die Menschen in Deutschland den gesellschaftlichen Institutionen vertrauen, müssen wir zunächst theoretisch klären was wir unter dem Vertrauen in gesellschaftliche Institutionen verstehen.\nDas Vertrauen in gesellschaftliche Institutionen ist ein abstraktes und komplexes theoretisches Konstrukt, das sich aus verschiedenen Bestandteilen zusammensetzt. Diese einzelnen Bestandteile (Vertrauen in das Bundesverfassungsgericht, Vertrauen in den Bundestag, etc.) können wir durch sogenannte Indikatoren beobachten und damit messbar machen (in einer Befragung wäre das bspw. die Frage nach dem Vertrauen in das Bundesverfassungsgericht). Auf der Grundlage dieser Indikatoren versuchen wir dann auf das Vertrauen in gesellschaftliche Institutionen zu schließen.\nLatente Variablen spielen eine wichtige Rolle in der statistischen Modellierung und Analyse, insbesondere in der Faktorenanalyse. Durch das Einbeziehen von latenten Variablen in die Analyse, können komplexe Beziehungen und Zusammenhänge zwischen verschiedenen Variablen besser verstanden und erklärt werden. Zusätzlich ist eine Reduktion von Komplexität möglich, indem mehrere beobachtbare Variablen (bzw. Indikatoren) in einer latenten Variable zusammengefasst werden, was zu einem besseren Verständnis der zugrunde liegenden Phänomene führen kann. Der Prozess des Zusammenfassens wird häufig auch als Indexbildung bezeichnet.\nDie Indexbildung umfasst drei relevante Schritte: (1) Zunächst muss ein theoretisches Konstrukt entwickelt werden. In unserem Fall würde das bedeuten, dass wir ein theoretisches Verständnis von Vertrauen in gesellschaftliche Institutionen entwickeln und uns klar werden was wir darunter verstehen. (2) Im nächsten Schritt müssten wir klären, ob sich dieses theoretisches Konstrukt in Form einer latenten Variable auch in unseren Daten finden lässt, hierfür werden wir eine explorative Faktorenanalyse durchführen. (3) Als letzten Schritt führen wir die eigentliche Indexbidlung durch. Hier berechnen wir eine neue Variable, den Index für Vertrauen in gesellschaftliche Institutionen und Überprüfen dessen Güte."
  },
  {
    "objectID": "Skript_5.2.html",
    "href": "Skript_5.2.html",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Das Entdecken der zugrunde liegenden Struktur der Stadt, Bild generiert von Midjourney\nWir möchten - wie bereits in Kaptitel 5.1 angesprochen - ein Index für die von uns angenommene latente Variable Vertrauen in gesellschaftliche Institutionen bilden. An dieser Stelle müssten wir uns zunächst Gedanken über das Konstrukt Vertrauen in gesellschaftliche Institutionen machen und eine theoretische Grundlage entwickeln. Wir kürzen diesen Prozess an dieser Stelle etwas ab und möchten die lesende Person ermutigen einen kurzen Blick in den Aufsatz von Nina Steindl aus dem Jahr 2019 zum Vertrauen in gesellschaftliche Institutionen von Journalist:innen zu werfen. Wenn wir eine theoretische Vorstellung entwickelt haben, können wir mit der empirische Überprüfung der latenten Variable mittels explorativer Faktorenanalyse starten.\nDie explorative Faktorenanalyse (EFA) ist eine statistischen Methoden, die dazu dient, die kleinste Anzahl hypothetischer Konstrukte (auch als Faktoren, Dimensionen, latente Variablen bezeichnet) zu ermitteln. Indem die beobachtete Kovarianz zwischen einer Reihe von Messvariablen (auch als beobachtete Variablen oder Indikatoren bezeichnet) erklärt wird. Konkret sollen die gemeinsamen Faktoren ermittelt werden, die die Struktur zwischen den gemessenen Variablen erklären. Wobei wir in den Sozialwissenschaften davon ausgehen, dass es sich bei diesen Faktoren um unbeobachtbare Merkmale - also latente Variablen - handelt.\nIn unserem Fall interessiert uns die latente Variable Vertrauen in gesellschaftliche Institutionen. Wir möchten Wissen, ob wir diese latente Variable bzw. diesen Faktor aus den einzelenen Indikatoren (den Fragen aus dem Allbus-Fragebogen) ableiten können. Konkret müssen wir überprüfen, ob sich die theoretisch angenommene latenten Variable auch in den Daten zu finden ist. Hierfür können wir die Faktorenanalyse oder Principal Component Analysis (PCA) verwenden."
  },
  {
    "objectID": "Skript_5.3.html",
    "href": "Skript_5.3.html",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Validieren, Bild generiert von Midjourney\nWir haben in den vorangegangenen Kapitel zur Faktorenanalyse festgestellt, dass es einen empirisch messbaren Faktor in unseren Daten gibt, welchen wir unter Rückgriff auf unsere theoretischen Grundlagen als Vertrauen in gesellschaftliche Institutionen interpretieren. Wir haben also die beiden ersten Ziele des Kapitels bereits erreicht und müssen nur noch die eigentliche Indexbildung sowie das Prüfen der Qualität des Index vornehmen.\nFalls ihr euch immer noch fragt was genau ein Index ist, könnte man etwas vereinfacht sagen, dass wir eine neue Variable (den Index) auf Grundlage der Indikatoren erstellen, welche die Informationen aus den Indikatoren möglichst gut bündelt. Das erleichtert uns die weitere Berechnung von statistischen Tests und Analysen und führt damit zu einer besseren Übersichtlichkeit und leichteren Interpretation der Ergebnisse. Wir müssen dann nur noch mit einer Index-Variable weiter arbeiten statt mit 15 Indikatoren.\nEntsprechend möchten wir jetzt eine neue Index-Variable für das Vertrauen in gesellschaftliche Institutionen auf Grundlage der Indikatoren bilden."
  },
  {
    "objectID": "Skript_6.1.html",
    "href": "Skript_6.1.html",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "Bild generiert von Midjourney\nBei der bivariaten Statistik schauen wir uns nicht Variablen isoliert an, sondern interessieren uns für die Beziehung von zwei Variablen zueinander. Grundsätzlich unterscheiden wir bei bivariaten Analysen in die Überprüfung von (Mittelwert-)Unterschieden und das Prüfen von Zusammenhängen. In diesem Kapitel konzentrieren wir uns auf die Überprüfung von Unterschieden. Die Überprüfung von Zusammenhängen mittels Korrelation und Regression findet sich hingegen im nächsten Kapitel."
  },
  {
    "objectID": "Skript_6.2.html",
    "href": "Skript_6.2.html",
    "title": "Bestimmen von Unterschieden in der Varianz mit Kreuztabellen und dem Chi-Quadrat Test",
    "section": "",
    "text": "Bestimmen von Unterschieden in der Varianz\n\n1 Kreuztabellen und Chi-Quadrat Test"
  },
  {
    "objectID": "Skript_6.3.html",
    "href": "Skript_6.3.html",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "Bild generiert von Midjourney\nWir nutzen den t-test um Mittelwertunterschiede zwischen zwei Gruppen zu analysieren.\nWir unterscheiden grundsätzlich in zwei verschiedene Arten von t-Test:\nt-test für unabhängige Stichproben\nMit dem t-test für unabhängige Stichproben überprüfen wir, ob sich die Mittelwerte in zwei gemessenen Stichproben unterscheiden. Die Stichproben sind dann unabhängig, wenn sie aus zwei unterschiedlichen Gruppen stammen und daher sich nicht gegenseitig beeinflussen können. Ein solches Beispiel liegt zum Beispiel dann vor, wenn wir schauen ob das Alter (dichotom kodiert in alt vs jung) einen Einfluss auf das Einkommen hat. In diesem Fall kann eine Person zum Untersuchungszeitpunkt nicht gleichzeitig alt und jung sein, sondern fällt in einer der beiden Gruppen. Ein anderes typisches Anwendungsbespiel ist der Vergleich von zwei Gruppen im Rahmen eines Experimentaldesign. Hier kann ein Proband ebenfalls nur einer der Untersuchungsgruppe zugeordnet sein.\nt-test für verbundene Stichproben\nDer t-test für verbundene Stichproben wiederum misst, inwiefern sich Mittelwerte bei denselben Personen oder Fällen verändern. Wenn wir bei unserem Beispiel mit dem Einflusses des Alters auf das Einkommen bleiben, so könnten wir dieselben Personen zu zwei Zeitpunkten befragen: einmal wenn sie jung sind und dann zu einem späteren Zeitpunkt noch einmal. In diesem Fall sind unsere Stichproben nicht unabhängig, da die Messwerte von denselben Personen stammen und ggf. der frühere Messzeitpunkt den späteren beeinflussen kann.\nDer Einstichprobentest\nZusätzlich gibt es noch den Einstichprobentest oder auch one-sample-test. Hier vergleichen wir nicht zwei Gruppenmittelwerte miteinander, sondern den Mittelwert einer Gruppe mit einem bereits bekannten feststehenden Wert. Wir können so beispielsweise überprüfen, ob das Einkommen in einer Stichprobe dem deutschlandweiten Mittelwert entspricht, sofern dieser uns vorab bekannt ist."
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben",
    "text": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben\n\n1.1.1 t-Test für unabhängige Stichproben\n\n\n1.1.2 Mann-Whitney\n\n\n1.1.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n1.1.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n\nCode\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\n\nCode\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n\n1.1.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n1.1.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n\nCode\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n\nCode\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n\nCode\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n1.1.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\nCode\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n1.1.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\nCode\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\n\nCode\nprint(fit)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n1.1.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt.\n\n\n\n1.1.4 Kruskal Wallis\n\n\n1.1.5 mehrfaktorielle Varianzanalyse\n\n1.1.5.1 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\n\nCode\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\n\nCode\nprint(fit2)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n\n1.1.5.2 Post-Hoc Tests\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n1.1.5.3 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n\nCode\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\n\n\nCode\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben",
    "text": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben\n\n1.2.1 t-Test für verbundene Stichproben\n\n\n1.2.2 Wilcoxon"
  },
  {
    "objectID": "Skript_6.4.html",
    "href": "Skript_6.4.html",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen."
  },
  {
    "objectID": "Skript_7.1.html",
    "href": "Skript_7.1.html",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "",
    "text": "Ränge und Treppen, Bild generiert von Midjourney\nIn diesem Kapitel schauen wir uns verschiedene statistische Verfahren an, um Zusammenhänge zwischen Variablen zu untersuchen. Statistische Verfahren, die den Zusammenhang zwischen Variablen überprüfen, bieten ein großes Aufklärungspotential, Muster, Beziehungen und Trends in Daten zu identifizieren und entsprechende Hypothesen zu testen.\nEs gibt verschiedene Arten von Zusammenhängen zwischen Variablen, und je nach Art des Zusammenhangs werden unterschiedliche statistische Verfahren eingesetzt. Die Wahl des jeweils angemessenen Verfahrens hängt von verschiedenen Faktoren ab, vor allem von 1) der Art der untersuchten Variablen und ihres Skalenniveaus (ob es sich also um nominale, ordinale oder metrische Variblen handelt) 2) der Art des (vermuteten) Zusammenhangs (z.B. linear, nicht-linear) sowie 3) den spezifischen Fragestellungen ab, die wir untersuchen wollen. Manchmal kann es sich auch anbieten, verschiedene Verfahren zu kombinieren, um ein umfassendes Verständnis des Zusammenhangs zu erhalten."
  },
  {
    "objectID": "Skript_7.2.html",
    "href": "Skript_7.2.html",
    "title": "Korrelation & Regression",
    "section": "",
    "text": "The Highway as regression curve, Bild generiert von Midjourney\nZur Analyse von Zusammenhängen bei zwei metrischen Variablen gibt es verschiedene statistische Verfahren. In diesem Teilkapitel schauen wir uns die Korrelationsanalyse sowie die einfache lineare Regression an."
  },
  {
    "objectID": "Skript_7.2.html#korrelation",
    "href": "Skript_7.2.html#korrelation",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.1 Korrelation",
    "text": "3.1 Korrelation"
  },
  {
    "objectID": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "href": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse",
    "text": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse\nIn diesem Notebook wird die einfache lineare Regression auf Grundlage der ESS-Daten vorgestellt. In der nächsten Sitzung gehen wir näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen die multiple lineare Regression kennen.\n\n\n\nPicture generated by Midjourney\n\n\n\n3.2.1 Anwendungsbereich der linearen Regression\nDie einfache lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen zwei metrischen Variablen besteht. Sie wird daher auch als bivariate Regression bezeichnet.\nZiel ist es, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?\n\n\n3.2.2 Vorbereitung und Laden der Daten\nZunächst laden wir die Pakete des tidyverse. Weiterhin laden wir das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt.\nDen Datensatz finet ihr hier.\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\") \n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n3.2.3 Ziel der Analyse\nMit Hilfe der Regression wollen wir die Annahme prüfen, dass das Alter (agea) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) hat. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n3.2.4 Data Management\nDamit der Output etwas nachvollziehbarer wird, benenne ich die Variablen mit dem rename-Befehl zunächst einmal um. Dann nutze ich den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifiziere ich, auf welche Variablen sich der Befehl beziehen soll). Das alles weise ich einem neuen Datenobjekt zu: daten_mod\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm) %&gt;% \n  drop_na(c(alter, internetnutzung)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gndr   alter marsts   edubde1 eduade2 eduade3 nwspol netusoft\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1       835 FR    Male      16 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 2  10006306 DE    Female    39 None of… Abitur… Diplom… Kein b…     25 Every d…\n 3  10008723 DE    Male      16 None of… (Noch)… Kein H… Kein b…     30 Most da…\n 4       231 SE    Female    80 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       270 Most da…\n 5 100003876 GB    Male      33 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 6       408 FR    Female    30 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 7  10009322 DE    Male      72 &lt;NA&gt;     Abitur… Diplom… Kein b…     90 Every d…\n 8 100003453 GB    Male      45 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 9      1126 SE    Female    60 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n10 100001390 GB    Female    77 Widowed… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        10 Every d…\n# ℹ 90 more rows\n# ℹ 156 more variables: internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;,\n#   pplhlp &lt;dbl&gt;, polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;,\n#   psppipla &lt;fct&gt;, cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;,\n#   trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;, trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;,\n#   vote &lt;fct&gt;, prtvede1 &lt;fct&gt;, prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;,\n#   wrkorg &lt;fct&gt;, badge &lt;fct&gt;, sgnptit &lt;fct&gt;, pbldmn &lt;fct&gt;, bctprd &lt;fct&gt;, …\n\n\n\n\n3.2.5 Prüfung der Voraussetzungen 1: Grafische Darstellung des Zusammenhangs der beiden Variablen, um die Annahme von Linearität zu prüfen\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und Internetnutzung. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Internetznutzung) und x (=Alter) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und Internetnutzung\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")\n\n\n\n\n\n\n3.2.6 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen schwachen negativen (aber linearen!) Zusammenhang zwischen Alter und Internetnutzung nahe: mit zunehmendem Alter sinkt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\nNicht wundern: Weil wir oben ein Zufallssample gezogen haben, sieht die Grafik bei Ihnen allen etwas anders aus. Sie kann dadurch auch so ausfallen, dass der lineare Zusammenhang nicht (gut) sichtbar ist – vor allem dann, wenn Ausreißer das Ergebnis massiv verzerren (z.B. wenn ein oder zwei ältere Nutzer mit [unrealistisch?] hoher Nutzungsdauer in ihrer Zufallstichprobe gelandet sind).\n\n\n3.2.7 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regressionsanalyse prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: Internetznutzung), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (Internetnutzung) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n3.2.8 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nmodel\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nCoefficients:\n(Intercept)        alter  \n    378.194       -3.982  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  378.194     59.747   6.330 0.0000000074 ***\nalter         -3.982      1.198  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.9 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängige Variable “alter” zu erklären.\n\n3.2.9.1 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n3.2.9.2 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n3.2.9.3 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n3.2.9.4 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n3.2.9.5 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n3.2.9.6 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (14,7) Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n3.2.9.7 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n3.2.9.8 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\n3.2.10 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (z.B. -4.642) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Die UV beeinflusst die AV, R2 = .24, F(1, 116) = 4.71, p = .003.\n\n\n\n\n\n3.2.11 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (Internetnutzung) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\npredict.lm(model, data.frame(alter = 25))\n\n       1 \n278.6328 \n\npredict.lm(model, data.frame(alter = 75))\n\n       1 \n79.51086 \n\n\n\n\n3.2.12 Inhaltliche Interpretation\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von (306) Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von (74) Minuten auf. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(alter = c(25, 75)))\n\n        1         2 \n278.63275  79.51086 \n\n\n\n\n3.2.13 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable Internetkonsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen Internetkonsum von (X) Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\nfitted(model) \n\n        1         2         3         4         5         6         7         8 \n314.47470 222.87862 314.47470  59.59867 246.77325 258.72057  91.45818 198.98400 \n        9        10        11        12        13        14        15        16 \n139.24743  71.54599 163.14206 195.00156 254.73813 242.79081 306.50982 175.08937 \n       17        18        19        20        21        22        23        24 \n131.28255 131.28255 155.17718 218.89619 111.37036 202.96644 119.33524  99.42305 \n       25        26        27        28        29        30        31        32 \n242.79081 274.65032 103.40549 103.40549 155.17718  99.42305 210.93131 167.12449 \n       33        34        35        36        37        38        39        40 \n310.49226 179.07181 107.38793 131.28255 222.87862  67.56355 198.98400 302.52738 \n       41        42        43        44        45        46        47        48 \n270.66788 278.63275 187.03668  99.42305 234.82594 226.86106 246.77325 262.70300 \n       49        50        51        52        53        54        55        56 \n115.35280  71.54599 163.14206 274.65032 155.17718 163.14206 187.03668 163.14206 \n       57        58        59        60        61        62        63        64 \n234.82594 226.86106  99.42305 250.75569 115.35280 175.08937 183.05425 202.96644 \n       65        66        67        68        69        70        71        72 \n159.15962 123.31768 123.31768 143.22987 155.17718 254.73813 242.79081 278.63275 \n       73        74        75        76        77        78        79        80 \n210.93131 151.19474 270.66788 234.82594 191.01912 119.33524 310.49226 226.86106 \n       81        82        83        84        85        86        87        88 \n159.15962 206.94887 270.66788 250.75569 147.21231 238.80838 230.84350 103.40549 \n       89        90        91        92        93        94        95        96 \n147.21231 159.15962 167.12449 151.19474 302.52738 210.93131 187.03668 119.33524 \n       97        98        99       100 \n175.08937 266.68544 302.52738 198.98400 \n\n\nNun haben wir aber im Rahmen unserer Befragung die Internetnutzung der Befragten aber ja schon erhoben. Wozu dient das dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nresiduals.lm(model)\n\n          1           2           3           4           5           6 \n-194.474695 -162.878625 -269.474695  -29.598673 -126.773252 -228.720565 \n          7           8           9          10          11          12 \n -31.458176  -78.983998 -109.247430  -11.545986  -43.142057  404.998440 \n         13          14          15          16          17          18 \n-194.738127  237.209186  -96.509820  304.910630  -41.282554  -71.282554 \n         19          20          21          22          23          24 \n -35.177181  -38.896187  -51.370365 -142.966435   90.664759  -49.423051 \n         25          26          27          28          29          30 \n-240.790814   25.349683  -13.405489   16.594511 -125.177181  -84.423051 \n         31          32          33          34          35          36 \n -30.931311  132.875505 -130.492257  -89.071808   72.612073  -11.282554 \n         37          38          39          40          41          42 \n -42.878625  112.436451  401.016002   -2.527382  629.332121  921.367245 \n         43          44          45          46          47          48 \n 112.963316  -84.423051 -114.825938 -136.861063  353.226748   37.296997 \n         49          50          51          52          53          54 \n 184.647197  108.454014   16.857943  385.349683 -125.177181 -133.142057 \n         55          56          57          58          59          60 \n-172.036684  -73.142057  365.174062 -166.861063  -39.423051 -205.755690 \n         61          62          63          64          65          66 \n 184.647197 -115.089370  -63.054246  -62.966435  -99.159619  176.682322 \n         67          68          69          70          71          72 \n-108.317678 -118.229868  -35.177181  -74.738127  -32.790814   81.367245 \n         73          74          75          76          77          78 \n-180.931311  -31.194743  449.332121 -114.825938  348.980878  -29.335241 \n         79          80          81          82          83          84 \n-280.492257 -166.861063  -99.159619  -86.948873  -30.667879  -10.755690 \n         85          86          87          88          89          90 \n -27.212306 -118.808376 -200.843500  -73.405489   92.787694 -114.159619 \n         91          92          93          94          95          96 \n  12.875505  328.805257 -212.527382   29.068689 -127.036684  120.664759 \n         97          98          99         100 \n-165.089370  273.314559  -92.527382 -108.983998 \n\n\n\n\n3.2.14 Inhaltliche Interpretation\nFür unseren Fall Nummer 3 beträgt die Abweichung der Prognose von der Beobachtung (121) Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 15 Prozent nicht besonders groß ist.\n\n\n3.2.15 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten_mod$vorhersage &lt;- predict(model) \ndaten_mod$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n  geom_segment(aes(xend = alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Internetnutzung\") + # Titel\n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\") # Achsen-Beschriftung\n\n\n\n\n\n\n3.2.16 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Standardized Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 378.1937           NA    59.7466   6.330 0.0000000074 ***\nalter        -3.9824      -0.3183     1.1979  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.17 Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   378.         NA         59.7       6.33 0.00000000740\n2 alter          -3.98       -0.318      1.20     -3.32 0.00125      \n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.101        0.0922  198.      11.1 0.00125     1  -670. 1346. 1353.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model)\n\n# A tibble: 100 × 8\n   internetnutzung alter .fitted .resid   .hat .sigma   .cooksd .std.resid\n             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1             120    16   314.  -194.  0.0452   198. 0.0239       -1.00  \n 2              60    39   223.  -163.  0.0124   199. 0.00428      -0.827 \n 3              45    16   314.  -269.  0.0452   197. 0.0458       -1.39  \n 4              30    80    59.6  -29.6 0.0496   199. 0.000613     -0.153 \n 5             120    33   247.  -127.  0.0172   199. 0.00364      -0.645 \n 6              30    30   259.  -229.  0.0206   198. 0.0143       -1.17  \n 7              60    72    91.5  -31.5 0.0327   199. 0.000441     -0.161 \n 8             120    45   199.   -79.0 0.0102   199. 0.000823     -0.400 \n 9              30    60   139.  -109.  0.0161   199. 0.00253      -0.556 \n10              60    77    71.5  -11.5 0.0428   199. 0.0000792    -0.0595\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "href": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden",
    "text": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.4 Prüfung der Voraussetzungen",
    "text": "3.4 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "href": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.5 Berechnung und Interpretation einer einfachen linearen Regression",
    "text": "3.5 Berechnung und Interpretation einer einfachen linearen Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\nHerzlich willkommen auf der Startseite unserer Website.\nDiese Website ist ein multimediales Online-Lehrbuch und begleitet die Lehrveranstaltung Statistik des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen. Entstanden ist die Website aus einer Kooperation des Zentrum für Medien, Kommunikations- und Informationsforschung und der Förderlinie Skill-Projekte der Universität Bremen.\nIn diesem Kurs führen wir schrittweise durch die Grundlagen der quantitativen Forschungsdesigns. Diese Webseite bündelt hierbei das für den Kurs bentötigte Wissen sowie alle benötigten Kurs-Materialien. Ziel des Kurses ist es, eigenständig ein Forschungsprojekt - vom Einlesen der Daten über das Datenmanagement bis zur statistischen Analyse - durchführen zu können.\nWir freuen uns darauf, Sie in unserem Kurs begrüßen zu dürfen!"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "href": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.1 t-Test für unabhängige Stichproben",
    "text": "2.1 t-Test für unabhängige Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#mann-whitney",
    "href": "Skript_6.3.html#mann-whitney",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.2 Mann-Whitney",
    "text": "2.2 Mann-Whitney"
  },
  {
    "objectID": "Skript_6.3.html#die-varianzanalyse",
    "href": "Skript_6.3.html#die-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.3 Die Varianzanalyse",
    "text": "2.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n2.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n2.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n2.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n2.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n2.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n2.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.3.html#kruskal-wallis",
    "href": "Skript_6.3.html#kruskal-wallis",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.4 Kruskal Wallis",
    "text": "2.4 Kruskal Wallis"
  },
  {
    "objectID": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.5 Mehrfaktorielle Varianzanalyse",
    "text": "2.5 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n2.5.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n2.5.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "href": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.1 t-Test für verbundene Stichproben",
    "text": "3.1 t-Test für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#wilcoxon",
    "href": "Skript_6.3.html#wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.2 Wilcoxon",
    "text": "3.2 Wilcoxon"
  },
  {
    "objectID": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "href": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.2 Multiple Regression mit Dummy-Codierung",
    "text": "1.2 Multiple Regression mit Dummy-Codierung\n\n1.2.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Geschlecht und Internetnutzung\nNun wollen wir noch Geschlecht (gndr) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei Gender haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist! Es handelt sich vielmehr um eine kategoriale Variable. Wie Sie schon gelernt haben, können Sie diese mit einem “Trick” ebenfalls in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Wir wollen uns hier mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl erst einmal umcodieren.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n1.2.2 Dummy Codierung der Variable Gender\n\ndaten_mod2 &lt;- daten_mod %&gt;%\nmutate(gender_r  = recode(gender, 'Male'='0', 'Female'='1')) %&gt;% # Recodierung der Var Gender zur Dummy-Variable\n  mutate(gender_r = as.numeric(as.character(gender_r))) # Variable als numerischen Wert behandeln\ndaten_mod2\n\n# A tibble: 100 × 167\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 159 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\ntable(daten_mod2$gender_r)\n\n\n 0  1 \n56 44 \n\nsummary(daten_mod2$gender_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    0.44    1.00    1.00 \n\nclass(daten_mod2$gender_r)\n\n[1] \"numeric\"\n\n\n\n\n1.2.3 Regressionsmodell zum Zusammenhang von Alter, Nachrichtenrezeptiion, Geschlecht und Internetnutzung spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable gender_r ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel_m2 &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption + gender_r, data = daten_mod2)\nsummary(lm.beta(model_m2))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption + \n    gender_r, data = daten_mod2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-202.12  -94.50  -51.31   32.24  471.47 \n\nCoefficients:\n                                 Estimate Standardized Std. Error t value\n(Intercept)                     265.27637           NA   45.47227   5.834\nalter                            -1.93149     -0.20696    0.92960  -2.078\npolitische_Nachrichtenrezeption   0.26651      0.20918    0.12545   2.124\ngender_r                        -18.36732     -0.06002   30.78140  -0.597\n                                   Pr(&gt;|t|)    \n(Intercept)                     0.000000073 ***\nalter                                0.0404 *  \npolitische_Nachrichtenrezeption      0.0362 *  \ngender_r                             0.5521    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147.5 on 96 degrees of freedom\nMultiple R-squared:  0.09507,   Adjusted R-squared:  0.06679 \nF-statistic: 3.362 on 3 and 96 DF,  p-value: 0.02189\n\n\n\n\n1.2.4 Inhaltliche Interpretation\nGender hat hier keinen signifikanten Einfluss auf den Internetkonsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy- Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (9,8) Minuten geringeren Internetkonsum als Männer (wobei dieser Befund statistisch ja (nicht) signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.5 Vorhersage des multivariaten Modells für die tägliche Internetnutzung durch Alter, Nachrichtenrezeption und Geschlecht\n\npredict.lm(model_m2, data.frame(alter = c(25, 75), gender_r = c(0,1), politische_Nachrichtenrezeption = c(5, 10)))\n\n       1        2 \n218.3217 104.7124 \n\n\n\n\n1.2.6 Inhaltliche Interpretation\nEine männliche Person, die 25 Jahre alt ist und 5 Minuten pro Tag politische Nachrichten rezipiert hat, einen prognostizierten Internetkonsum von 293 Minuten. Eine weibliche Person, die 75 Jahre alt ist und ebenfalls 5 Minuten pro Tag politische Nachrichten rezipiert, hat einen prognostizierten Internetkonsum von 41 Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.7 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie diese ausführen, haben Sie ja heute zu Anfang der Sitzung gelernt (siehe oben). Zusätzlich müssen Sie bei einer multiplen Regresssion noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model_m2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                            Term  VIF    VIF 95% CI Increased SE Tolerance\n                           alter 1.05 [1.00,  3.50]         1.03      0.95\n politische_Nachrichtenrezeption 1.03 [1.00, 26.71]         1.01      0.97\n                        gender_r 1.07 [1.00,  2.29]         1.04      0.93\n Tolerance 95% CI\n     [0.29, 1.00]\n     [0.04, 1.00]\n     [0.44, 1.00]\n\n\n\n\n1.2.8 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5; wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "href": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse",
    "text": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse\n\ndaten_mod3 &lt;- daten %&gt;% \n  select(agea, netustm, cntry) %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm,\n         land = cntry) %&gt;% \n  filter(land %in% c(\"DE\", \"FR\", \"IS\", \"PL\")) %&gt;% \n  drop_na() %&gt;% \n  group_by(land) %&gt;% \n  slice_sample(n = 100) %&gt;% \n  ungroup()\ndaten_mod3\n\n# A tibble: 200 × 3\n   alter internetnutzung land \n   &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;\n 1    59             240 DE   \n 2    35              60 DE   \n 3    38             240 DE   \n 4    20             240 DE   \n 5    51              60 DE   \n 6    64             150 DE   \n 7    60              30 DE   \n 8    28             540 DE   \n 9    21             300 DE   \n10    53              45 DE   \n# ℹ 190 more rows\n\n\n\nggplot(daten_mod3, aes(alter, internetnutzung, colour = land)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(method = lm, formula = \"y ~ x\", se = FALSE) + \n  scale_colour_brewer(palette = \"Set1\") + \n  ggtitle(\"Lineare Regression für Alter und Internetnutzung (vier Länder)\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")"
  },
  {
    "objectID": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "href": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.3 Literatur und Beispiele aus der Praxis",
    "text": "3.3 Literatur und Beispiele aus der Praxis\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link\n\n:::"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\nHerzlich willkommen auf der Startseite unserer Website.\nDiese Website ist ein multimediales Online-Lehrbuch und begleitet die Lehrveranstaltung Statistik des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen. Entstanden ist die Website aus einer Kooperation des Zentrum für Medien, Kommunikations- und Informationsforschung und der Förderlinie Skill-Projekte der Universität Bremen.\nIn diesem Kurs führen wir schrittweise durch die Grundlagen der quantitativen Forschungsdesigns. Diese Webseite bündelt hierbei das für den Kurs bentötigte Wissen sowie alle benötigten Kurs-Materialien. Ziel des Kurses ist es, eigenständig ein Forschungsprojekt - vom Einlesen der Daten über das Datenmanagement bis zur statistischen Analyse - durchführen zu können.\nWir freuen uns euch in unserem Kurs begrüßen zu dürfen!"
  },
  {
    "objectID": "Autoren.html",
    "href": "Autoren.html",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Vita\n\n\n\n\n\nPatrick Zerrer\n\n\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Er schloss den „B.A. Governance and Public Policy - Staatswissenschaften” an der Universität Passau sowie den „M.A. Öffentliche Kommunikation” an der Friedrich-Schiller-Universität ab. Seine Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation, der Nachrichten- und Mediennutzung sowie der politischen Partizipation mit einem Fokus auf (digitalen) Klimaprotest und -bewegungen. Für seine Forschung nutzer er unterschiedliche Methoden, u.a. digitales (mobiles) Tracking, digitale Spurdaten, Umfragen, (automatisierter) Inhaltsanalysen und Interviews.\nForschungsschwerpunkte\n\nPolitische Kommunikation\n(mobile) Mediennutzungsforschung\nKlimaprotestforschung\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Autoren.html#patrick-zerrer",
    "href": "Autoren.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Vita\n\n\n\n\n\nPatrick Zerrer\n\n\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Er schloss den „B.A. Governance and Public Policy - Staatswissenschaften” an der Universität Passau sowie den „M.A. Öffentliche Kommunikation” an der Friedrich-Schiller-Universität ab. Seine Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation, der Nachrichten- und Mediennutzung sowie der politischen Partizipation mit einem Fokus auf (digitalen) Klimaprotest und -bewegungen. Für seine Forschung nutzer er unterschiedliche Methoden, u.a. digitales (mobiles) Tracking, digitale Spurdaten, Umfragen, (automatisierter) Inhaltsanalysen und Interviews.\nForschungsschwerpunkte\n\nPolitische Kommunikation\n(mobile) Mediennutzungsforschung\nKlimaprotestforschung\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Hintergrund.html",
    "href": "Hintergrund.html",
    "title": "Lernen durch Praxis - Der Weg zu fundierten statistischen Kenntnissen",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nDie Grundidee unseres Kurses besteht darin, praktisches Lernen, grundlegende theoretische Kenntnisse und eigenständiges Forschen zu kombinieren. Ziel ist es dabei, dass im Anschluss an den Kurs eigenständig quantitative Forschungsdesigns mit R umgesetzt werden können.\nDer Kurs beginnt dabei mit den Grundlagen von R und dem Aufbau von RStudio und vermittelt die notwendigen Kompetenzen um mit R erfolgreich arbeiten zu können. Anschließend schauen wir uns das Einlesen von Datensätzen sowie Grundlagen der Datenmanipulation an. Dies alles ist Teil unseres Data Bootcamps 🎖️.\nAnschließend wenden wir uns statistischen Analysen zu, welche schrittweise auf die Umsetzung eines eigenen Forschungsprojektes vorbereiten sollen. Wir schauen uns an, wie wir Hypothesen aufstellen, mit verschiedenen Datenstrukturen umgehen, diese mit verschiedenen Verfahren analysieren und im Anschluss die Ergebnisse korrekt interpretieren.\nWir freuen uns darauf Sie bei dieser Lernreise zu begleiten und Sie bei dem Erwerben von statistischen Kenntnissen, analytischen Fähigkeiten und praktischer Erfahrungen zu unterstützen. Wir sind davon überzeugt, dass Sie durch unser praxisorientieres Lernkonzept einen guten und informativen Einstieg in die Welt der quantitativen Forschung haben werden."
  },
  {
    "objectID": "Hintergrund.html#patrick-zerrer",
    "href": "Hintergrund.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Patrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften\". Das Masterstudium „Öffentliche Kommunikation\" an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action\" mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik\" mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_5.2.html#laden-der-nötigen-pakete",
    "href": "Skript_5.2.html#laden-der-nötigen-pakete",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "library(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.2.html#laden-des-datensatzes",
    "href": "Skript_5.2.html#laden-des-datensatzes",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "daten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "href": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "title": "Die Faktorenanalyse",
    "section": "1.2 Deskriptive Statistik für den Teildatensatz",
    "text": "1.2 Deskriptive Statistik für den Teildatensatz\nWir werfen einen kurzen Blick in die deskriptive Statistik für unseren Teildatensatz, um ein besseres Verständnis für die Daten zu erhalten.\n\n1summary(allbus_vertrauen)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n Ver_Gesundheitswesen   Ver_BVerfG    Ver_Bundestag   Ver_Verwaltung \n Min.   :1.000        Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000        1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000  \n Median :5.000        Median :6.000   Median :4.000   Median :5.000  \n Mean   :4.939        Mean   :5.255   Mean   :4.058   Mean   :4.482  \n 3rd Qu.:6.000        3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000        Max.   :7.000   Max.   :7.000   Max.   :7.000  \n Ver_kath_Kirche Ver_evan_Kirche   Ver_Justiz        Ver_TV     \n Min.   :1.000   Min.   :1.00    Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.00    1st Qu.:4.000   1st Qu.:3.000  \n Median :2.000   Median :3.00    Median :5.000   Median :4.000  \n Mean   :2.331   Mean   :3.04    Mean   :4.581   Mean   :3.577  \n 3rd Qu.:3.000   3rd Qu.:4.00    3rd Qu.:6.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.00    Max.   :7.000   Max.   :7.000  \n  Ver_Zeitung       Ver_Uni      Ver_Regierung    Ver_Polizei     Ver_Parteien \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000   1st Qu.:2.00  \n Median :4.000   Median :5.000   Median :4.000   Median :5.000   Median :3.00  \n Mean   :4.012   Mean   :5.203   Mean   :4.054   Mean   :4.948   Mean   :3.19  \n 3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:4.00  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00  \n   Ver_Kom_EU      Ver_EU_Par   \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000  \n Median :4.000   Median :4.000  \n Mean   :3.515   Mean   :3.556  \n 3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000"
  },
  {
    "objectID": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "href": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Hauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben."
  },
  {
    "objectID": "Skript_5.2.html#quellen-für-das-script",
    "href": "Skript_5.2.html#quellen-für-das-script",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Stephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html#laden-der-nötigen-pakete",
    "href": "Skript_5.3.html#laden-der-nötigen-pakete",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Laden der nötigen Pakete",
    "text": "1.1 Laden der nötigen Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, psych, dplyr, htmlTable)\n\nUnd laden im Anschluss den notwendigen Datensatz."
  },
  {
    "objectID": "Skript_5.3.html#laden-des-datensatzes",
    "href": "Skript_5.3.html#laden-des-datensatzes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Laden des Datensatzes",
    "text": "1.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Teildatensatz mit den benötigten Index-Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Index-Variablen\nWir greifen natürlich auf die gleiche Datengrundlage zurück, welche wir auch für die Faktorenanalyse verwendet haben. Was in unserem Fall bedeutet, dass wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nWir bereiten die Daten entsprechend vor, indem wir die fehlenden Werte entfernen und die Variablen in numerische umwandeln.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nWir haben nun alle Daten geladen und die Variablen entsprechend vorbereitet. Wir können eigentlich mit der Indexbidlung beginnen, müssen uns allerdings davor noch entscheiden, welche Art von Index wir bilden möchten."
  },
  {
    "objectID": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "href": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "title": "Reliabilität von Skalen",
    "section": "1.3 Berechnen eines Ungewichteten Summenindex",
    "text": "1.3 Berechnen eines Ungewichteten Summenindex\nWir haben bereits in Kapitel 5.2 mittels der explorativen Faktorenanalyse statistisch getestet, ob wir einen Index aus den genannten Variablen bilden können. Dies ist der Fall. Wir berechnen nur die einfachste Form eines Index, den ungewichteten Summenindex. Das bedeutet, dass wir die Werte pro befragter Person für die genannten Variablen aufsummieren und KEINE Gewichtungen einbauen. Eine Gewichtung wäre bspw. wenn wir eine Variable doppelt zählen würden.\nWir erstellen eine neue Variable vertrauen_ges_inst und summieren die Werte aller Indikatoren pro Fall (befragte Person) auf, bevor wir diese durch die Anzahl der Indikatoren teilen. Auf diese Art und Weise erhalten wir die selben Werteausprägungen, wie in den Indikatoren was uns die Interpretation erleichtert.\n\nindex_vertrauen = allbus_vertrauen %&gt;% \n1  mutate(vertrauen_ges_inst = (Ver_Gesundheitswesen + Ver_BVerfG + Ver_Bundestag + Ver_Verwaltung + Ver_kath_Kirche + Ver_evan_Kirche + Ver_Justiz+ Ver_TV +  Ver_Zeitung + Ver_Uni + Ver_Regierung + Ver_Polizei + Ver_Parteien + Ver_Kom_EU + Ver_EU_Par) / 15)\n2htmlTable(head(index_vertrauen))\n\n\n1\n\nWir bilden mit Hilfe von mutate die neue Variable vertrauen_ges_inst, welche sich aus der Summe der Indikatoren geteilt durch die Anzahl der Indikatoren zusammensetzt.\n\n2\n\nDie htmlTable Funktion ermöglicht uns eine schönere Darstellung der Tabelle. Mit head wählen wir die ersten paar Fälle aus dem Datensatz index_vertrauen aus\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\nvertrauen_ges_inst\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n4.6\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n4.8\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n4.2\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n4.66666666666667\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n4\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n2.8\n\n\n\n\n\nWir können uns noch die deskriptive Statistik für den Index anschauen, diese ist wichtig um den berechneten Index korrekt zu interpretieren.\n\n1summary(index_vertrauen$vertrauen_ges_inst)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.400   4.133   4.049   4.733   7.000"
  },
  {
    "objectID": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "href": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "title": "Reliabilität von Skalen",
    "section": "1.4 Reliabilität des Indizes berechnen",
    "text": "1.4 Reliabilität des Indizes berechnen\nBevor wir diesen Index einsetzen können, müssen wir zunächst noch checken, ob die Variablen auch inhaltlich zusammenpassen. Dazu ermitteln wir Cronbach’s Alpha als Maß der Skalenreliabilität:\n\nindex_vertrauen %&gt;%\n1  select(Ver_Gesundheitswesen:Ver_EU_Par) %&gt;%\n2  psych::alpha(check.keys=TRUE)\n\n\n1\n\nWir wählen mit select alle Variablen zwischen Ver_Gesundheitswesen und Ver_EU_Par aus\n\n2\n\nHier rufen wir das Paket psych auf, nutzen aus diesem die Funktion alpha, um Cronbach’s Alpha zu berechnen\n\n\n\n\n\nReliability analysis   \nCall: psych::alpha(x = ., check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.92      0.92    0.94      0.43  11 0.0021    4  1     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.92  0.92\nDuhachek  0.91  0.92  0.92\n\n Reliability if an item is dropped:\n                     raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r\nVer_Gesundheitswesen      0.92      0.92    0.94      0.44 11.0   0.0021 0.024\nVer_BVerfG                0.91      0.91    0.94      0.43 10.5   0.0022 0.023\nVer_Bundestag             0.91      0.91    0.93      0.41  9.9   0.0024 0.020\nVer_Verwaltung            0.91      0.91    0.94      0.43 10.6   0.0022 0.025\nVer_kath_Kirche           0.92      0.92    0.94      0.45 11.7   0.0020 0.018\nVer_evan_Kirche           0.92      0.92    0.94      0.45 11.3   0.0020 0.021\nVer_Justiz                0.91      0.91    0.93      0.42 10.3   0.0023 0.023\nVer_TV                    0.91      0.91    0.93      0.43 10.7   0.0022 0.023\nVer_Zeitung               0.91      0.91    0.93      0.43 10.5   0.0022 0.023\nVer_Uni                   0.91      0.92    0.94      0.44 10.8   0.0022 0.024\nVer_Regierung             0.91      0.91    0.93      0.41  9.8   0.0024 0.020\nVer_Polizei               0.91      0.91    0.94      0.43 10.7   0.0022 0.024\nVer_Parteien              0.91      0.91    0.93      0.42 10.1   0.0023 0.022\nVer_Kom_EU                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\nVer_EU_Par                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\n                     med.r\nVer_Gesundheitswesen  0.43\nVer_BVerfG            0.41\nVer_Bundestag         0.41\nVer_Verwaltung        0.42\nVer_kath_Kirche       0.43\nVer_evan_Kirche       0.43\nVer_Justiz            0.41\nVer_TV                0.43\nVer_Zeitung           0.41\nVer_Uni               0.42\nVer_Regierung         0.41\nVer_Polizei           0.43\nVer_Parteien          0.41\nVer_Kom_EU            0.41\nVer_EU_Par            0.41\n\n Item statistics \n                        n raw.r std.r r.cor r.drop mean  sd\nVer_Gesundheitswesen 3238  0.58  0.58  0.53   0.51  4.9 1.4\nVer_BVerfG           3238  0.70  0.69  0.67   0.64  5.3 1.5\nVer_Bundestag        3238  0.83  0.82  0.82   0.79  4.1 1.6\nVer_Verwaltung       3238  0.66  0.66  0.62   0.60  4.5 1.3\nVer_kath_Kirche      3238  0.47  0.46  0.42   0.38  2.3 1.5\nVer_evan_Kirche      3238  0.54  0.52  0.49   0.45  3.0 1.7\nVer_Justiz           3238  0.73  0.73  0.70   0.67  4.6 1.5\nVer_TV               3238  0.64  0.65  0.62   0.58  3.6 1.3\nVer_Zeitung          3238  0.67  0.68  0.66   0.62  4.0 1.3\nVer_Uni              3238  0.61  0.63  0.58   0.56  5.2 1.2\nVer_Regierung        3238  0.83  0.83  0.83   0.80  4.1 1.6\nVer_Polizei          3238  0.63  0.64  0.60   0.57  4.9 1.4\nVer_Parteien         3238  0.78  0.77  0.76   0.74  3.2 1.3\nVer_Kom_EU           3238  0.80  0.79  0.81   0.75  3.5 1.5\nVer_EU_Par           3238  0.80  0.79  0.81   0.76  3.6 1.6\n\nNon missing response frequency for each item\n                        1    2    3    4    5    6    7 miss\nVer_Gesundheitswesen 0.02 0.04 0.10 0.17 0.28 0.28 0.11    0\nVer_BVerfG           0.02 0.04 0.07 0.15 0.19 0.28 0.24    0\nVer_Bundestag        0.08 0.10 0.16 0.25 0.24 0.14 0.04    0\nVer_Verwaltung       0.03 0.05 0.13 0.26 0.31 0.18 0.04    0\nVer_kath_Kirche      0.42 0.20 0.16 0.13 0.05 0.03 0.02    0\nVer_evan_Kirche      0.26 0.16 0.19 0.19 0.11 0.07 0.02    0\nVer_Justiz           0.04 0.06 0.13 0.21 0.25 0.24 0.07    0\nVer_TV               0.08 0.13 0.21 0.33 0.18 0.05 0.01    0\nVer_Zeitung          0.05 0.09 0.18 0.30 0.26 0.11 0.01    0\nVer_Uni              0.01 0.02 0.05 0.17 0.29 0.34 0.12    0\nVer_Regierung        0.10 0.10 0.14 0.22 0.24 0.16 0.04    0\nVer_Polizei          0.02 0.04 0.08 0.18 0.28 0.30 0.10    0\nVer_Parteien         0.13 0.18 0.25 0.28 0.13 0.03 0.00    0\nVer_Kom_EU           0.13 0.14 0.19 0.26 0.19 0.08 0.02    0\nVer_EU_Par           0.13 0.14 0.18 0.25 0.19 0.09 0.02    0"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Zur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "href": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.6 Interpretation des Wirksamkeit-Indizes",
    "text": "1.6 Interpretation des Wirksamkeit-Indizes\nDie Werte sind ein gutes Ergebnis. Die Items zeigen eine gute Inter-Item-Korrelation.\nWir können noch nachschauen, ob wir die Skalen-Reliabilität verbessern können, indem wir einzelne Items herauswerfen. Denn der Output von Cronbachs Alpha gibt uns auch hilfreiche Aufschlüsse darüber, welche Items man evtl. ausschließen kann, um Cronbachs Alpha bei ungenügender Höhe noch auf ein mindestens akzeptables Maß zu heben. Diese Information findet sich im Bereich “Reliability if an item is dropped”:. In unserem Fall wird die reliabitlitä aber noch schlechter - wir können nichts mehr verbessern.\nEntsprechend haben wir erfolgreich einen Index für die latente Variable Vertrauen in gesellschaftliche Institutionen gebildet. Wir haben eine theoretische Grundlage gefunden, diese empirisch anhand der Daten des Allbus mittels explorativer Faktorenanalyse geprüft und einen Summenindex berechnet, dessen Qualität wir mittels Cronbachs Alpha zeigen konnten.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der Indexbildung werden meistens direkt im Text angegeben:\n✅ die Art des gebildeten Index (Summenindex, etc.)\n✅ Cronbachs Alpha\n✅ Enthaltene Indikatoren\nDas Format ist normalerweise:\n\nBeispiel: Der Summenindex individuelle Identität umfasst fünf Indikatoren (Ziele und Befriedigung, Regeln und Verantwortung, Gefühle oder Emotionen, Verständnis der Welt, individuelle Identität im Allgemeinen; α = 0,84)."
  },
  {
    "objectID": "Skript_5.3.html#quellen-für-das-script",
    "href": "Skript_5.3.html#quellen-für-das-script",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Quellen für das Script",
    "text": "1.8 Quellen für das Script\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "href": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Video\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten."
  },
  {
    "objectID": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "href": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse",
    "text": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse\nDie Faktorenanalyse bringt, wie jedes statistische Verfahren, eine Reihe von Vorraussetzungen mit. Diese Vorraussetzungen sollten wir kennen und bei der Anwendung der Faktorenanalyse beachten. Viele der Vorraussetzungen beziehen sich auf Pearson-Korrelationskoeffizienten, welcher die statistsiche Grundlage für die Berechnung der Faktoren bildet.\n\nVarianz: Wir sollten sichergehen, dass die Daten aus unserer Stichprobe ausreichend varrieren. Wir werfen hierfür ein Blick in die Daten.\n\n\n1colors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n1\n\nVisuelle Überprüfung mit einem Histogram für die erste Variable Ver_Gesundheitswesen. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nLinearität: Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Variablen. Wenn die tatsächliche Beziehung nicht linear ist, dann verringert sich der Wert von r. Wir können auf Linearität u.a. visuell durch das Betrachten der Daten mittel Streudiagramm prüfen.\n\n\n1ggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen, y = Ver_BVerfG)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"darkgreen\", size = 1)\n\n\n1\n\nVisuelle Überprüfung mit einem Streudiagramm für die erste Variable Ver_Gesundheitswesen & Ver_BVerfG. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nNormalverteilung: Der Pearson-Korrelationskoeffizient setzt eine Normalverteilung voraus. Allerdings finden sich in der Realität fast nie perfekt normalverteilte Daten. Schiefe und Kurtosis sind besonders einflussreich die Ergebnisse der Faktorenanalyse und können im Extremfall artefaktische Ergenbnisse erzeugen.\n\n\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n2\n\nStatistische Überprüfung mittels Shapiro Wilk Test für die erste Variable Ver_Gesundheitswesen. Ein p-Wert unter 0.05 = keine Normalverteilung und ein p-Wert über 0.05 = Normalverteilung\n\n\n\n\n\n\n2shapiro.test(allbus_vertrauen$Ver_Gesundheitswesen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_vertrauen$Ver_Gesundheitswesen\nW = 0.92081, p-value &lt; 2.2e-16\n\n\n\nLevel der Messung: Bei Pearson-Korrelationen wird davon ausgegangen, dass normalverteilte Variablen auf Intervall- oder Verhältnisskalen gemessen werden, d. h. es handelt sich um kontinuierliche Daten mit gleichen Intervallen. Diese Eigenschaften treffen nicht auf ordinale (bspw. Kategorien) oder dochotome (bspw. Wahr-Falsch-Items) Variablen zu, was sich negativ auf Pearson-Korrelationskoeffizieten auswirkt und zu verzerrten Ergebnissen führen kann. Allerdings ist ein beträchtlicher Teil der Daten, mit denen wir zu tun haben, ordinal oder dichotom skaliert, um auch mit diesen Daten arbeiten zu können nutzen wir die polychorische Korrelation, welche robuster Nicht-Normalverteilung ist.\nFehlende Werte: In jeder Studie sollten wir die Anzahl und die Art der fehlenden Werte sowie die Gründe und die Methoden für den Umgang mit diesen Daten angegeben werden.\n\n\nallbus_vertrauen = allbus_vertrauen %&gt;%   \n1  mutate(across(Ver_Gesundheitswesen:Ver_EU_Par, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n2  na.omit()\n\n\n1\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter. Während %in% angibt, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n2\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "title": "Die Faktorenanalyse",
    "section": "1.1 Teildatensatz mit den benötigten Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Variablen\nDie Variablen werden aufgrund ihrer Nützlichkeit als Indikatoren für die zu untersuchende latente Variable ausgewählt. Entsprechend ist es wichtig, dass die Variablen inhaltliche, diskriminante und konvergente Validität aufweisen. Etwas vereinfacht ausgedrückt sollten die Indikatoren über eine inhaltliche Passung zur latenten Variable verfügen, möglichst gut von anderen latenten Variablen abgrenzbar und mit mehreren unterschiedlichen Arten der Messung nachweisbar sein.\nIn unserem Fall möchten wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nFür die statistische Identifizierung einer latenten Variablen bzw. eines Faktors werden mindestens drei gemessene Variablen benötigt, obwohl mehr Indikatoren vorzuziehen sind. Es werden beispielsweise auch vier bis sechs Indikatoren pro Faktor empfohlen. Im Allgemeinen funktioniert die EFA besser, wenn jeder Faktor überdeterminiert ist (d. h. es werden mehrere gemessene Variablen von der zu entdeckenden latenten Variable bzw. Faktor beeinflusst). Unabhängig von der Anzahl sollten Variablen, die voneinander abhängig sind, nicht in eine Faktorenanalyse einbezogen werden.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nNeben der Auswahl der Variablen bzw. Indikatoren müssen auch die Fälle (in unserem Fall die Anzahl der befragten Personen) festgelegt werden. Hier sollten wir uns zunächst fragen, ob die Stichprobe der Teilnehmer:innen in Bezug auf die gemessenen Indikatoren sinnvoll ist? Handelt es sich um eine repräsentative Stichprobe? Bei dem Allbus ist das der Fall und entsprechend können wir davon ausgehen, dass wir eine passende Stichprobe für die Durchführung eine Faktorenanalyse vorliegen haben."
  },
  {
    "objectID": "Skript_5.2.html#referenzen",
    "href": "Skript_5.2.html#referenzen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. https://doi.org/10.1177/0095798418771807"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-würdigung",
    "href": "Skript_5.2.html#danksagung-und-würdigung",
    "title": "Die Faktorenanalyse",
    "section": "1.4 Danksagung und Würdigung",
    "text": "1.4 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-würdigung",
    "href": "Skript_5.3.html#danksagung-und-würdigung",
    "title": "Reliabilität von Skalen",
    "section": "1.9 Danksagung und Würdigung",
    "text": "1.9 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "href": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Verschiedene Arten von Indizes",
    "text": "1.2 Verschiedene Arten von Indizes\nEs gibt eine ganze Reihe von möglichen Arten von Indizes, welche wir theoretisch berechnen könnten.\n\n\n\n\n\n\n\n\nArt des Index\nBildung (Beispiel)\nBeschreibung\n\n\n\n\nUngewichteter additivier Index\nIndex = Indikator_1 + Indikator_2 + Indikator_3\nDie Ausprägungen der Indikatorvariablen werden addiert bzw. zu gemittelt.\n\n\nUngewichteter multiplikativer Index\nIndex = Indikator_1 * Indikator_2 * Indikator_3\nWenn ein Index Mindestausprägungen auf allen Indikatorvariablen voraussetzt sollte multiplikativ zu einem Gesamtindex verknüpft werden.\n\n\nGewichteter additivier Index\nIndex = (2*Indikator_1) + Indikator_2 + Indikator_3\nGewichtete additive Indizes ermöglichen eine differenzierte Behandlung der einzelnen Indikatoren.\n\n\n\nDie Entscheidung, welche Art der Indexbildung gewählt wird sollte vor dem Hintergrund der Daten, sowie der latenten Variable und deren Eigenschaften erfolgen. Beispielsweise würde es für ein Index, welcher die Zufriedenheit mit einer Bahnreise widerspiegelt und aus den Inidkatoren Reisedauer, Service während der Reise, Komfort während der Fahrt gebildet wird, Sinn ergeben einen Ungewichteten multiplikativen Index zu bilden, da bei einer Reisedauer von Null keine Reise stattgefunden hat und somit auch die anderen beiden Indikatoren nicht von Bedeutung sind."
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "1.5 Interpretation von Cronbach’s Alpha",
    "text": "1.5 Interpretation von Cronbach’s Alpha\nZur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-literatur",
    "href": "Skript_5.2.html#danksagung-und-literatur",
    "title": "Die Faktorenanalyse",
    "section": "2.1 Danksagung und Literatur",
    "text": "2.1 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-literatur",
    "href": "Skript_5.3.html#danksagung-und-literatur",
    "title": "Reliabilität von Skalen",
    "section": "6.1 Danksagung und Literatur",
    "text": "6.1 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Cronbach, L (1951). Coefficient alpha and the internal structure of tests. Link\n\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_1.1.html#installation-von-r-und-rstudio",
    "href": "Skript_1.1.html#installation-von-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "3 Installation von R und RStudio",
    "text": "3 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "href": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "title": "Einführung in R und RStudio",
    "section": "4 Projekte und Ordnerstrukturen",
    "text": "4 Projekte und Ordnerstrukturen\nEinführung in die Logik von R, R Studio Server und das R Environment Einführung in Markdown (Quarto?) Basics der Befehlssyntax in Base R und Tidyverse (im Vergleich), ab dann aber alles in dplyr Laden von Daten und Importieren von anderen Datenformaten Einführung in das Datenmanagement: Projekte und Ordnerstrukturen auf dem PC Öffnen von Datensätzen, Laden von Daten, Importieren von anderen Datensätzen Speichern von Daten aus R in verschiedenen Formaten (Rda, csv etc.)"
  },
  {
    "objectID": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "href": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Pakete als Erweiterung von R",
    "text": "5 Pakete als Erweiterung von R\nInstallieren von Paketen und Laden von Librarys (p_load) Vorschau: Datentypen und deren Charakteristika."
  },
  {
    "objectID": "Skript_1.3.html#r-markdown",
    "href": "Skript_1.3.html#r-markdown",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown",
    "text": "1 R Markdown\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen. Markdown bzw. Quarto-Dokumente bestehen aus drei Bestandteilen\n\n1.1 YAML-Header\n\n\n1.2 Text\n\n\n1.3 Code Chunks\n💡"
  },
  {
    "objectID": "Skript_1.3.html#quarto",
    "href": "Skript_1.3.html#quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Quarto",
    "text": "2 Quarto\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen)."
  },
  {
    "objectID": "Skript_1.3.html#literatur",
    "href": "Skript_1.3.html#literatur",
    "title": "Die Logik von Markdown und Quarto",
    "section": "7 Literatur",
    "text": "7 Literatur\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link\n\n\n📖 Xie, Y., Allaire, J. J., & Grolemund, G. (2020). R markdown: The definitive guide. Chapman; Hall/CRC Link\n\n\n📖 Xie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman and Hall/CRC Link\n\n\n📖 Allaire, J. J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., & Iannone, R. (2020a). rmarkdown: Dynamic documents for r. Link"
  },
  {
    "objectID": "Autoren.html#katharina-maubach",
    "href": "Autoren.html#katharina-maubach",
    "title": "Das Team stellt sich vor",
    "section": "Katharina Maubach",
    "text": "Katharina Maubach\nVita\n\n\n\n\n\nKatharina Maubach, © B. Köhler\n\n\nKatharina Maubach ist seit Dezember 2022 als wissenschaftliche Mitarbeiterin am Zentrum für Medien-, Kommunikations- und Informationsforschung der Universität Bremen im Lab „Politische Kommunikation und Innovative Methoden” tätig. Sie forscht im Rahmen des DFG-geförderten Projektes „Remixing Political News Reception” unter der Leitung von Prof. Stephanie Geise zur Rezeption multimodaler Medieninhalte. Innerhalb des Projektes war sie seit Februar 2021 an der Universität Münster beschäftigt, wo Sie zudem von Januar 2019 bis Januar 2021 im Arbeitsbereich von Prof. Volker Gehrau forschte und lehrte. Ihr Studium der Kommunikationswissenschaft absolvierte sie ebenfalls an der Universität Münster; in ihrer Masterarbeit forschte sie zum Einfluss politischer Nachrichtensatire auf politisches Interesse und Informiertheit.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nRezeptions- und Wirkungsforschung\nKlimakommunikation\nStatistische Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_1.4.html",
    "href": "Skript_1.4.html",
    "title": "Vorbereiten der R-Umgebung",
    "section": "",
    "text": "Unser Schreibtisch, Bild generiert von Midjourney\n\n\nBevor wir uns näher mit Datensätzen und deren Analyse beschäftigen, sollten wir uns zunächst einige Grundlagen widmen. Wir beschäftigen uns im Folgenden mit einigen fundamentalen Begriffen, deren Funktionsweisen wir kennen sollten - das erleichtert uns zumindest die Arbeit deutlich und vermeidet wahrscheinlich den ein oder anderen Fehler samt dazugehörigen frustrierten Verfluchen des Kurses und R.\n\n\n\n\n\n\nVideo\n\n\n\n\n\n\n\n\n\nLasst uns Beginnen und Schritt für Schritt die wichtigsten Punkte durchgehen, die ihr zur Vorbereitung eures Analyse-Projekts machen solltet.\n\n1 Die R-Studio-Cloud der Universität\nFalls ihr an der Universität Bremen studiert könnt ihr den Service des Zentrum für Multimedia in der Lehre nutzen indem ihr euch hier mit eurer Uni-Kennung einloggt.\nAls nächstes habt ihr die Möglichkeit die für euren Kurs vorgesehene Programmierumgebung auszuwählen. In unserem Fall ist das die “Einführung in quantitative Forschungsdesigns und Datenanalyse”. Mit der Auswahl wird eure eigenen Programmierumgebung eingerichtet, auf dieser könnt ihr die Schaltfälche RStudio auswählen.\n\n\n\nScreenshot RStudio\n\n\nMit dem Klick gelangt ihr in R-Studio.\n\n\n2 Das Anlegen eines R-Projekts\nStarten wir zunächst mit dem Anlegen eines sogenannten R-Projekts. RStudio-Projekte ermöglichen es uns alle mit einem Projekt verbundenen Dateien an einem Ort zu speichern. Das umfasst Datensätze, R-Skripte, Ergebnisse, Abbildungen, Berichte usw. Projekte sind bereits in RStudio integriert.\n\n\n\nScreenshot RStudio\n\n\nDas gerade aktive RProjekt sehen wir in der rechts oberen Ecke des Nutzer:innen Interfaces von RStudio. Hier können wir durch durch den Button New Project ein neues Projekt anlegen, hierfür folgen wir einfach dem Menüverlauf. Für einen guten, reproduzierbaren Arbeitsablauf sollten alle Analyse Projekte mit der Erstellung eines Projekts beginnen.\nIn eurem Fall könnt ihr euch direkt alle notwendigen Materialen und Skripte für das erfolgreiche Bearbeiten dieses Kurses in R-Studio laden. Hierfür geht ihr in der rechts oberen Ecke des Nutzer:innen Interfaces auf den Button New Project und wählt dann Version Control aus.\n\n\n\nScreenshot RStudio\n\n\nIm nächsten Schritt wählt ihr dann Git aus.\n\n\n\nScreenshot RStudio\n\n\nUnd fügt unter Respository URL die folgende Adresse ein: https://github.com/patrickzerrer/R_Hands_on_Book.\nMit einem Klick auf den Button Create Project werden alle notwendigen Materialien und Ordner direkt in euer eigenes neugeschaffenens R-Projekt geladen.\n\n\n\nScreenshot RStudio\n\n\nIhr habt jetzt alle Materialien und Skripte und könnt mit dem Kurs beginnen.\n\n\n3 Das Working-Dictonary (wd)\nIm Allgemeinen ist das Arbeitsverzeichnis (das Working-Dictonary, wd) der Ort, an dem R nach Dateien (vor allem nach Datensätzen) sucht. Wenn ihr kein Projekt verwenden, müsst ihr wahrscheinlich durch die Funktion setwd oder das Interface (siehe Screenshot) ein Working-Dictonary setzen, bevor ihr euren R-Code ausführen könnt.\n Wenn ihr beispielsweise den Code verwenden möchtet, den wir euch in diesem Kurs zu Verfügung stellen, müsst ihr darauf achten, dass ihr die Dateipfade auf euer Working-Dictonary und Ordnerstruktur anpasst.\n\n1daten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n1\n\nDer Bereich in Anführungszeichen gibt den Dateipfad an. Damit sagt ihr R, wo sich die zu ladende Datei befindet.\n\n\n\n\nIn dem Beispiel ist im Working-Dictonary des Projekts ein Unterordner mit der zu ladenden Datei. Der Dateipfad befindet sich im R-Code in den Anführungszeichen, also \"Datensatz/Allbus_2021.dta\". Der erste Teil \"Datensatz/\" gibt an, dass sich die Datei in einem Ordner befindet, der Datensatz heißt. Der zweite Teil Allbus_2021.dta\" gibt den eigentlichen Dateinamen mit der entsprechend Dateiendung an (also .dta). Wahrscheinlich müsst ihr den R-Code entsprechend anpassen.\n\n\n4 Das Zugreifen auf Dateien und Datensätze\nBevor wir allerdings auf Dateien und Datensätze zugreifen können, müssen wir diese zunächst in unseren Projekt-Ordner (also in das Working-Dictonary) verschieben. Falls ihr für diesen Kurs die Desktop-Version von RStudio nutzt, könnt ihr hier einfach die entsprechenden Datein auf eurem lokalen Rechner in das zuvor von euch festgelegte Working-Dictonary kopieren. Wenn ihr die Cloud-Version von RStudio nutzt, müsst ihr hier die Dateien und Datensätze zunächst in die Cloud laden. Hierfür könnt ihr auf den Upload-Button in der unteren rechten Ecke des Interfaces gehen und in dem sich öffnenden Fenster die entsprechende Datei auswählen.\n\n\n\nScreenshot RStudio\n\n\nSobald ihr durch das Klicken von OK bestätigt habt, wird die Datei hochgeladen und erscheint in dem unteren rechten Fenster im Files-Reiter.\n\n\n5 Die Session\nDie sogenannte Session wurde bereits mehrmals kurz erwähnt. Bei der Session handelt es sich um eine laufende Instanz des R-Programms, die ihr beenden und neustarten könnt. Das macht insbesondere dann Sinn, wenn ihr die falsche Version eines Pakets installiert habt oder ihr einen sauberen Start für euren R Code haben möchtet. Durch einen Neustart der Session könnt ihr euch sicher sein, dass ihr nicht ausversehen irgendwelche zuvor verwendeten Pakete oder Berechnungen mitnehmt.\nIhr könnt die Session mit der .rs.restartR Funktion in der Konsole neustarten. Alternativ könnt ihr im R Studio Interface auf Sessionund Restart R gehen.\n\n\n\nScreenshot RStudio\n\n\nEin kurze Mitteilung in der Konsole gibt euch an, dass die Session neu gestartet wurde.\n\n\n6 Hilfe zur Selbsthilfe\nIhr werdet im Laufe des Kurses immer wieder mit Fehlermeldungen zu tun haben. Hier ist es wichtig einen kühlen Kopf zu bewahren und möglichst systematisch zu versuchen den Fehler zu finden und zu beheben. Der Umgang mit Fehlern ist ein großer und wichtiger Teil bei jeglicher Programmierung und es gibt hier ein paar Tipps, wie man damit am besten umgeht.\n\nAtmen: Bleib ruhig und atme ein paar mal tief durch, gerade wenn es nicht der erste Fehler ist der dir heute begegnet.\nLesen: Lies aufmerksam die Fehlermeldung.\nCode überprüfen: Schaue nach, ob sich ein Warnzeichen am Rand deines R Codes befindet. Dieses Warnzeichen weist dich auf einen Syntaxfehler (bspw. eine vergessene Klammer oder Komma) hin.\nVariablen überprüfen: Überprüfe kurz, ob du die richtigen Variablen verwendest oder sich ein Tippfehler eingeschlichen hat. Achtet auch auf Groß- und Kleinschreibung.\nInformationen suchen: Rufe eine Vignette (mit dem Befehl vignette) oder rufe die Hilfeseite mit einem der Funktion vorgelagerten Fragezeichen auf (mit bspw. ?dpylr()) auf und lies sie dir durch.\nSich im Netz Hilfe suchen: Suche auf Stack Overflow nach deinem Problem, meistens hat jemand auf der Welt das oder ein ähnliches Problem schon einmal gelöst.\nFrag deine Kommilitonen: Tauscht euch aus, gemeinsam findet ihr eine Lösung!\nDie KI fragen: Nutze ein KI Chatbot, um dir die Fehlermeldung erklären zu lassen und Lösungsvorschläge zu unterbreiten. Beachte hierbei bitte, dass die Antworten nicht immer korrekt sind und meist Kontextwissen von dir benötigt wird, um die Korrektheit der Antworten zu prüfen Link.\nFrag uns: Frag dein:e Dozent:in :)"
  },
  {
    "objectID": "Skript_1.4.html#section",
    "href": "Skript_1.4.html#section",
    "title": "Die Kunst vom Umgang mit Daten",
    "section": "1.1 ",
    "text": "1.1"
  },
  {
    "objectID": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "href": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.1 Das Anlegen eines R-Projekts",
    "text": "1.1 Das Anlegen eines R-Projekts\nStarten wir zunächst mit dem Anlegen eines sogenannten R-Projekts. RStudio-Projekte ermöglichen es uns alle mit einem Projekt verbundenen Dateien an einem Ort zu speichern. Das umfasst Datensätze, R-Skripte, Ergebnisse, Abbildungen, Berichte usw. Projekte sind bereits in RStudio integriert.\n Das gerade aktive RProjekt sehen wir in der rechts oberen Ecke des Nutzer:innen Interfaces von RStudio. Hier können wir durch durch den Button New Project ein neues Projekt anlegen, hierfür folgen wir einfach dem Menüverlauf. Für einen guten, reproduzierbaren Arbeitsablauf sollten alle Analyse Projekte mit der Erstellung eines Projekts beginnen."
  },
  {
    "objectID": "Skript_1.4.html#das-working-dictonary-wd",
    "href": "Skript_1.4.html#das-working-dictonary-wd",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.2 Das Working-Dictonary (wd)",
    "text": "1.2 Das Working-Dictonary (wd)\nIm Allgemeinen ist das Arbeitsverzeichnis (das Working-Dictonary, wd) der Ort, an dem R nach Dateien (vor allem nach Datensätzen) sucht. Wenn ihr kein Projekt verwenden, müsst ihr wahrscheinlich durch die Funktion setwd oder das Interface (siehe Screenshot) ein Working-Dictonary setzen, bevor ihr euren R-Code ausführen könnt.\n Wenn ihr beispielsweise den Code verwenden möchtet, den wir euch in diesem Kurs zu Verfügung stellen, müsst ihr darauf achten, dass ihr die Dateipfade auf euer Working-Dictonary und Ordnerstruktur anpasst.\n\n1daten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n1\n\nDer Bereich in Anführungszeichen gibt den Dateipfad an. Damit sagt ihr R, wo sich die zu ladende Datei befindet.\n\n\n\n\nIn dem Beispiel ist im Working-Dictonary des Projekts ein Unterordner mit der zu ladenden Datei. Der Dateipfad befindet sich im R-Code in den Anführungszeichen, also \"Datensatz/Allbus_2021.dta\". Der erste Teil \"Datensatz/\" gibt an, dass sich die Datei in einem Ordner befindet, der Datensatz heißt. Der zweite Teil Allbus_2021.dta\" gibt den eigentlichen Dateinamen mit der entsprechend Dateiendung an (also .dta). Wahrscheinlich müsst ihr den R-Code entsprechend anpassen."
  },
  {
    "objectID": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "href": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.3 Das Zugreifen auf Dateien und Datensätze",
    "text": "1.3 Das Zugreifen auf Dateien und Datensätze\nBevor wir allerdings auf Dateien und Datensätze zugreifen können, müssen wir diese zunächst in unseren Projekt-Ordner (also in das Working-Dictonary) verschieben. Falls ihr für diesen Kurs die Desktop-Version von RStudio nutzt, könnt ihr hier einfach die entsprechenden Datein auf eurem lokalen Rechner in das zuvor von euch festgelegte Working-Dictonary kopieren. Wenn ihr die Cloud-Version von RStudio nutzt, müsst ihr hier die Dateien und Datensätze zunächst in die Cloud laden. Hierfür könnt ihr auf den Upload-Button in der unteren rechten Ecke des Interfaces gehen und in dem sich öffnenden Fenster die entsprechende Datei auswählen.\n\n\n\nScreenshot RStudio\n\n\nSobald ihr durch das Klicken von OK bestätigt habt, wird die Datei hochgeladen und erscheint in dem unteren rechten Fenster im Files-Reiter."
  },
  {
    "objectID": "Skript_1.4.html#die-installation-von-paketen",
    "href": "Skript_1.4.html#die-installation-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Installation von Paketen",
    "text": "1.4 Die Installation von Paketen\nWir möchten für unser Analyse-Projekt nicht nur die Standartausführung von R verwenden, welche auch als base R bezeichnet wird, sondnern einige Pakte installieren. R-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Wir können Pakete über CRAN (Comprehensive R Archive Network) - das ist ein zentrales Software-Repository - installieren. R hat den Vorteil, dass es über eine große Zahl an frei verfügbaren und einfach zu installierenden Paketen verfügt. Das hat unter anderem dazu geführt, dass die Programmiersprache im Data Science Bereich verbreitet ist.\nWenn wir ein R-Paket installieren möchten können wir hierfür die Funktion install.packages verwenden, in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\nPaket 'tidyverse' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpyghIHy\\downloaded_packages\n\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und Bestätigt.\n Einige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden."
  },
  {
    "objectID": "Skript_1.4.html#das-laden-von-paketen",
    "href": "Skript_1.4.html#das-laden-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Das Laden von Paketen",
    "text": "1.5 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet.\nDies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen).\n\nlibrary(tidyr)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio"
  },
  {
    "objectID": "Skript_1.4.html#die-session",
    "href": "Skript_1.4.html#die-session",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Session",
    "text": "1.4 Die Session\nDie sogenannte Session wurde bereits mehrmals kurz erwähnt. Bei der Session handelt es sich um eine laufende Instanz des R-Programms, die ihr beenden und neustarten könnt. Das macht insbesondere dann Sinn, wenn ihr die falsche Version eines Pakets installiert habt oder ihr einen sauberen Start für euren R Code haben möchtet. Durch einen Neustart der Session könnt ihr euch sicher sein, dass ihr nicht ausversehen irgendwelche zuvor verwendeten Pakete oder Berechnungen mitnehmt.\nIhr könnt die Session mit der .rs.restartR Funktion in der Konsole neustarten. Alternativ könnt ihr im R Studio Interface auf Sessionund Restart R gehen.\n\n\n\nScreenshot RStudio\n\n\nEin kurze Mitteilung in der Konsole gibt euch an, dass die Session neu gestartet wurde."
  },
  {
    "objectID": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "href": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Hilfe zur Selbsthilfe",
    "text": "1.5 Hilfe zur Selbsthilfe\nIhr werdet im Laufe des Kurses immer wieder mit Fehlermeldungen zu tun haben. Hier ist es wichtig einen kühlen Kopf zu bewahren und möglichst systematisch zu versuchen den Fehler zu finden und zu beheben. Der Umgang mit Fehlern ist ein großer und wichtiger Teil bei jeglicher Programmierung und es gibt hier ein paar Tipps, wie man damit am besten umgeht.\n\nAtmen: Bleib ruhig und atme ein paar mal tief durch, gerade wenn es nicht der erste Fehler ist der dir heute begegnet.\nLesen: Lies aufmerksam die Fehlermeldung.\nCode überprüfen: Schaue nach, ob sich ein Warnzeichen am Rand deines R Codes befindet. Dieses Warnzeichen weist dich auf einen Syntaxfehler (bspw. eine vergessene Klammer oder Komma) hin.\nVariablen überprüfen: Überprüfe kurz, ob du die richtigen Variablen verwendest oder sich ein Tippfehler eingeschlichen hat.\nInformationen suchen: Rufe eine Vignette (mit dem Befehl vignette) oder rufe die Hilfeseite mit einem der Funktion vorgelagerten Fragezeichen auf (mit bspw. ?dpylr()) auf und lies sie dir durch.\nSich im Netz Hilfe suchen: Suche auf Stack Overflow nach deinem Problem, meistens hat jemand auf der Welt das oder ein ähnliches Problem schon einmal gelöst.\nDie KI fragen: Nutze ein KI Chatbot, um dir die Fehlermeldung erklären zu lassen und Lösungsvorschläge zu unterbreiten. Beachte hierbei bitte, dass die Antworten nicht immer korrekt sind und meist Kontextwissen von dir benötigt wird, um die Korrektheit der Antworten zu prüfen Link.\nFrag uns: Frag dein:e Dozent:in :)"
  },
  {
    "objectID": "Skript_1.1.html#was-ist-r",
    "href": "Skript_1.1.html#was-ist-r",
    "title": "Einführung in R und RStudio",
    "section": "1 Was ist R?",
    "text": "1 Was ist R?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten."
  },
  {
    "objectID": "Skript_1.1.html#was-ist-rstudio",
    "href": "Skript_1.1.html#was-ist-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "2 Was ist RStudio?",
    "text": "2 Was ist RStudio?\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten."
  },
  {
    "objectID": "Skript_1.1.html#das-interface-von-rstudio",
    "href": "Skript_1.1.html#das-interface-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Das Interface von RStudio",
    "text": "4 Das Interface von RStudio\nGrundsätzliches zur Oberfläche\nSchaltflächen\nMenüleiste\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "href": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Die Programmoberfläche von RStudio",
    "text": "4 Die Programmoberfläche von RStudio\n\n4.1 Schaltflächen\nInnerhalb von RStudio unterscheiden wir 4 Schaltflächen welche sich beliebig via Drag and Drop verschieben oder auch minimieren lassen.\n\n1 Ist der Bereich in welchen wir Skripte, Markdown und Quarto-Dokumente bearbeiten und ausführen können (siehe auch Markdown und Quarto)\n2 Ist die Konsole. Dies ist der Bereich in welchem wir weiterhin oldschool R angezeigt bekommen. Dieser Bereich kann sehr hilfreich sein, wenn man kurz Befehle benötigt, welche nicht im Skript auftauchen sollen (beispielsweise eine kurze Hilfe zu Funktionen mittels ?)\n3 In diesem Bereich findet sich alles zu den innerhalb von R geladenen Datensätzen. Unter Environment finden sich die Datensätze (Data), die Historie der genutzten Befehle (History), eine Schnittstelle zu Datenbanken (Connection) sowie R-interne Tutorials (Tutorial).\n4 In diesem Bereich finden sich verschiedene Reiter, welche die Organisation der Arbeit mit R erleichtern. Unter File befinden sich alle innerhalb des Ordners oder Projektes befindlichen Dateien. Unter Plots kann man sich die in R erstellten Grafiken anzeigen lassen. Packages zeigt alle in R installierten Pakete an. Hier lassen sich mittels install auch neue Pakete installieren oder über update die aktuellen Pakete updaten. Bei Help können wir eine Hilfeseite aufrufen. Es kann wahlweise direkt innerhalb der Seite nach Hilfen gesucht werden oder mit dem Befehl ?Name der Funktion bzw. ??Name des Packages innerhalb der Konsole. Möchte man nach einem Befehl aus einem Paket suchen nutzt man den Suchbefehle?Name des Packages::Name der Funktion. Unter Viewer finden sich gerenderte Dokumente (beispielsweise ein gerendertes Markdown oder Quarto Dokument) und unter Presentation gerenderte Shiny-Dokumente.\n\n\n4.2 Menüleiste\nZusätzlich zu den Schaltflächen findet sich oben links eine Menüleiste:\n\n\nUnter File können neue Dateien erstellt, geöffnet und gespeichert werden.\nEdit bietet Möglichkeiten der Dateibearbeitung (bspw. Kopieren, Ausschneiden, Rückgängig etc.) falls ihr keine Short-Cuts nutzen wollt.\nCode gibt eine Übersicht über Funktionen innerhalb des Markdown-Dokumentes (bspw. Codechunks einfügen).\nUnter View können die einzelnen Schaltflächen und deren Aufteilung geändert werden. Wahlweise geht dies auch via Drag & Drop.\nPlots vereinfacht den Umgang mit in r erstellten Grafiken. Wahlweise könnt ihr hier auch den Reiter Plots in Schaltfläche 4 nutzen.\nSession hier kann eine R-Session neu gestartet oder beendet werden (siehe auch 1.6 Die Session).\nBuild, Debug und Profile beinhaltet Sonderanwendungen, wie beispielsweise das Debugging von Funktionen oder Fragen nach der Speed-Optimierung von R-Code.\nTools hat viele hilfreiche Funktionen. Hier können unter anderem Pakete installiert und hilfreiche Keyboard Shortcuts ausgegeben werden. Am bedeutsamsten ist hier jedoch der Bereich Global Options, in welchem unter anderem grundlegende Einstellungen zum Speicherort von R und den Paketen, zur Aufteilung und Aussehen von RStudio und zur Funktionalität von R Markdown getroffen werden können.\n\nAuch der Reiter Help kann sehr hilfreich sein. Hier finden sich eine Hilfeseite (siehe auch Reiter Help in Schaltfläche 4), Möglichkeiten der besseren Zugänglichkeit (Accessibility), Cheat Sheets für die Arbeit mit R und erneut eine Übersicht über Shortcuts für die Arbeit mit R."
  },
  {
    "objectID": "Skript_1.1.html#die-logik-von-r",
    "href": "Skript_1.1.html#die-logik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Logik von R",
    "text": "5 Die Logik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen. Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-grundlogik-von-r",
    "href": "Skript_1.1.html#die-grundlogik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Grundlogik von R",
    "text": "5 Die Grundlogik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen (💡 Fun-Fact, R “denkt nicht mit”, also auch wenn für euch klar ist, dass ihr doch die gesamte Zeit mit dem selben Datensatz arbeitet, muss R das immer wieder gesagt bekommen). Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\nDabei haben die meisten Funktionen noch weitere Zusatzoptionen, welche wir nutzen können. Bei dem Befehl mean können wir beispielsweise angeben, ob fehlende Werte in die Berechnung mit einfließen sollen oder nicht. Dies geschieht mit den Zusatz na.rm = TRUE bzw. na.rm = FALSE. Na.rm steht in diesem Fall für NA (=not available, fehlende Fälle) und remove (also entfernen), fragt demnach ob fehlende Fälle aus der Berechnung ausgeschlossen werden sollen. Dabei ist die default-Option, also die Option die in dem Befehl voreingestellt ist, dass fehlende Werte nicht aus der Berechnung ausgeschlossen werden (na.rm=F). Die meisten Befehle haben bestimmte defaults, da dies diese den Normalfall der Nutzung beschreiben und uns beim Programmieren Schreibaufwand ersparen (möchten wir die defaults nutzen, müssen wir immerhin nichts zusätzliches in der Funktion angeben). Allerdings können wir hier auch immer die anderen Optionen nutzen, wir müssen dies nur in unserem Befehl angeben:\n\nmean(data$Alter, na.rm=T)\n\nHier haben wir auch bereits eine weitere Funktionalität von R kennengelernt, nämlich die Operationalisierung einzelner Parameter über T (TRUE) und F (FALSE). Doch woher weiß ich nun als neuer Nutzer, welche Optionen mit bei einzelnen Befehlen zur Verfügung stehen?\nHier hilf ein Blick in die Hilfeseite, welche wir beispielsweise für den Mittelwert mit dem Befehl ?mean() aufrufen können. Für jede Funktion stehen - wie oben bereits erwähnt - Hilfeseiten zur Verfügung. Diese beinhalten zunächst eine kurze Beschreibung Description, anschließend einen Überblick zur Nutzung Usage sowie dem Default des Befehles. Anschließend finden sich die Arguments, dies sind die Möglichkeiten, wie wir die Funktion nutzen können und welche zusätzlichen Optionen zur Verfügung stehen. Oftmals finden sich zudem weitere Erklärungen und Beispiele der Nutzung.\n\n5.1 Kurz-Exkurs: das tidyverse\nIn R selbst findet sich eine Vielzahl von Befehlen. Zusätzlich wird R von den Nutzern immer weiter entwickelt und es kommen neue Funktionen in Form von Paketen hinzu. Eines der meist genutzten Pakete(-universen) stellt dabei das tidyverse dar. Mit den Befehlen und Funktionen dieses Paketes kommt eine etwas andere Programmiersprache, welche uns jedoch die Arbeit mit R erleichtert. Gerade auch bei Fragen des Datenmanagementes ist das tidyverse hilfreich, denn hier kommt ein zweiter Fun-Fact über R:💡 R kann nicht nur manchmal etwas dumm sein (wir erinnern uns, es denkt nicht mit), es ist auch recht vergesslich. Wir haben oben bereits gelernt, dass wir in Befehlen immer den Datensatz und die Variable spezifizieren müssen. Dies stellt kein Problem bei einzelnen Befehlen dar, ist jedoch bei einer Vielzahl von Befehlen etwas nervig. Hier kommt uns die tidyverse Logik zu Nutze, in welcher einmal zu Beginn des Dokumentes der Datensatz spezifziert wird und anschließend alle weiteren Schritte durch eine Pipe %&gt;% verbunden werden. Die Pipe (Shortcut Windows: Ctrl + Shift + M; MAC: Cmd + Shift + M) bedeutet so viel wie “und dann”. Also im Prinzip sagen wir R, nehme diesen Datensatz und dann mache die folgenden Dinge, wobei wir so viele Schritte wie wir möchten jeweils mit Pipes verbinden können. Wir nutzen in unseren Skripten hauptsächlich die tidyverse-Logik, erklären diese daher grundlegender in Kapitel 3.1, wenn wir uns mit den ersten Schritten des Datenmanagements beschäftigen."
  },
  {
    "objectID": "Skript_1.1.html#pakete",
    "href": "Skript_1.1.html#pakete",
    "title": "Einführung in R und RStudio",
    "section": "6 Pakete",
    "text": "6 Pakete\nR-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Diese erweiteren die Funktionen, die bereits in der Standardausführung von R (bekannt als base R) gegeben sind.\n\n6.1 Die Installation von Paketen\nPakete können wahlweise R-intern über CRAN (das steht für Comprehensive R Archive Network und ist das zentrale Software-Repository) oder direkt von GitHub (für sehr neue Pakete, welche noch nicht auf CRAN sind) installiert werden. Im Normalfall installieren wir jedoch direkt von CRAN, da hier eine Vielzahl von Paketen und Funktionen vorhanden sind und die Installation sehr simpel ist.\nWenn wir ein R-Paket von CRAN installieren möchten nutzen wir die Funktion install.packages(), in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen. Für das Paket tidyverse wäre der Befehl wie folgt:\n\ninstall.packages(\"tidyverse\")\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und bestätigt die Anwendung wiederum mit Install.\n\n\n\nScreenshot RStudio\n\n\nEinige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden. Grundsätzlich ist es jedoch immer ratsam, einmal zu checken, ob die Pakete in der aktuellen Version installiert sind, da ansonsten die Funktionen der Pakete nicht funktionieren können.\n\n\n6.2 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet. Dies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen):\n\nlibrary(tidyverse)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio\n\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#was-sind-r-und-rstudio",
    "href": "Skript_1.1.html#was-sind-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "1 Was sind R und RStudio?",
    "text": "1 Was sind R und RStudio?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten.\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten.\n\n1.1 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "href": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Getting Started mit Markdown und Quarto",
    "text": "3 Getting Started mit Markdown und Quarto\n\n3.1 Installation\n\n\n3.2 Dokument laden\n\n\n3.3 Neues Dokument anlegen"
  },
  {
    "objectID": "Skript_1.3.html#r-markdown-und-quarto",
    "href": "Skript_1.3.html#r-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown und Quarto",
    "text": "1 R Markdown und Quarto\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen.\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen). Markdown-Dokumente weisen die Endung .rmd auf wohingegen Quarto-Dokumente den Endung .qmd haben. Beide Dokumenttypen können für unseren Kurszweck gleichwertig genutzt werden. Lediglich bei der Arbeit mit interaktiven, multimedialen oder mehrsprachigen Dokumenten ist Quarto besser geeignet als Markdown."
  },
  {
    "objectID": "Skript_1.3.html#installation-der-programme",
    "href": "Skript_1.3.html#installation-der-programme",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Installation der Programme",
    "text": "2 Installation der Programme\nMarkdown kann innerhalb von R mit dem Befehl install.packages(\"Rmarkdown\") installiert und anschließend geladen werden. Da es sich bei Quarto nicht um ein Package, sondern ein eigenes Interface handelt, muss das Programm extern heruntergeladen und auf dem Rechner installiert werden."
  },
  {
    "objectID": "Skript_1.3.html#bestehende-dokument-laden",
    "href": "Skript_1.3.html#bestehende-dokument-laden",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Bestehende Dokument laden",
    "text": "3 Bestehende Dokument laden\nZum Laden eines Quarto- oder Markdown Dokumentes könnt ihr dieses einfach auf eurem Rechner doppelklicken. Wahlweise könnt ihr auch über File -&gt; Open File (oder Strg + O) innerhalb von RStudio ein Dokument öffnen."
  },
  {
    "objectID": "Skript_1.3.html#neue-dokumente-anlegen",
    "href": "Skript_1.3.html#neue-dokumente-anlegen",
    "title": "Die Logik von Markdown und Quarto",
    "section": "4 Neue Dokumente anlegen",
    "text": "4 Neue Dokumente anlegen\nUm ein neues Dokument anzulegen, könnt ihr einfach über File -&gt; New File das gewünschte Dokument anlegen. Sowohl bei Markdown, als auch bei Quarto öffnet sich dann das Folgende Fenster:\n\n\n\nNeues Dokument anlegen\n\n\nDort könnt ihr euer Dokument benennen (Title), die Autoren festlegen (Author) sowie das Output-Format (HTML, PDF oder WORD) festlegen. Zudem könnt ihr bei Quarto angeben, wie ist das Dokument gerendert werden soll (Knitr vs. Jupyter). Hier könnt ihr die Auswahl auf Knitr belassen. Zuletzt könnt ihr festlegen, ob ihr den visual markdown editor oder den source editor nutzen möchtet. All diese Punkte könnt ihr jedoch auch noch nachfolgend im YAML-Header oder im Menü) ändern."
  },
  {
    "objectID": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "href": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6 Überblick über die Dokumentkomponenten",
    "text": "6 Überblick über die Dokumentkomponenten\nSowohl Markdown, als auch Quarto-Dokumente bestehen aus drei Bestandteilen: dem YAML-Header, Textbereichen und Codebereichen.\n\n6.1 YAML-Header\nInnerhalb des YAML Headers, welcher jeweils von --- umgeben ist, legen wir die Dokumentstruktur fest.\n\n\n\nYAML Header in einem Quarto Dokument\n\n\nDies beinhaltet beispielsweise den Titel des Dokumentes title:, die Autoren author, sowie Spezifikationen zur Dokumentstruktur, wie beispielsweise das Outputformat format: oder auch in Quarto Spezifikationen zum Umgang mit den Codechunks execute: echo : true auf Gesamtdokumentebene.\n\n\n6.2 Text\nIn Markdown und Quarto-Dokumenten können wir Text einbinden und diesen beliebig formatieren. Dazu können wir wahlweise die Source-Variante oder die Visual-Variante nutzen. In der Source-Variante variieren wir Text mittels Syntax. Typische Syntaxbefehle sind:\n\n*kursiv*: jeweils einen Stern vor und nach einem Wort um dieses kursiv zu schreiben\n**fett**: jeweils zwei Sterne vor und nach einem Wort um dieses fett zu schreiben\n#: Rauten für Überschriften, wobei eine Raute die erste Überschrift signalisiert, zwei Rauten die zweite usw.\n![Bildunterschrift](Link des Bildes): um Bilder einzufügen\n[Linktext](url): Um Links einzufügen\n\nMöchten wir übrigens die oben genutzten Symbole im Text nutzen, so können wir mit einem  vor dem jeweiligen Symbol die Formatierung umgehen.\nWahlweise können wir auch den Visual-Modus nutzen, indem wir oben in der Dokumentleiste von Sourceauf Visual umstellen. In diesem Modus erhalten wir ein Word-ähnliches Interface und können Formatoptionen durch Klicken auf die jeweilige Formatierung umsetzen:\n\n\n\nFormatierungsoptionen im Visual Modus\n\n\n\n\n6.3 Code Chunks\nInnerhalb von Markdown und Quarto können wir Codebefehle direkt in unser Dokument innerhalb von sogenannten Codechunks integrieren. Hier können wir alle Arten von Code schreiben sowie diese mit Hilfe von # direkt kommentieren (alles hinter einer Raute wird dabei nicht ausgeführt). Codechunks beginnen mit drei Backticks und einem r in geschweiften Klammern und enden wieder mit drei Backticks:\n\n```{r}\n# Dies ist ein Code Chunk\n```\n\nUm die Codechunks zu erzeugen können wir einfach auf Code -&gt; Insert Codechunk gehen, auf das +C-Symbol in der Dokumentmenüleiste oder den Shortcut Alt + Strg + I nutzen. Innerhalb der Chunks können wir Code schreiben und ausführen. Dies geschieht für eine einzelne Codezeile mit dem Shortcut Strg + Enter und für den gesamten Codechunk mit dem Shortcut Strg + Shift + Enter. Wahlweise könnt ihr auch den kleinen grünen Pfeil in der rechten oberen Ecke des Chunks, Code -&gt; Run Selected Lines oder den Punkt Run in der Quarto-Dokumentleiste auswählen.\nZusätzlich können wir hinter dem {r} angeben, wie R mit dem Code des Chunks umgehen soll. Wir können beispielsweise auswählen, ob R den Code ausführen soll (eval = T/F) ob der Codebereich in unserem Enddokument aufgeführt sein soll (echo = T) oder wir lediglich die Ergebnisse angezeigt wollen (echo = F) oder ob wir beispielsweise Warnungen (warnings = T/F) oder Messages (message = T/F) in unserem Output-Dokument wünschen:\n\n```{r, echo = T}\n# Dies ist ein Code Chunk\n```\n\nWahlweise können wir diese Optionen auch für das Gesamtdokument im YAML-Header festlegen. Dafür nutzen wir den Zusatz execute: und geben anschließend alle unsere Dokumentoptionen (für einen Überblick siehe hier) an.\n\n\n\nCodeoptionen im Quarto Header\n\n\nAchtung: wir nutzen hier : statt = und schreiben true und false statt TRUE/T und FALSE/F."
  },
  {
    "objectID": "Skript_1.3.html#übergreifende-menüpunkte",
    "href": "Skript_1.3.html#übergreifende-menüpunkte",
    "title": "Die Logik von Markdown und Quarto",
    "section": "5 Übergreifende Menüpunkte",
    "text": "5 Übergreifende Menüpunkte\nUnterhalb der allgemeinen RStudio-Menüleiste findet ihr eine eigene Markdown und Quarto-Dokumentleiste:\n\n\n\nDokumentmenüleistet\n\n\nHier könnt ihr euer Dokument speichern (Diskettensymbol), in eurem Dokument suchen und ersetzen (Lupensymbol) oder euer Dokument Rendern (Quarto) oder Knitten (Markdown). Hiermit rendert ihr euer Dokument von R in euer gewünschtes Outputformat (Word, Pdf, oder Html). Zusätzlich könnt ihr mit dem Zahnrad Optionen für den Umgang mit dem gerenderten Dokument und den Codechunks festlegen. Zusätzlich könnt ihr hier neue Codechunks anlegen (+C-Symbol auf der rechten Seite) oder die bestehenden Code-Chunks ausführen (Run-Symbol)"
  },
  {
    "objectID": "Skript_6.4.html#die-varianzanalyse",
    "href": "Skript_6.4.html#die-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Die Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(knitr,car, tidyverse, ggplot2, ggpubr, DescTools, dplyr,afex, emmeans, PMCMRplus, sjmisc)\n\nAnschließend laden wir unseren Datensatz:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\ntheme_set(theme_classic())"
  },
  {
    "objectID": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse",
    "text": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n2.1 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden."
  },
  {
    "objectID": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "href": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "title": "Die Varianzanalyse",
    "section": "2.2 Überprüfung der Homogenität der Fehlervarianzen",
    "text": "2.2 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n1  leveneTest(trustges ~ gesund, data = ., center = mean)\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n2  leveneTest(trustges ~ gesund*agef, data = ., center = mean)\n\n\n1\n\nInnerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an.\n\n2\n\nHaben wir mehrere unabhängige Variablen so können wir diese mit einem * verbinden. Anschließend wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value     Pr(&gt;F)    \ngroup    4  5.9109 0.00009738 ***\n      3470                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value       Pr(&gt;F)    \ngroup    9  5.2437 0.0000004058 ***\n      3465                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test wie Tamhame T2 wählen."
  },
  {
    "objectID": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "href": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "title": "Die Varianzanalyse",
    "section": "1.2 Einfaktorielle Varianzanalyse (ohne Messwiederholung)",
    "text": "1.2 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir überprüfen, inwiefern sich der Gesundheitszustand (Variable gesund; Von sehr gut bis schlecht) auf das Vertrauen in das Gesundheitswesen (Variable trustges) auswirkt.\n\ndaten %&gt;% \n1  group_by(gesund) %&gt;%\n  summarise(Mittelwert = mean(trustges, na.rm = T),\n            Standardabweichung = sd(trustges, na.rm = T)) %&gt;% \n2  kable(digits = 2, col.names = c(\"Gesundheitszustand\", \"M\", \"SD\"), caption = \"Descriptives Vertrauen\")\n\n\n1\n\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion.\n\n2\n\nIm nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\n\n\n\nDescriptives Vertrauen\n\n\nGesundheitszustand\nM\nSD\n\n\n\n\nSEHR GUT\n5.13\n1.45\n\n\nGUT\n4.99\n1.32\n\n\nZUFRIEDENSTELLEND\n4.85\n1.37\n\n\nWENIGER GUT\n4.82\n1.48\n\n\nSCHLECHT\n4.71\n1.55\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass das Vertrauen in das Gesundheitswesen am höchsten ausgeprägt ist, bei Personen die einen guten Gesundheitszustand aufweisen. Ob dieser augenscheinliche Unterschied auch statistisch signifikant ist, möchten wir in einem nächsten Schritt mit der einfaktoriellen ANOVA berechnen. Dafür nutzen wir die Funktion aov_car aus dem afex-Packag:\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \n1aov_car(trustges ~ gesund + Error(respid), data = ., anova_table = \"pes\")\n\nprint(fit)\n\n\n1\n\nInnerhalb der Funktion aov_car müssen wir zunächst die abhängige Variable (trustges) angeben und nach einer Tilde die unabhängige Variabel (gesund). Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier respid) angeben. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: trustges\n  Effect      df  MSE        F  pes p.value\n1 gesund 4, 3470 1.91 5.46 *** .006   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1"
  },
  {
    "objectID": "Skript_6.4.html#posthoctests",
    "href": "Skript_6.4.html#posthoctests",
    "title": "Die Varianzanalyse",
    "section": "3.4 PostHocTests",
    "text": "3.4 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen:\n\n1emmeans::emmeans(fit, specs = \"gesund\") %&gt;%\n  pairs()\n\n\n1\n\nInnerhalb von emmeans können wir einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\n\n\n contrast                        estimate     SE   df t.ratio p.value\n SEHR GUT - GUT                    0.1336 0.0696 3470   1.919  0.3075\n SEHR GUT - ZUFRIEDENSTELLEND      0.2749 0.0734 3470   3.746  0.0017\n SEHR GUT - WENIGER GUT            0.3077 0.0924 3470   3.332  0.0078\n SEHR GUT - SCHLECHT               0.4123 0.1474 3470   2.797  0.0414\n GUT - ZUFRIEDENSTELLEND           0.1413 0.0567 3470   2.490  0.0930\n GUT - WENIGER GUT                 0.1741 0.0798 3470   2.183  0.1864\n GUT - SCHLECHT                    0.2787 0.1398 3470   1.993  0.2696\n ZUFRIEDENSTELLEND - WENIGER GUT   0.0328 0.0831 3470   0.395  0.9949\n ZUFRIEDENSTELLEND - SCHLECHT      0.1374 0.1418 3470   0.969  0.8691\n WENIGER GUT - SCHLECHT            0.1046 0.1524 3470   0.686  0.9595\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\n\nBei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus.\n\ndaten %&gt;% \naov(trustges ~ gesund, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n                  SEHR GUT GUT    ZUFRIEDENSTELLEND WENIGER GUT\nGUT               0.4692   -      -                 -          \nZUFRIEDENSTELLEND 0.0029   0.1037 -                 -          \nWENIGER GUT       0.0174   0.3186 1.0000            -          \nSCHLECHT          0.1230   0.5471 0.9923            0.9996"
  },
  {
    "objectID": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "href": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "title": "Die Varianzanalyse",
    "section": "1.7 Exkurs: Kruskal Wallis Test",
    "text": "1.7 Exkurs: Kruskal Wallis Test"
  },
  {
    "objectID": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "3.8 Mehrfaktorielle Varianzanalyse",
    "text": "3.8 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Variablen zum Gesundheitszustand (gesund) zusätzlich die Variable Alter (agef) um das Vertrauen in das Gesundheitssystem (trustges) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(gesund, agef) %&gt;% \n  summarise(Mittelwert = mean(trustges, na.rm = T), \n            Standardabweichung = SD(trustges, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Gesundheitszustand\", \"Alter\", \"M\", \"SD\"), caption = \"Descriptives Vertrauen\")\n\n\nDescriptives Vertrauen\n\n\nGesundheitszustand\nAlter\nM\nSD\n\n\n\n\nSEHR GUT\njung\n5.15\n1.42\n\n\nSEHR GUT\nalt\n5.05\n1.57\n\n\nGUT\njung\n4.89\n1.30\n\n\nGUT\nalt\n5.14\n1.35\n\n\nZUFRIEDENSTELLEND\njung\n4.58\n1.42\n\n\nZUFRIEDENSTELLEND\nalt\n5.01\n1.31\n\n\nWENIGER GUT\njung\n4.50\n1.60\n\n\nWENIGER GUT\nalt\n4.97\n1.40\n\n\nSCHLECHT\njung\n4.41\n1.95\n\n\nSCHLECHT\nalt\n4.82\n1.39\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \n  afex::aov_car(trustges ~ gesund * agef + Error(respid),\n                                     data = ., anova_table = \"pes\")\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: trustges\n       Effect      df  MSE         F  pes p.value\n1      gesund 4, 3465 1.89  7.66 *** .009   &lt;.001\n2        agef 1, 3465 1.89 14.63 *** .004   &lt;.001\n3 gesund:agef 4, 3465 1.89    2.93 * .003    .020\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon",
    "text": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs: nicht parametrische Tests",
    "text": "3.2 Exkurs: nicht parametrische Tests\nMann-Whitney-U & Wilcoxon"
  },
  {
    "objectID": "Skript_3.1.html",
    "href": "Skript_3.1.html",
    "title": "Die Kunst vom Umgang mit Daten",
    "section": "",
    "text": "Bild von Jae Rue auf Pixabay\n\n\nIn einer Ära, in der Daten in nahezu allen Bereichen unseres Lebens eine immer größere Rolle spielen, ist die Fähigkeit, mit Daten effektiv umzugehen, zu einer entscheidenden Kompetenz geworden. Ob in der Wissenschaft, der Wirtschaft oder im Alltag – die Kunst der Datenanalyse ermöglicht es uns, aus einer Fülle von Informationen wertvolle Erkenntnisse zu gewinnen und informierte Entscheidungen zu treffen.\nIn diesem Kapitel werden wir uns eingehend mit den wesentlichen Aspekten des Umgangs mit Daten auseinandersetzen. Wir beginnen mit einer Betrachtung der verschiedenen Datentypen und Strukturen, die uns begegnen. Von numerischen Werten über kategoriale Daten bis hin zu sogenannten logischen Werten – wir werden verstehen, wie wir Daten in R effizient repräsentieren können.\nDie Kunst des Umgangs mit Daten erstreckt sich jedoch weit über das bloße Verständnis von Datentypen hinaus. Wir werden lernen, wie wir Daten auswählen, manipulieren und transformieren können, um sie optimal für unsere Analysezwecke vorzubereiten. Dabei werden wir uns Techniken ansehen, die es ermöglichen, relevante Informationen zu extrahieren und Daten in die gewünschte Form zu bringen.\nEine besonders mächtige Fähigkeit auf unserer Reise ist die Fähigkeit, Daten visuell zu interpretieren. Wir werden in die Welt von ‘ggplot2’ eintauchen, eines R-Pakets zur Erstellung ansprechender und aussagekräftiger Grafiken. Wir werden entdecken, wie wir Daten effektiv visualisieren können, um Muster, Trends und Abhängigkeiten sichtbar zu machen.\nDieses Kapitel bietet eine umfassende Einführung in die Kunst des Umgangs mit Daten. Wir werden praktische Fertigkeiten erlernen und gleichzeitig die theoretischen Grundlagen hinter diesen Techniken verstehen."
  },
  {
    "objectID": "Skript_3.2.html#datentypen-in-r",
    "href": "Skript_3.2.html#datentypen-in-r",
    "title": "Datentypen und -strukturen",
    "section": "1.1 Datentypen in R",
    "text": "1.1 Datentypen in R\nIn der Datenanalyse ist die korrekte Interpretation und Handhabung von verschiedenen Datentypen von entscheidender Bedeutung. R bietet eine Vielzahl von Datentypen, die es uns ermöglichen, eine breite Palette von Informationen zu repräsentieren und zu verarbeiten. In diesem Kapitel werden wir uns mit den fünf grundlegenden Datentypen in R vertraut machen: Numeric, Integer, Logical, Character und Factor.\n\n1.1.1 Numeric\nDer Datentyp Numeric repräsentiert dezimale Zahlen. Beispiele für numerische Daten sind 4.5, -12, 0.75 usw. Numerische Daten werden oft für Berechnungen und mathematische Operationen verwendet.\n\n\n1.1.2 Integer\nDer Integer-Datentyp repräsentiert ganze Zahlen. Beachten Sie, dass Ganze Zahlen ebenfalls zum Datentyp NUmeric gehören, da diese als spezielle Art von Dezimalzahlen betrachtet werden - dabei gilt: alle ganzen Zahlen (Integer-Datentyp) sind numerische Daten, aber nicht alle numerischen Daten (Numeric-Datentyp) sind ganze Zahlen. Beispiele für Integer-Daten sind 4, -7, 100 usw.\n\n\n1.1.3 Logical\nDer Logical-Datentyp besteht aus sogenannten booleschen Werten, die genau zwei verschiedene Ausprägungen annehmen können: entweder TRUE (wahr) oder FALSE (falsch). Boolesche Werte werden häufig in logischen Ausdrücken und Bedingungen verwendet, um Entscheidungen zu treffen und Filter anzuwenden.\n\n\n1.1.4 Character\nDer Character-Datentyp repräsentiert Textwerte oder Zeichenketten. Textwerte werden in R in doppelten Anführungszeichen (” “) oder einfachen Anführungszeichen (’ ’) eingeschlossen. Beispiele für Character-Daten sind”Hallo”, “Datenanalyse” usw.\n\n\n1.1.5 Factor\nDer Factor-Datentyp wird für kategoriale Variablen verwendet, die eine begrenzte Anzahl von Merkmalsausprägungen (levels) haben. Ein Factor besteht aus den Merkmalsausprägungen selbst und optionalen Bezeichnungen (labels) für diese Ausprägungen. Factors sind hilfreich, um kategorische Daten in einer strukturierten und sinnvollen Weise zu speichern und zu analysieren.\nIn den kommenden Abschnitten dieses Kapitels werden wir uns ausführlicher mit jedem dieser Datentypen befassen. Wir werden lernen, wie man Daten erfasst, speichert und manipuliert, um aussagekräftige Analysen durchzuführen. Die korrekte Handhabung der Datentypen bildet die Grundlage für jede erfolgreiche Datenanalyse in R."
  },
  {
    "objectID": "Skript_3.2.html#skalenniveaus",
    "href": "Skript_3.2.html#skalenniveaus",
    "title": "Datentypen und -strukturen",
    "section": "1.2 Skalenniveaus",
    "text": "1.2 Skalenniveaus\nBei der quantitativen Datenanalyse ist es von entscheidender Bedeutung, die verschiedenen Skalenniveaus von Variablen zu verstehen. Skalenniveaus geben an, wie die Werte einer Variable gemessen werden und welchen Interpretationsraum sie besitzen. Die Kenntnis dieser Skalenniveaus hilft uns dabei, angemessene statistische Methoden anzuwenden und aussagekräftige Schlussfolgerungen aus unseren Analysen zu ziehen.\n\n1.2.1 Nominales Skalenniveau\nAm niedrigsten in der Hierarchie der Skalenniveaus befindet sich das nominale Skalenniveau. Hierbei werden Werte einer Variable lediglich zur Kategorisierung genutzt, ohne dass eine Reihenfolge oder quantitative Bedeutung besteht. Ein klassisches Beispiel wäre die Kategorisierung von Geschlechtern. Nominalskalierte Variablen können lediglich auf Gleichheit oder Ungleichheit überprüft werden.\n\n\n1.2.2 Ordinales Skalenniveau\nEin Schritt weiter ist das ordinale Skalenniveau. Hierbei können die Werte einer Variable nicht nur kategorisiert werden, sondern es besteht auch eine natürliche Ordnung zwischen den Kategorien. Ein Beispiel hierfür wäre die Bewertung von Produkten auf einer Skala von “schlecht”, “mittel” bis “gut”. Allerdings sind die Abstände zwischen den Kategorien nicht quantitativ interpretierbar.\n\n\n1.2.3 Intervall-Skalenniveau\nDas Intervall-Skalenniveau ermöglicht nicht nur die Kategorisierung und Ordnung von Werten, sondern auch die Quantifizierung von Abständen zwischen den Werten. Bei dieser Skala ist die Differenz zwischen zwei Werten stets konstant, jedoch besitzt der Wert “null” keine inhärente Bedeutung. Ein Beispiel wäre die Temperaturskala in Celsius, bei der eine Temperaturdifferenz von 10 Grad Celsius immer gleichbedeutend ist, aber das Fehlen von Wärme (0 Grad Celsius) nicht bedeutet, dass keine Temperatur vorhanden ist.\n\n\n1.2.4 Verhältnis-Skalenniveau\nDas höchste Skalenniveau ist das Verhältnis-Skalenniveau. Hierbei besitzen die Werte nicht nur eine Ordnung und gleichbleibende Abstände, sondern der Wert “null” hat auch eine klare und inhärente Bedeutung. Dies ermöglicht die Durchführung von mathematischen Operationen wie Multiplikation und Division auf den Werten. Beispiele hierfür sind Gewicht, Größe oder Einkommen.\n\n\n1.2.5 Skalenniveau versus Datentyp\nEs ist wichtig zu beachten, dass die allgemeinen Skalenniveaus nicht eins zu eins den spezifischen Datentypen in R entsprechen. Zum Beispiel können kategoriale Variablen in R als Faktoren dargestellt werden. Diese können das Skalenniveau nominal oder ordinal aufweisen - um welches von den beiden es sich handelt, kann nicht mit R bestimmt werden. Numerische Variablen können ebenfalls unterschiedliche Skalenniveaus haben. Ob eine Variable dabei intervall- oder verhältnisskaliert ist muss durch die Analystin/den Analysten bestimmt werden. Dieser Unterschied zwischen den Konzepten der allgemeinen Skalenniveaus und den Datentypen in R erfordert eine sorgfältige Betrachtung, um sicherzustellen, dass wir unsere Daten angemessen interpretieren und analysieren."
  },
  {
    "objectID": "Skript_3.2.html#datenstrukturen-in-r",
    "href": "Skript_3.2.html#datenstrukturen-in-r",
    "title": "Datentypen und -strukturen",
    "section": "1.3 Datenstrukturen in R",
    "text": "1.3 Datenstrukturen in R\nIn R sind Datenstrukturen essentiell, um Informationen auf organisierte und effiziente Weise zu speichern und zu verarbeiten. In diesem Kapitel werden wir uns mit drei grundlegenden Datenstrukturen vertraut machen: Vektor, Matrix und Datenrahmen.\n\n1.3.1 Der Vektor\nDer Vektor ist die essentiellste und einfachste Datenstruktur in R. Ein Vektor ist eindimensional und enthält eine geordnete Sammlung von Elementen desselben Datentyps, er kann also entweder aus numerischen Werten, Integers (ganzen Zahlen), logischen Werten oder Characters (Textwerten) bestehen. Vektoren sind fundamental für viele Berechnungen und Operationen in R. Vektoren können in R mithilfe der Funktion c() erstellt werden, die die Elemente durch Kommas trennt. Im folgenden Beispiel erstellen wir für verschiedene Datentypen, die wir im vorherigen Kapitel kennengelernt haben, Beispiel-Vektoren und ordnen diesen Vektoren Namen zu.\n\n1a &lt;- c(1,2,3,4,6,-4)\n2b &lt;- c(1L,2L,3L)\n3c &lt;- c(\"one\",\"two\",\"three\")\n4d &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\n1\n\nnumeric\n\n2\n\ninteger\n\n3\n\ncharacter\n\n4\n\nlogical\n\n\n\n\nWir haben nun mit der Funktion c() Vektoren erstellt mit verschiedenen Datentypen. Wollen wir einen Vektor des Datentyps numeric erstellen, können wir diesen mit Zahlen befüllen, wie es bei Vektor a der Fall ist. Wollen wir hingegen spezifizieren, dass es sich ausschließlich um ganze Zahlen handelt und der Datentyp des Vektors daher integer ist, müssen wir dies mit einem L hinter den Zahlen angeben, wie wir es für Vektor b gemacht haben. Zur Erstellung eines Vektors des Datentyps character setzen wir die Werte in doppelte oder einfache Anführungszeichen, wie bei Vektor c. Für einen logischen Datentyp, befüllen wir den Vektor mit den Werten TRUE und False, wie in Beispiel d. Wollen wir uns den Datentyp eines Vektors anzeigen lassen, können wir dies mit dem Befehl class() machen.\n\nclass(a)\n\n[1] \"numeric\"\n\nclass(b)\n\n[1] \"integer\"\n\nclass(c)\n\n[1] \"character\"\n\nclass(d)\n\n[1] \"logical\"\n\n\nWollen wir uns bestimmte Werte in einem Vektor anzeigen lassen, können wir diese mit eckigen Klammern in R auswählen. Wollen wir beispielsweise den Wert, der an zweiter Stelle steht im Vektor a, machen wir das wie folgt:\n\na[2]\n\n[1] 2\n\n\nWollen wir den zweiten, dritten und vierten Wert von Vektor a, können wir durch einen Doppelpunkt die Spanne zwischen dem zweiten und vierten Wert angeben:\n\na[2:4]\n\n[1] 2 3 4\n\n\nWollen wir einen Vektor des fünften Datentyps factor erstellen, müssen wir etwas anders vorgehen als bei den anderen. Hier starten wir zunächst mit der Erstellung eines Vektors mit der Funktion c() und wandeln diesen dann anschließend mit der Fuktion factor() zum gewünschten Datentyp um. In folgendem Beispiel erstellen wir einen Vektor, der Daten für die Nutzung von verschiedenen Verkehrsmitteln enthalten soll. Zuerst erstellen wir den numerischen Vektor, der die Verkehrsmittel als Zahlenwerte enthält.\n\nverkehrsmittel &lt;- c(1,2,3,4,2,3,1,2,5,3,2,1,3,2,4,1)\nclass(verkehrsmittel)\n\n[1] \"numeric\"\n\n\nAnschließend können wir den numerischen Vektor in einen des Datentyps factor umwandeln und den verschiedenen Verkehrsmitteln jeweils ein Label zuordnen. Bei der Zuordnung der Labels müssen wir die Reihenfolge beachten: Das erste Label, das wir vergeben, wird dem niedrigsten Zahlenwert (in unserem Fall 1) zugeordnet, das zweite dem zweitniedrigsten (2) und so weiter.\n\nfactor_verkehrsmittel &lt;- factor(verkehrsmittel, labels=c(\"bus\",\"zug\",\"fahrrad\",\"auto\",\"andere\"))\nclass(factor_verkehrsmittel)\n\n[1] \"factor\"\n\n\nLassen wir uns den zuerst erstellten numerischen Vektor verkehrsmittel anzeigen, bekommen wir die numerischen Werte:\n\nverkehrsmittel\n\n [1] 1 2 3 4 2 3 1 2 5 3 2 1 3 2 4 1\n\n\nLassen wir uns hingegen den in einen factor umgewandelten Vektor factor_verkehrsmittel anzeigen, bekommen wir die Werte mit ihren entsprechenden Labeln angezeigt. In der letzten Zeile des Outputs sehen wir außerdem, welche möglichen Merkmalsausprägungen vorkommen können (siehe Levels):\n\nfactor_verkehrsmittel\n\n [1] bus     zug     fahrrad auto    zug     fahrrad bus     zug     andere \n[10] fahrrad zug     bus     fahrrad zug     auto    bus    \nLevels: bus zug fahrrad auto andere\n\n\n\n\n1.3.2 Die Matrix\nNeben der essentiellen Datenstruktur des Vektors gibt es noch weitere Strukturen, die wir kennen müssen, um mit R Daten zu analysieren. Eine davon ist die Matrix, eine zweidimensionale Datenstruktur, die aus Zeilen und Spalten von Elementen besteht. Alle Elemente in einer Matrix müssen denselben Datentyp aufweisen. Matrizen können in R mit der Funktion matrix() erstellt werden. Grundlage hierfür ist wieder zunächst ein Vektor, erstellt mit der c(), der dann mit der Funktion matrix() umgewandelt wird:\n\nvec_1 &lt;- c(1,2,3,4,5,6,7,8,9)\nmatrix_1 &lt;- matrix(vec_1)\nmatrix_1\n\n      [,1]\n [1,]    1\n [2,]    2\n [3,]    3\n [4,]    4\n [5,]    5\n [6,]    6\n [7,]    7\n [8,]    8\n [9,]    9\n\n\nIn diesem Beispiel erstellen wir zunächst mit der Funktion c() einen numerischen Vektor namens vec_1. Diesen wandeln wir mit der Funktion matrix() in eine Matrix um, die wir matrix_1 nennen. Dann lassen wir uns matrix_1 anzeigen. Wir können sehen, dass die erstellte Matrix aus einer Spalte und neun Reihen besteht, die mit den Zahlenwerten des Vektors vec_1 befüllt wurden. Wir können die Anzahl der Spalten und Reihen in der Matrix mit den Argumenten ncol (number of columns = Spaltenanzahl) oder nrow (number of rows = Reihenanzahl) verändern:\n\nmatrix_2 &lt;- matrix(vec_1,ncol=3)\nmatrix_2\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nFür matrix_2 verwenden wir wieder den Vektor vec_1, haben dieses Mal aber angegeben, dass die Matrix drei Spalten (ncol = 3) haben soll. Die Anzahl an Reihen ergibt sich damit automatisch. Anstatt der Spaltenanzahl, können wir aber auch angeben, dass die Matrix drei Reihen haben soll (nrow = 3), in diesem Fall ergibt sich die Anzahl an Spalten automatisch:\n\nmatrix_3 &lt;- matrix(vec_1,nrow=3)\nmatrix_3\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nWir sehen, dass matrix_3 identisch ist mit matrix_2, beide haben jeweils drei Reihen und drei Spalten. Eine Matrix wird also immer befüllt mit allen Werten des Vektors, auf dem sie basiert. Versuchen wir, die Spalten- und Zeilenanzahl so festzulegen, dass die Matrix nicht alle Werte des Vektors enthalten kann - geben wir also eine konfligierende Spalten- und Zeilenanzahl an - erhalten wir eine Warnung:\n\nmatrix_4 &lt;- matrix(vec_1,ncol=3,nrow=2)\nmatrix_4\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n1.3.3 Der Datenrahmen bzw. Dataframe\nNeben dem eindimensionalen Vektor und der zweidimensionalen Matrix, gibt es noch eine weitere grundlegende Datenstruktur, mit der wir in der quantitativen Datenanalyse arbeiten: der Datenrahmen. Hierbei handelt es sich um eine zweidimensionale Datenstruktur, die ähnlich einer Tabelle aussieht. Im Gegensatz zur Matrix ermöglicht der Datenrahmen die Speicherung unterschiedlicher Datentypen in verschiedenen Spalten, was besonders hilfreich ist, wenn wir mit realen Datensätzen arbeiten.\nIn einem Datenrahmen repräsentiert eine “Variable” eine einzelne Messgröße oder Eigenschaft. Variablen können verschiedene Datentypen aufweisen (numerische Werte, Texte, logische Werte oder kategoriale Merkmale). Jede Variable wird durch eine Spalte im Datenrahmen dargestellt. Eine “Beobachtung” hingegen entspricht einer einzelnen Einheit, von der wir Daten gesammelt haben. Beobachtungen werden als Zeilen im Datenrahmen angeordnet. Jede Zeile enthält die Datenwerte für jede der zugehörigen Variablen.\nBeispielsweise könnte eine Beobachtung in einem Datenrahmen zur Erfassung von Studierendendaten Informationen wie den Namen, das Alter und die erreichte Note für einen bestimmten Kurs enthalten. Zusammen repräsentieren Variablen und Beobachtungen die Informationen, die wir in einem Datensatz haben. Der Datenrahmen organisiert diese Informationen in einer tabellarischen Struktur, die es uns ermöglicht, Muster, Trends und Beziehungen zwischen den Variablen und Beobachtungen zu identifizieren. Dies ist der Grundstein quantitative Datenanalysen. Um in R einen Datenrahmen zu erstellen, bilden Vektoren wieder die Grundlage. Diese dienen als Variablen, d.h. als Spalten, in unserem Datenrahmen und werden mit dem Befehl data.frame() zusammengefügt:\n\nname &lt;- c('Esra','Mara','Adam','Luca')\nalter &lt;- c(22, 19, 18, 24)\nnote &lt;- c(1.3, 2.0, 1.7, 2.0)\n\nstudierendendaten &lt;- data.frame(name,alter,note)\n\nIn diesem Beispiel erstellen wir zunächst mit der Funktion c() die Vektoren (Variablen) name, alter und note mit jeweils vier Werten. Anschließend erstellen wir aus diesen einen Datenrahmen mit dem Befehl data.frame(), der die Variablen durch Kommata trennt. Lassen wir uns den erstellten Datenrahmen, den wir studierendendaten genannt haben, anzeigen, sehen wir, dass jede Zeile eine Beobachtung enthält, nämlich den Namen, das Alter und die Note einer Person. Jede Spalte enthält eine der drei Variablen:\n\nstudierendendaten\n\n  name alter note\n1 Esra    22  1.3\n2 Mara    19  2.0\n3 Adam    18  1.7\n4 Luca    24  2.0"
  },
  {
    "objectID": "Skript_3.2.html#zusammenfassung",
    "href": "Skript_3.2.html#zusammenfassung",
    "title": "Datentypen und -strukturen",
    "section": "1.4 Zusammenfassung",
    "text": "1.4 Zusammenfassung\nIn diesem Kapitel haben wir die Grundlagen der Datentypen, Skalenniveaus und Datenstrukturen erkundet, die die Grundlage für unsere quantitative Datenanalyse bilden. Wir haben gelernt, wie unterschiedliche Datentypen uns erlauben, verschiedene Arten von Informationen darzustellen, und wie Skalenniveaus die Bedeutung von Variablenwerten verdeutlichen. Weiterhin haben wir Datenstrukturen wie Vektoren, Matrizen und Datenrahmen kennengelernt, die uns ermöglichen, diese Informationen strukturiert zu speichern und zu verarbeiten. Im folgenden Kapitel werden wir lernen, wie wir Variablen und einzelne Beobachtungen auswählen, manipulieren und transformieren zu können, um sie auf unsere Analysezwecke anzupassen."
  },
  {
    "objectID": "Skript_3.3.html#die-tidyverse-schreibweise-und-der-pipe-operator",
    "href": "Skript_3.3.html#die-tidyverse-schreibweise-und-der-pipe-operator",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.1 Die Tidyverse-Schreibweise und der Pipe-Operator %>%",
    "text": "1.1 Die Tidyverse-Schreibweise und der Pipe-Operator %&gt;%\nIn der tidyverse-Logik verwenden wir eine bestimmte Schreibweise, die es uns ermöglicht, viele Funktionen zu kombinieren und unseren Code trotzdem einfach verständlich und leserlich zu halten. Dabei ist der Pipe-Operator %\\&gt;% eines der markantesten Merkmale des Tidyverse und dient dazu, komplexe Datenverarbeitungsketten auf eine lesbarere und effizientere Weise zu erstellen. Die Pipe ermöglicht es, die Ergebnisse einer vorherigen Operation als Eingabe für die nächste Operation zu verwenden, ohne explizit Zwischenvariablen erstellen zu müssen. Dies fördert nicht nur die Lesbarkeit des Codes, sondern verringert auch die Wahrscheinlichkeit von Fehlerquellen.\nDie grundlegende Logik der Pipe %\\&gt;% ist wie folgt:\ndaten %\\&gt;% operation_1 %\\&gt;% operation_2 %\\&gt;% operation_3\nHier ist daten der Ausgangspunkt (Datensatz oder Objekt), auf dem verschiedene Operationen nacheinander ausgeführt werden. Jede Operation erzeugt eine modifizierte Version des vorherigen Objekts, die dann als Eingabe für die nächste Operation verwendet wird. Dies erzeugt eine Kette von Operationen, die von links nach rechts ausgeführt werden."
  },
  {
    "objectID": "Skript_3.3.html#daten-selektion-und-subset-bildung",
    "href": "Skript_3.3.html#daten-selektion-und-subset-bildung",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.2 Daten-Selektion und Subset-Bildung",
    "text": "1.2 Daten-Selektion und Subset-Bildung\nDie Datenanalyse beginnt oft damit, die relevanten Informationen aus einem umfangreichen Datensatz auszuwählen. Hierbei können wir uns auf bestimmte Variablen (Spalten im Datensatz) konzentrieren, die für unsere Fragestellung von Interesse sind. Das “dplyr”-Paket in R bietet uns eine einfache und mächtige Möglichkeit, diese Daten auszuwählen.\n\n1.2.1 Auswahl von Spalten\nDie Funktion select() ermöglicht es uns, Spalten basierend auf ihren Namen auszuwählen. Dies ist besonders hilfreich, wenn wir nur bestimmte Aspekte unserer Daten benötigen. Wollen wir beispielsweise aus unserem ALLBUS-Datensatz ein Subset ziehen, dass nur das Alter der Befragten enthält und die Variablen, die mit Vertrauen in Institutionen zu tun haben, können wir dies wie folgt machen:\n\nvertrauen_institutionen &lt;- daten %&gt;% \n  select(age, pt01, pt02, pt03, pt04, pt06, pt07, pt08, pt09, pt10, pt11, pt12, pt14, pt15, pt19, pt20)\n\nIn diesem Beispiel generieren wir einen neuen Datenrahmen, dem wir den Namen vertrauen_institutionen zuordnen. Innerhalb der Funktion select() geben wir zuerst den Datensatz an, aus dem wir Variablen auswählen wollen. Dieser heißt in unserem Fall daten. Darauf folgt nach einem Komma die Auflistung jener Variablen anhand ihrer Namen im Datensatz, die wir auswählen wollen. Ein Blick in das Codebook des ALLBUS-Datensatzes zeigt uns, welche Variablen mit dem Vertrauen in Institutionen zu tun haben (STRG+F zur Suche im Dokument nach “Vertrauen in Institutionen”, Auflistung der Variablen in der Tabelle auf Seite xiii). Mit dem Befehl View(), oder indem wir auf den generierten Datensatz in unserem Global Environment (RStudio-Schaltfläche 3) klicken, können wir uns das Subset ansehen:\n\nView(vertrauen_institutionen)\n\nAuf eine einzelne Variable in einem Datensatz können wir mit dem Dollar-Zeichen \\$ zugreifen:\n\nvertrauen_institutionen$age \n\n&lt;labelled&lt;double&gt;[5342]&gt;: ALTER: BEFRAGTE(R)\n  [1]  54  53  89  79  62  23  31  57  68  51  57  85  55  26  38  58  54  45\n [19]  49  26  83  48  48  73  62  25  54  54  51  60  49  57  58  58  39  82\n [37]  77  79  22  77  54  50  23  25  65  56  72  52  68  55  31  79  62  67\n [55]  66  23  83  62  41  57  22  38  69  62  48  64  26  73  49  38  40 -32\n [73]  50  57  42  55  31  55  68  91  63  56  77  56  30  58  60  59  51  25\n [91]  62  59  65  50  36  25  44  59  46  44\n [ reached getOption(\"max.print\") -- omitted 5242 entries ]\n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n\n\n\n\n1.2.2 Filtern von Fällen\nNeben der Auswahl von Spalten ist es oft auch erforderlich, Fälle bzw. Zeilen basierend auf bestimmten Bedingungen auszuwählen. Hierfür verwenden wir die Funktion filter(). Angenommen wir möchten für unser Subset vertrauen_institutionen nur die Daten der Personen auswählen, die sehr großes Vertrauen in das Gesundheitswesen (Variable pt01) haben, können wir dies wie folgt machen - aus dem Codebook können wir ableiten, dass “sehr großes Vertrauen” dem Zahlenwert 7 entspricht:\n\nvertrauen_institutionen_filter1 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7)\n\nIn diesem Beispiel haben wir aus dem Subset vertrauen_institutionen erneut ein Subset gezogen, indem wir mit der Funktion filter() nur die Zeilen aus dem ursprünglichen Subset ausgewählt haben, in denen die Variable pt01 (Vertrauen in das Gesundheitswesen) den Wert 7 annimmt. Hier ist es wichtig, ein doppeltes Gleichheitszeichen zu verwenden: pt01 == 7. Wir können auch mehrere Filter gleichzeitig anwenden. Zum Beispiel können wir ein Subset ziehen, dass nur die Daten von Personen enthält, die sehr großes Vertrauen in das Gesundheitswesen haben und gar kein Vertrauen in das Bundesverfassungsgericht (pt02):\n\nvertrauen_institutionen_filter2 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7 & pt02 == 1)\n\nMüssen beide Konditionen zutreffen, damit eine Zeile ausgewählt wird, verbinden wir diese mit dem und-Zeichen &. In unserem Global Environment sehen wir, dass dies auf vier Fälle (4 obs.) zutrifft. Wollen wir hingegen alle Fälle auswählen, bei denen entweder ein sehr großes Vertrauen in das Gesundheitswesen oder gar kein Vertrauen in das Bundesverfassungsgericht besteht, trennen wir durch den senkrechten Strich |:\n\nvertrauen_institutionen_filter3 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7 | pt02 == 1)\n\nIn unserem Global Environment sehen wir, dass dies auf 486 Fälle (486 obs.) zutrifft."
  },
  {
    "objectID": "Skript_3.3.html#manipulation-und-transformation-von-daten",
    "href": "Skript_3.3.html#manipulation-und-transformation-von-daten",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.3 Manipulation und Transformation von Daten",
    "text": "1.3 Manipulation und Transformation von Daten\nIn vielen Datenanalyseprojekten ist es notwendig, Variablen zu bearbeiten, um sie für Analysen oder Visualisierungen vorzubereiten. Dieser Prozess kann das Recodieren von Werten, das Umbenennen von Variablen und die Berechnung neuer Variablen umfassen. Das “dplyr”-Paket ermöglicht uns, diese Aufgaben auf eine intuitive Weise zu erledigen. Zum Beispiel verwenden wir die Funktion mutate() für die Transformation von Variablen und das Erstellen neuer Spalten.\n\n1.3.1 Neue Spalten erstellen\nAngenommen wir möchten eine neue Variable (Spalte) für unser Subset vertrauen_institutionen erstellen, die den Wert “großes vertrauen” annehmen soll, wenn die Variable pt01 (Vertrauen in das Gesundheitswesen) einen Wert größer vier (&gt; 4) annimmt und “wenig vertrauen”, wenn pt01 einen anderen Wert annimmt, können wir dies folgermaßen erreichen:\n\ntransformierte_daten &lt;- vertrauen_institutionen %&gt;% \n  mutate(vertrauen_gesundheitswesen = ifelse(pt01 &gt; 4, \"großes vertrauen\", \"wenig vertrauen\"))\n\nIn diesem Beispiel haben wir einen neuen Datensatz generiert und ihm den Namen transformierte_daten zugewiesen. Mit den Funktionen mutate() und ifelse() haben wir basierend auf der Variable pt01 eine neue Spalte im Datensatz erstellt mit dem Namen vertrauen_gesundheitswesen. Innerhalb der Funktion mutate() haben wir dafür zunächst unseren Datensatz angegeben, in diesem Fall vertrauen_institutionen. Nach einem Komma folgt der Name der neuen Variable (Spalte) mit einem Gleichheitszeichen =. In der Funktion ifelse() geben wir erst die Kondition an, die wir testen wollen (pt01 &gt; 4), gefolgt von einem Komma und dem Wert, den die Variable annehmen soll, sofern die getestete Kondition zutrifft. Dann folgt nach einem erneuten Komma der Wert, den die Variable annehmen soll, falls die Kondition nicht zutrifft.\n\n\n1.3.2 Variablen umbenennen\nManchmal ist es sinnvoll, Variablennamen zu ändern, um sie verständlicher zu machen oder um Konventionen zu folgen. Die Funktion rename() ermöglicht das Umbenennen von Variablen:\n\ntransformierte_daten &lt;- transformierte_daten %&gt;%\n  rename(vertrauen_gesundheit = pt01)\n\nHier ändern wir den Namen der Variable pt01 im Datensatz transformierte_daten zu vertrauen_gesundheit."
  },
  {
    "objectID": "Skript_3.3.html#gruppieren-von-daten-und-aggregation",
    "href": "Skript_3.3.html#gruppieren-von-daten-und-aggregation",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.4 Gruppieren von Daten und Aggregation",
    "text": "1.4 Gruppieren von Daten und Aggregation\nEine weitere wichtige Fähigkeit ist das Gruppieren von Fällen basierend auf bestimmten Kategorien. Die Funktion group_by() aus dem dplyr-Paket ermöglicht es, Datensätze nach bestimmten Variablen zu gruppieren. Zum Beispiel könnten wir den Datensatz vertrauen_institutionen nach dem Alter der Befragten gruppieren und das durchschnittliche Vertrauen in das Gesundheitswesen pro Alter berechnen. Wollen wir den Datensatz vertrauen_institutionen nach dem Alter der Befragten gruppieren und das durchschnittliche Vertrauen in das Gesundheitswesen pro Alter berechnen, kombinieren wir die Befehle group_by(), summarize() und mean() nach der tidyverse-Logik wie folgt:\n\naggregierte_daten &lt;- transformierte_daten %&gt;%\n  group_by(age) %&gt;%\n  summarize(durchschnitt_vertrauen = mean(vertrauen_gesundheit))\n\nSehen wir uns den Datensatz aggregierte_daten mit View() oder über unser Global Enviroment an, können wir feststellen, dass dieser aus zwei Variablen (Spalten) besteht: age und durchschnitt_vertrauen. Die Spalte age enthält für jedes Alter, das im Datensatz transformierte_daten vorkommt, eine Zeile. Die Spalte durchschnitt_vertrauen gibt den durchschnittlichen Wert für die Variable vertrauen_gesundheit für jedes Alter an. Den Durchschnitt haben wir mit der Funktion mean() berechnet."
  },
  {
    "objectID": "Skript_3.3.html#umgang-mit-realen-datensätzen-und-fehlenden-werten",
    "href": "Skript_3.3.html#umgang-mit-realen-datensätzen-und-fehlenden-werten",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.5 Umgang mit realen Datensätzen und fehlenden Werten",
    "text": "1.5 Umgang mit realen Datensätzen und fehlenden Werten\nBei der Arbeit mit realen Datensätzen ist es häufig der Fall, dass diese nicht perfekt und sauber sind. Es können verschiedene Probleme auftreten, wie fehlende Daten, inkonsistente Codierungen oder unerwartete Werte. In diesem Abschnitt werden wir uns damit beschäftigen, wie Sie solche Herausforderungen bewältigen können.\nFehlende Daten sind ein häufiges Problem in Datensätzen. In R werden fehlende Werte oft mit dem Wert NA (Not Available) oder NaN (Not a Number) dargestellt. Ist dies der Fall, können wir fehlende Werte mit der Funktion drop_na() entfernen:\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  drop_na(vertrauen_gesundheit)\n\nJe nach Datensatz kann die Codierung fehlender Werte aber stark variieren. Im ALLBUS-Datensatz ist die Variable pt01 (vertrauen_gesundheit) zum Beispiel so codiert, dass fehlende Werte mit einer -9 (keine Angabe), -42 (Datenfehler: Mehrfachnennung) oder -11 (keine Teilnhame an Split A oder B) gekennzeichnet sind. Dadurch wurden durch obenstehenden Code, der drop_na verwendet keinerlei Zeilen gelöscht und der Datensatz vertrauen_institutionen_dropna entspricht vertrauen_institutionen. In diesem Fall müssen wir die spezifischen fehlenden Werte angeben, die wir herausfiltern wollen:\n\nmissing_codes &lt;- c(-9, -42, -11)\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  filter(!vertrauen_gesundheit %in% missing_codes)\n\nHier geben wir zunächst die möglichen Ausprägungen für fehlende Werte an und speichern diese als Vektor missing_codes. Dann wenden wir die Funktion filter() auf den Datensatz vertrauen_institutionen so an, dass nur Zeilen ausgewählt werden, in denen die Variable keinen der Werte in missing_codes annimmt. Das Gegenteil einer Kondition errreichen wir mit einem Ausrufezeichen ! . vertrauen_gesundheit %in% missing_code würde alle Fälle filtern, die einen der Werte in missing_codes annehmen. Setzen wir ein ! davor, erhalten wir genau den umgekehrten Fall.\nWir können auch die fehlenden Werte aus allen Vertrauensvariablen löschen:\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  filter(across(vertrauen_gesundheit:pt20, ~ !. %in% missing_codes))\n\nIn dieser Version wird die Funktion across() verwendet, um Operationen auf mehreren Spalten gleichzeitig durchzuführen. Es werden nur die Zeilen beibehalten, in denen keine fehlenden Werte (definiert in missing_codes) in den angegebenen Variablen zwischen vertrauen_gesundheit und pt20 vorkommen."
  },
  {
    "objectID": "Skript_3.3.html#erstellung-von-indizes",
    "href": "Skript_3.3.html#erstellung-von-indizes",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.6 Erstellung von Indizes",
    "text": "1.6 Erstellung von Indizes\nOft müssen aus vorhandenen Variablen neue abgeleitete Variablen berechnet werden. Dies kann beispielsweise das Berechnen von Indizes oder Skalen sein, um Zusammenfassungen oder Vergleiche zu erleichtern. Wir können zum Beispiel alle Vertrauensvariablen zu einem Index zusammenfassen:\n\nvertrauen_institutionen_index &lt;- vertrauen_institutionen_dropna %&gt;%\n  mutate(index_vertrauen = rowSums(across(vertrauen_gesundheit:pt20)))\n\nHier haben wir eine neue Spalte index_vertrauen erstellt, die die Summe der Vertrauensvariablen enthält. Wir verwenden wieder die Funktion across(), um Operationen auf mehreren Spalten gleichzeitig durchzuführen. Für jede Zeile haben wir mit dem Befehl rowSums() die Werte für alle Variablen von vertrauen_gesundheit bis pt20 addiert und den resultierenden Wert in die neue Spalte geschrieben. Anstatt der Summe können wir auch den Mittelwert der Variablen berechnen und diesen als Index verwenden, dazu teilen wir die Summe, die wir wie im Beispiel zuvor berechnen, durch die Anzahl an Vertrauensvariablen (20):\n\nvertrauen_institutionen_index_2 &lt;- vertrauen_institutionen_dropna %&gt;%\n  mutate(index_vertrauen = rowSums(across(vertrauen_gesundheit:pt20))/20)"
  },
  {
    "objectID": "Skript_4.1.html#das-skalenniveau",
    "href": "Skript_4.1.html#das-skalenniveau",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "1.1 Das Skalenniveau",
    "text": "1.1 Das Skalenniveau\nDurch eine Messung wird bestimmten Eigenschaften eines Merkmalsträgers ein bestimmter Wert zugeordnet. Dazu bedient man sich einer festgelegten Skala, damit alle Ergebnisse miteinander vergleichbar sind. Wir kennen solche Skalen aus unserem Alltag. Manche bestehen aus Zahlen (z.B. Maßbänder, Uhren), andere aus Symbolen (z.B. Vereinslogos). Das Niveau einer Skala hängt von ihrem Informationsgehalt ab. Je höher das Skalenniveau, desto vielfältiger ist die Auswahl an Methoden, die man auf die Daten anwenden kann und desto höher ist ihr Informationsgehalt. Allerdings neigen Methoden, die höhere Skalenniveaus erfordern, oft dazu, anfälliger gegenüber Ausreißern zu sein. Bei der Einschätzung des Skalenniveaus ist es hilfreich zu prüfen, ob ein Merkmal diskret oder kontinuierlich ist. Diskret ist ein Merkmal, wenn es abzählbar viele Ausprägungen annehmen kann, z.B. die Anzahl an Haustieren: Unter 2 Katzen kann sich jeder was vorstellen, bei 1,5 Katzen wird es hingegen schwierig. Dagegen gibt es bei einem kontinuierlichen Merkmal zwischen zwei Ausprägungen unendlich viele weitere, z.B. bei der Körpergröße: Hier sind nicht nur 1, 2 oder 199 cm denkbar, sondern auch jeder beliebig genaue weitere Wert dazwischen. Dafür macht eine Körpergröße von -5 cm wieder keinen Sinn.\n\n1.1.1 Nominalskala\nEs kann nur eine Aussage darüber gemacht werden, ob zwei Objekte gleich sind, aber es gibt keine sinnvolle hierarchische Einteilung, d.h. es kann keine Rangfolge im Sinne von “x ist größer/kleiner als y” erstellt werden. Beispiele: Geschlecht, Nationalität. Diese Skala setzt diskrete Merkmale voraus.\n\n\n1.1.2 Ordinalskala\nEs kann nicht nur festgestellt werden, ob sich zwei Objekte gleichen, sondern man kann ihnen Eigenschaften zuordnen, anhand derer sie sich in eine bestimmte Reihenfolge bringen lassen. Sie können also auch sortiert werden. Das kann sowohl diskrete Messwerte beinhalten, die nur nach Rängen geordnet werden können, als auch kontinuierliche Messwerte, bei denen man die Differenz zwischen zwei Werten genau bestimmen kann. Beispiele: Schulnoten,\n\n\n1.1.3 Kardinalskala\n\nIntervall: Man kann die Abstände zwischen zwei Messwerten genau berechnen. Es gibt allerdings keinen natürlichen Nullpunkt. Beispiele: Temperatur in Grad Celsius (willkürlich festgelegter Nullpunkt).\nVerhältnis: Wie die Intervallskala, aber es gibt einen natürlichen Nullpunkt. Beispiele: Distanz zwischen zwei Orten, Körpergröße."
  },
  {
    "objectID": "Skript_4.1.html#maßzahlen",
    "href": "Skript_4.1.html#maßzahlen",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "1.3 Maßzahlen",
    "text": "1.3 Maßzahlen\nWir werden hier, abhängig vom Skalenniveau, zwei verschiedene Arten von Maßzahlen betrachten. Das ist zum einen das Lagemaß, das uns etwas über die zentrale Tendenz der Daten sagt, also wo sich besonders viele Messwerte häufen bzw. wo der zentrale Punkt ist, um den sich die Messwerte gruppieren. Zum anderen schauen wir uns verschiedene Streuungsparameter an, mit deren Hilfe wir einschätzen können, wie stark unsere Messwerte vom Lageparameter abweichen."
  },
  {
    "objectID": "Skript_4.1.html#häufigkeiten",
    "href": "Skript_4.1.html#häufigkeiten",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "1.2 Häufigkeiten",
    "text": "1.2 Häufigkeiten\nZur Beschreibung insbesondere nominaler Merkmale ist der Begriff der Häufigkeit wichtig.\n\n1.2.1 Absolute Häufigkeit\nDie absolute Häufigkeit gibt an, wie oft eine bestimmte Merkmalsausprägung im Datensatz vorkommt. Die absolute Häufigkeit kann folglich nur eine natürliche Zahl sein.\n\n\n1.2.2 Relative Häufigkeit\nDie relative Häufigkeit gibt den Anteil eines bestimmten Messwertes im Datensatz an. Sie berechnet sich, indem man die absolute Häufigkeit des jeweiligen Messwertes durch die Gesamtgröße des Datensatzes teilt. Die relativen Häufigkeiten summieren sich also zu 1 auf."
  },
  {
    "objectID": "Skript_4.1.html#zu-den-programmierbeispielen",
    "href": "Skript_4.1.html#zu-den-programmierbeispielen",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "1.4 Zu den Programmierbeispielen",
    "text": "1.4 Zu den Programmierbeispielen\nIm Folgenden werden wir aus dem Allbus-2021-Datensatz ein paar Beispiele herausgreifen, um die Berechnung und Visualisierung von Häufigkeiten und Parametern zu demonstrieren. Dazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, ggplot2, haven, dplyr) \n\nDann legen wir den Visualisierungshintergrund fest:\n\ntheme_set(theme_classic()) \n\nNun laden wir den Allbus-Datensatz:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nBei den Häufigkeiten beschränken wir uns auf einen nominal skalierten Datensatz: Das Geschlecht (“sex”).\nAnschließend konvertieren wir die Daten zu Zahlenwerten und entfernen fehlerhafte Daten:\n\nallbus_messniveau_bsp = subset(daten, select=c(\"sex\")) %&gt;%\n  mutate(across(c(\"sex\"), ~ as.numeric(.))) %&gt;%\n  mutate(across(c(\"sex\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%\n  na.omit()"
  },
  {
    "objectID": "Skript_4.1.html#berechnung-und-visualisierung-von-häufigkeiten",
    "href": "Skript_4.1.html#berechnung-und-visualisierung-von-häufigkeiten",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "1.5 Berechnung und Visualisierung von Häufigkeiten",
    "text": "1.5 Berechnung und Visualisierung von Häufigkeiten\nDie absoluten Häufigkeiten lassen sich einfach mit der table-Funktion abfragen. Im folgenden Codebeispiel schauen wir uns dazu die Häufigkeiten im Datensatz “sex” an. Uns werden zwei Zeilen ausgegeben: Die erste Zeile enthält den Tabellenkopf (1: männlich, 2: weiblich, 3: divers). Die zweite Zeile enthält zu jeder Merkmalsausprägung die zugehörige Anzahl (= absolute Häufigkeit):\n\ngeschlecht_haeufigkeit_abs = table(allbus_messniveau_bsp$sex) \nstichprobengroesse = length(allbus_messniveau_bsp$sex)\n\ngeschlecht_haeufigkeit_abs        # Ausgabe der Tabelle\n\n\n   1    2    3 \n2614 2705    3 \n\n\nInsgesamt summieren sie sich (wie erwartet) zu 5322 Merkmalsträgern auf, was auch der Größe des Datensatzes entspricht:\n\nsum(geschlecht_haeufigkeit_abs)   # Ausgabe der Summe der absoluten Häufigkeiten\n\n[1] 5322\n\nstichprobengroesse          # Ausgabe der Gesamtanzahl an Merkmalsträgern im Datensatz\n\n[1] 5322\n\n\nDie relativen Häufigkeiten können wir uns ausgeben lassen, indem wir den Inhalt der Tabelle durch die Gesamtanzahl der Merkmalsträger teilen:\n\ngeschlecht_haeufigkeit_rel = geschlecht_haeufigkeit_abs / stichprobengroesse # Berechnung der relativen Häufigkeiten\ngeschlecht_haeufigkeit_rel        # Ausgabe der Tabelle mit den relativen Häufigkeiten\n\n\n           1            2            3 \n0.4911687336 0.5082675686 0.0005636979 \n\n\nErwartungsgemäß summieren sich die relativen Häufigkeiten zu 1 auf:\n\nsum(geschlecht_haeufigkeit_rel) # Ausgabe der Summe der relativen Häufigkeiten\n\n[1] 1\n\n\nEine Möglichkeit der Darstellung von Häufigkeiten ist das Kuchendiagramm. Zu diesem Zweck erstellen wir einen eigenen Dataframe:\n\ndf.sex = data.frame(\n  sex=c(\"Mann\", \"Frau\", \"divers\"), \n  frequency=geschlecht_haeufigkeit_abs\n)\n\nggplot(df.sex, aes(x=\"\", y=frequency.Freq, fill=sex)) +\n  geom_bar(stat=\"identity\") +\n  coord_polar(\"y\")\n\n\n\n\nAuf den ersten Blick lässt sich so ein generelles Bild des Datensatzes machen: Es sind in etwa so viele Männer wie Frauen im Datensatz, wobei Frauen etwas stärker vertreten sind. Personen, die sich weder als Mann noch als Frau identifizieren, kommen nur in sehr geringer Zahl vor."
  },
  {
    "objectID": "Skript_4.2.html#der-modus",
    "href": "Skript_4.2.html#der-modus",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Der Modus",
    "text": "1.1 Der Modus\nDer Modus, auch Modalwert genannt, gibt an, welche Ausprägung eines gemessenen Merkmals am häufigsten vorkommt.\n\n1.1.1 Berechnung\nWir müssen einfach nur zählen, wie oft jede Merkmalsausprägung vorkommt. Diejenige mit dem höchsten Wert (bzw. der größten absoluten Häufigkeit) ist der Modus. In R gibt es keine vorgefertigte Funktion, die diesen Parameter berechnet. Das folgende Code-Beispiel zeigt eine mögliche Lösung speziell für unseren “allbus.df$Geschlecht”-DataFrame.\n\nget_mode = function(vector){\n\n  # Häufigkeitstabelle erstellen:\n  frequencies = table(vector) \n  \n  # Höhe der größten Häufigkeit ermitteln:\n  max_freq = max(frequencies)  \n  \n  # Teiltabelle erstellen, die nur die Spalten mit der höchsten Häufigkeit enthält:\n  where_max =frequencies == max_freq \n  \n  # Namen der verbliebenen Spalten (= Modus) ermitteln:\n  modus = names(frequencies[where_max]) \n  return(modus)\n}\n#Ausgabe des Modus:\ncat(\"Der Modus lautet \", get_mode(allbus_df$Geschlecht), \".\")\n\nDer Modus lautet  1 ."
  },
  {
    "objectID": "Skript_4.2.html#der-median",
    "href": "Skript_4.2.html#der-median",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Der Median",
    "text": "1.2 Der Median\nFür mindestens ordinal skalierte Messwerte empfielt sich neben dem Modus zusätzlich der Median. Einen der Größe nach aufsteigend sortierten Datensatz teilt der Median genau in der Mitte, es liegen also genauso viele Elemente links wie rechts davon.\n\n1.2.1 Berechnung\nFür den Fall, dass die Anzahl an Elementen im Datensatz \\(n\\) ungerade ist, entspricht der Median dem Messwert, der genau in der Mitte liegt:\n\\[\nx_{Med} = x_{\\frac{n + 1}{2}}\n\\]\nIst \\(n\\) gerade, kann jedes der beiden Elemente, die in der Mitte liegen, als Median verwendet werden. Es ist aber eher üblich, beide zu addieren und dann durch zwei zu teilen:\n\\[\nx_{Med} = \\frac{1}{2} (x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})\n\\]\nR stellt eine Funktion zur Berechnung des Medians bereit. Wir schauen uns als Beispiel das Vertrauen der Allbus-Befragten in die Bundesregierung an: Im Gegensatz zum Geschlecht können wir hier eine Rangfolge festlegen, jedoch nicht die Abstände dazwischen exakt messen. Wir haben es folglich mit ordinalen Daten zu tun.\nWir verschaffen uns zunächst wieder einen Überblick mit der table-Funktion und sehen sieben verschiedene Werte.\n\ntable(allbus_df$VertrauenBR)\n\n\n  1   2   3   4   5   6   7 \n 68  79 120 169 175 155  32 \n\n\nDie Daten sind bereits von “gar nicht” zu “sehr hoch” sortiert. Wir wenden die median-Funktion an und erhalten “4” als Ausgabe:\n\nmedian_vertrauen = median(allbus_df$VertrauenBR)\n\n\n\n1.2.2 Visualisierung\nWenn man sich die Daten als Säulendiagramm bzw. Barplot ausgeben lässt und den Median einzeichnet (im folgenden Codebeispiel die rote gestrichelte Linie), kann man erahnen, dass der Median die sieben Klassen so teilt, dass beidseitig gleich viele abgegebene Stimmen liegen:\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = median_vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#quantile",
    "href": "Skript_4.2.html#quantile",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Quantile",
    "text": "1.3 Quantile\nEin Quantil legt fest, wie viele Werte über bzw. unter einer bestimmten Grenze liegen und teilt den Datensatz damit in zwei Teile. Den bekanntesten Spezialfall haben wir mit dem Median bereits kennengelernt. Die Grenze lag in dem Fall genau in der Mitte, es liegen also 50% unterhalb der Grenze und 50% darüber. Bei einem 31%-Quantil würden hingegen 31% der Werte unter der Grenze liegen und 69% darüber. Wichtige Quantile sind die sogenannten Quartile, zu denen das 25%-Quantil, der Median und das 75%-Quantil zusammengefasst werden. Sie teilen die Gesamtmenge an Messwererten in vier gleich große Teile.\n\n1.3.1 Berechnung\nDas folgende R-Beispiel gibt die drei Quartile des Vertrauens-Datensatzes aus:\n\nquartile = quantile(allbus_df$VertrauenBR, probs = c(0.25, 0.5, 0.75))\nquartile\n\n25% 50% 75% \n  3   4   5 \n\n\n\n\n1.3.2 Visualisierung\nDas folgende Codebeispiel zeichnet neben dem Median (rot) auch das 25%-Quantil (blau) und das 75%-Quantil (grün) in das Säulendiagramm ein:\n\nquantile25 = quartile[\"25%\"]  #quantile(allbus_df$VertrauenBR, probs=c(0.25)) \nquantile75 = quartile[\"75%\"]\n\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = quantile25, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quantile75, color = \"green\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#der-mittelwert",
    "href": "Skript_4.2.html#der-mittelwert",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.4 Der Mittelwert",
    "text": "1.4 Der Mittelwert\nDer Begriff “Mittelwert” ist etwas ungenau, da es mehrere verschiedene Mittelwerte gibt. Oft ist damit das arithmetische Mittel gemeint. Es lässt sich nur bei mindestens kardinal skalierten Daten anwenden und bezieht die Gewichte der jeweiligen Merkmalsausprägungen mit ein.\n\n1.4.1 Berechnung\nDas arithmetische Mittel erhält man, indem man alle Messwerte addiert und durch die Gesamtzahl an Messwerten teilt:\n\\[\n\\bar{x} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} = \\frac{1}{n}  \\sum_{i=1}^{n} {x_{i}}\n\\] Schauen wir uns das am Beispiel des Netto-Einkommens der Befragten im Allbus-Datensatz an: Wir können dazu die bereits vorhandene Funktion mean nutzen:\n\nmean_einkommen = mean(allbus_df$Einkommen)\nmean_einkommen\n\n[1] 2445.346\n\n\n\n\n1.4.2 Visualisierung\nVergleichen wir das mit dem Median, fällt auf, dass zwischen beiden Lageparametern über 200 Euro Differenz bestehen:\n\nmedian_einkommen = median(allbus_df$Einkommen)\nmedian_einkommen\n\n[1] 2200\n\n\nDer Modus liegt noch weiter weg:\n\nmodus_einkommen = get_mode(allbus_df$Einkommen)\nmodus_einkommen\n\n[1] \"3000\"\n\n\nDas liegt daran, dass die Einkommensdaten kontinuierlich sind und es keinen homogenen An- und Abstieg der Häufigkeitsverteilung gibt. Der Einkommenswert, den am meisten Personen exakt gleich angegeben haben, ist deshalb wenig aussagekräftig und der Modus macht nur Sinn, nachdem man die Daten in Form von Einkommensklassen diskretisiert hat.\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =..density..),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Netto-Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean_einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus_einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1) \n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean_einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus_einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-3",
    "href": "Skript_4.2.html#berechnung-3",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "5 Berechnung",
    "text": "5 Berechnung\nDas arithmetische Mittel erhält man, indem man alle Messwerte addiert und durch die Gesamtzahl an Messwerten teilt:\n\\[\n\\bar{x} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} = \\frac{1}{n}  \\sum_{i=1}^{n} {x_{i}}\n\\] Schauen wir uns das am Beispiel des Netto-Einkommens der Befragten im Allbus-Datensatz an: Wir können dazu die bereits vorhandene Funktion mean nutzen:\n\nmean(allbus_messniveau_bsp$di01a)\n\n[1] 2445.346\n\n\nVergleichen wir das mit dem Median, fällt auf, dass zwischen beiden Lageparametern ca. 100 Euro Differenz bestehen:\n\nmedian(allbus_messniveau_bsp$di01a)\n\n[1] 2200\n\n\n\nplot(sort(allbus_messniveau_bsp$di01a))\nabline(h = median(allbus_messniveau_bsp$di01a), col = 'red')\nabline(h = mean(allbus_messniveau_bsp$di01a), col = 'orange')\n\n\n\n\n\nhist(allbus_messniveau_bsp$di01a, )\n\n\n\n\n\nggplot(allbus_messniveau_bsp, aes(x=di01a)) + geom_histogram(binwidth=100)\n\n\n\n\nNicht unerwähnt bleiben sollte das geometrische Mittel, das bei der Berechnung des Mittelwerts von prozentualen Veränderungen angewendet wird. Dabei werden die einzelnen Messwerte multipliziert und die n-te Wurzel aus dem Ergebnis gezogen, wobei n die Gesamtzahl an Messwerten ist:\n\\[\nx_{Geom} = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}\n\\]"
  },
  {
    "objectID": "Skript_4.3.html#die-spannweite",
    "href": "Skript_4.3.html#die-spannweite",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "1 Die Spannweite",
    "text": "1 Die Spannweite\nDie Spannweite (auch Range genannt) gibt den Abstand zwischen der kleinsten und der größten Merkmalsausprägung an. Sie ist das einfachste Streuungsmaß, zugleich aber nur wenig aussagekräftig.\n\n1.1 Berechnung\n\\[\nSP = x_{max} - x_{min}  \n\\] Mit der range-Funktion kann man sich in R die beiden Extremwerte als Vektor ausgeben lassen:\n\nrange(allbus_df$Einkommen)\n\n[1]   -41 14100\n\n\nAlternativ kann man auch einfach Das Maximum und das Minimum ermitteln und die Differenz berechnen:\n\neinkommen_max = max(allbus_df$Einkommen) - min(allbus_df$Einkommen)\neinkommen_max\n\n[1] 14141"
  },
  {
    "objectID": "Skript_4.3.html#quantilsabstände",
    "href": "Skript_4.3.html#quantilsabstände",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "2 Quantilsabstände",
    "text": "2 Quantilsabstände\nDieses Maß gibt die Differenz zweier Quantile an. Insbesondere der Quartilsabstand (75%-Quantil - 25%-Quantil) ist hier von Bedeutung.\n\n2.1 Berechnung\nDer Quaantilsabstand berechnet sich für das obere Quantil \\(Q_{o}\\) und das untere Quantil \\(Q_{u}\\) folgendermaßen:\n\\[\nQA = Q_{o} - Q_{u}\n\\]\nFür den Quartilsabstand gibt es eine Funktion in R:\n\n# Berechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range):\nIQR(allbus_df$VertrauenBR)\n\n[1] 2\n\n\n\nBerechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range)\n\nUm den mittleren Quartilsabstand zu ermitteln, kann man das Ergebnis noch durch 2 teilen."
  },
  {
    "objectID": "Skript_4.3.html#varianz-und-standardabweichung",
    "href": "Skript_4.3.html#varianz-und-standardabweichung",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "3 Varianz und Standardabweichung",
    "text": "3 Varianz und Standardabweichung\nEin nützlicheres Maß dafür, wie die einzelnen Merkmalsausprägungen um den Mittelwert verteilt sind - vorausgesetzt, man hat es mit kardinal skalierten Daten zu tun, kann die Varianz sein.\n\n3.1 Berechnung\nEs wird unterschieden zwischen der Varianz der Grundgesamtheit und der Stichprobenvarianz. Bei ersterer berechnet man für jede einzelne Merkmalsausprägung ihre Abweichung vom Mittelwert, quadriert die Ergebnisse und summiert diese. Anschließend teilt man durch die Größe der Grundgesamtheit:\n\\[\nVar = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\] Da man wie gesagt meist nur auf eine Stichprobe zurückgreifen kann, kann man den Mittelwert der Grundgesamtheit durch den Mittelwert der Stichprobe nur schätzen. Um die damit verbundene Verzerrung auszugleichen, kann es sinnvoll sein die Summe der quadrierten Abweichungen nicht durch \\(n\\), sondern durch \\((n-1)\\) zu teilen. So ergibt sich folgende Formel, wobei \\(n\\) hier die Stichprobengröße darstellt:\n\\[\nVar = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nIn R bekommen wir die Varianz mit der var-Funktion, hier am Beispiel des Netto-Einkommens:\n\nvar(allbus_df$Einkommen)\n\n[1] 2593715\n\n\nWie wir sehen, ist das Ergebnis sehr groß und auf den ersten Blick nicht leicht interpretierbar. Nützlicher ist da die Standardabweichung, die wir einfach dadurch erhalten, dass wir die Quadratwurzel der Varianz ziehen:\n\\[\nSD = \\sqrt{Var} = s =  \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] R bietet zur Berechnung der Standardabweichung die sd-Funktion an.\n\nsd(allbus_df$Einkommen)\n\n[1] 1610.501"
  },
  {
    "objectID": "Skript_4.3.html#zusammenfassungen-ausgeben-lassen",
    "href": "Skript_4.3.html#zusammenfassungen-ausgeben-lassen",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "4 Zusammenfassungen ausgeben lassen",
    "text": "4 Zusammenfassungen ausgeben lassen\nWenn man nicht jeden Parameter einzeln abfragen will und kardinal skalierte Daten hat, kann man sich die wichtigsten Lageparameter auch als Zusammenfassung ausgeben lassen:\n\nsummary(allbus_df$Einkommen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    -41    1500    2200    2445    3000   14100"
  },
  {
    "objectID": "Skript_4.4.html#die-normalverteilung",
    "href": "Skript_4.4.html#die-normalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "0.1 Die Normalverteilung",
    "text": "0.1 Die Normalverteilung\nDiese Verteilung ist auch ihrer charakteristischen Form wegen als Glockenkurve oder nach ihrem maßgeblichen Entdecker Carl Friedrich Gauß als Gauß-Verteilung bekannt. Relativ viele andere Verteilungen lassen sich bei ausreichend großer Stichprobengröße mit der Normalverteilung approximieren. Sie ist symmetrisch und der Mittelwert ist ihr Maximum.\nCa. 68% der Werte liegen innerhalb einer Standardabweichung vom Mittelwert entfernt, ca. 95% innerhalb von zwei Standardabweichungen und ca. 99,7%, also fast alle, innerhalb von drei Standardabweichungen.\n\n0.1.1 Berechnung\nDie Dichtefunktion der Normalverteilung wird folgendermaßen berechnet:\n\\[\nf(x,\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\] Dabei ist \\(\\mu\\) der Mittelwert (arithm. Mittel, Median, oder Modus) und \\(\\sigma\\) die Standardabweichung.\nBei einer Normalverteilung mit einem Mittelwert von null und einer Varianz von eins spricht man von einer Standardnormalverteilung:\n\\[\n\\phi(z) = f(z, 0, 1) = \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{- \\frac{z^2}{2}}\n\\]\nNormal- aber nicht standardnormal verteilte Werte lassen sich leicht standardisieren, indem man den Mittelwert von ihnen abzieht und das Ergebnis durch die Standardabweichung teilt:\n\\[\nz = \\frac{Messwert - Mittelwert}{Standardabweichung} = \\frac{x - \\mu}{\\sigma}\n\\] \\(\\mu\\) kann dabei das arithmetische Mittel, der Median oder der Modus sein.\nDer Vollständigkeit halber sei hier auch noch die Formel der Verteilungsfunktion aufgeschrieben:\n\\[\nF(x, \\mu, \\sigma) = \\frac{1}{2} \\left[1 + \\text{erf} \\left (\\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right]\n\\] \\(erf\\) steht für die Gauß’sche Fehlerfunktion.\nDa R umfangreiche Funktionalitäten bereitstellt, um diese Verteilungen zu berechnen, ist es an dieser Stelle nicht notwendig, sich diese Formel einzuprägen oder irgendwas damit per Hand zu rechnen. R hat eine ganze Reihe an Verteilungen implementiert und stellt zu jeder davon u.a. vier Funktionen bereit:\n\ndie Wahrscheinlichkeits-/Dichtefunktion, beginnend mit dem Buchstaben d,\ndie Verteilungsfunktion, beginnend mit p,\nQuantile, beginnend mit q sowie\nZufallszahlen auf Basis der jeweiligen Verteilung, beginnend mit r."
  },
  {
    "objectID": "Skript_4.4.html#binomialverteilung",
    "href": "Skript_4.4.html#binomialverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Binomialverteilung",
    "text": "1.1 Binomialverteilung\nDiese Verteilung basiert auf Zufallsexperimenten, die genau zwei Versuchsausgänge aufweisen, welche sich gegenseitig ausschließen und konstante Wahrscheinlichkeiten für die beiden Ausgänge haben. Die einzelnen Versuche sollen voneinander unabhängig sein.\nEin bekanntes Beispiel für so ein Zufallsexperiment ist der Münzwurf. Anwendungen in der KMW wären etwa Kaufentscheidungen (Ja/Nein) in der Werbewirkungsforschung oder allgemein Interview-Fragen, auf die es nur Ja/Nein-Antworten gibt.\n\n1.1.1 Berechnung\nBei einer Eintrittswahrscheinlichkeit \\(p\\) für ein bestimmtes Ereignis berechnet sich die Wahrscheinlichkeit, dass das Ereignis nach \\(n\\) Wiederholungen \\(k\\) mal eintritt, mit folgender Formel:\n\\[\nf(k;n,p) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nDas folgende Beispiel zeigt, wie man in R die Wahrscheinlichkeit berechnet, bei zehn Münzwürfen genau 7 mal Kopf zu werfen:\n\nn &lt;- 10 # Festlegen der Gesamtzahl an Würfen \nkopf &lt;- 7 # Festlegen der Anzahl, wie oft Kopf geworfen werden soll \np_kopf &lt;- 0.5 # Festlegen der Wahrscheinlichkeit, Kopf zu werfen (hier: 50%)\nprob &lt;- dbinom(kopf, size=n, prob=p_kopf) \nprob\n\n[1] 0.1171875"
  },
  {
    "objectID": "Skript_4.4.html#hypergeometrische-verteilung",
    "href": "Skript_4.4.html#hypergeometrische-verteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.2 Hypergeometrische Verteilung",
    "text": "1.2 Hypergeometrische Verteilung\nDiese Verteilung kann vorliegen, wenn die Bedingung der Unabhängigkeit der einzelnen Versuche nicht einhaltbar ist.\n\n1.2.1 Berechnung\nEs wird wieder von zwei Ereignissen ausgegangen, die eintreten können. Angenommen, eine Grundgesamtheit setzt sich zusammen aus \\(N + M\\) Ereignissen, wobei \\(M\\) und \\(N\\) sich gegenseitig ausschließen. Aus dieser Grundgesamtheit wird nun eine Stichprobe der Größe \\(k\\) gezogen. Dann berechnet sich die Wahrscheinlichkeit, dass sich in der Stichprobe \\(x\\) mal das Ereignis \\(M\\) eintritt, folgendermaßen: \\[\nf(x, k, M, N) = \\frac{\\binom{M}{x} \\binom{N}{k-x}}{\\binom{N + M}{k}}\n\\]\nMachen wir uns das an einem Beispiel deutlich: In einer Stadt mit ca. 600000 Einwohnern sind ca. 2000 Menschen mit HIV infiziert. Wie groß ist die Wahrscheinlichkeit, dass von 50 Leuten, die man bei einem Ausflug in die Stadt zufällig trifft, kein einziger HIV hat?\n\nn &lt;- 600000 # Grundgesamtheit\nm &lt;- 2000   # Fälle, die eine bestimmte Merkmalsausprägung aufweisen\nk &lt;- 50     # Stichprobengröße\nx &lt;- 0      \nprob &lt;- dhyper(x, m, n, k) \nprob\n\n[1] 0.8467106\n\n\nDas heißt, die Wahrscheinlichkeit, dass mindestens eine Person, der man begegnet, HIV hat, beträgt ca. \\(1 - 0.8467\\), also ca. 15,33 Prozent."
  },
  {
    "objectID": "Skript_4.4.html#poissonverteilung",
    "href": "Skript_4.4.html#poissonverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Poissonverteilung",
    "text": "1.3 Poissonverteilung\nDiese Verteilung kann herangezogen werden, wenn es um das durchschnittliche Eintreten bestimmter Ereignisse innerhalb es festen Zeitintervalls geht. Dabei steht \\(\\lambda\\) für die Rate, mit der ein Ereignis durchschnittlich eintritt.\n\n1.3.1 Berechnung\nDie Poissonverteilung berechnet sich nach folgender Formel: \\[\nf(x;\\lambda) =  \\frac{\\lambda^x e^{-\\lambda} }{x!}\n\\]\nDazu wieder ein Beispiel: Eine Nachrichtenagentur veröffentlicht zu einem bestimmten Thema normalerweise 3 Artikel pro Tag. Wie wahrscheinlich ist es, dass sie an einem Tag 7 Artikel zu demselben Thema veröffentlicht?\n\nx &lt;- 7 \nlambda &lt;- 3 \ndpois(x, lambda)\n\n[1] 0.02160403"
  },
  {
    "objectID": "Skript_4.4.html#gleichverteilung",
    "href": "Skript_4.4.html#gleichverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.4 Gleichverteilung",
    "text": "1.4 Gleichverteilung\nEine Zufallsvariable ist in einem gegebenen Intervall gleichmäßig verteilt. Diese Verteilung ist besonders zur Erzeugung von Zufallszahlen hilfreich.\n\n1.4.1 Berechnung\n\\[\nf(x;min,max) = \\begin{cases}\n\\frac{1}{max-min}, & min \\leq x \\leq max \\\\\n0, & \\text{sonst}\n\\end{cases}\n\\]\n\n# Fünf Zufallszahlen, die im voreingestellten Intervall von 0 bis 1 liegen:\nrunif(5) \n\n[1] 0.9093424 0.1476882 0.2362888 0.9087583 0.5536340\n\n# Drei Zufallszahlen, die im Intervall von -10 bis 10 liegen:\nrunif(3, min=-10, max=10) \n\n[1]  8.0444784 -5.8739108 -0.8097793\n\n\n\n\n1.4.2 Exponentialverteilung\nÄhnlich wie die Poissonverteilung, allerdings stetig.\n\n\n1.4.3 Berechnung\n\\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Beispiel:\nEine Nachrichtenagentur berichtet durchschnittlich alle 30 Tage über Proteste und Widerstandsaktionen in einem Land X. Wie wahrscheinlich ist es, dass zwischen den Nachrichten plötzlich nur noch maximal 5 Tage liegen?\n\nda &lt;- 30 # Durchschnittlicher Abstand in Tagen\nlambda &lt;- 1 / da\nx &lt;- 5 \nprob &lt;- pexp(x, rate=lambda)\nprob\n\n[1] 0.1535183"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-der-streuungsparameter",
    "href": "Skript_4.3.html#visualisierung-der-streuungsparameter",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "5 Visualisierung der Streuungsparameter",
    "text": "5 Visualisierung der Streuungsparameter\nIn folgendem Codebeispiel lassen wir uns das Histogramm aus dem letzten Abschnitt ausgeben, allerdings reduziert auf Median (blau, durchgezogene Linie) und das arithmetische Mittel (rot, durchgezogene Linie). Wir ergänzen diese Lageparameter um den Minimal- und den Maximalwert (schwarz, gestrichelt), das untere und obere Quartil (blau, gestrichelt) und die Standardabweichung, hier beiderseits vom Mittelwert aufgetragen(rot, gestrichelt).\n\nmean_einkommen = mean(allbus_df$Einkommen)\nmedian_einkommen = median(allbus_df$Einkommen)\nrange_einkommen = range(allbus_df$Einkommen) \niqr_einkommen = IQR(allbus_df$Einkommen) \nsd_einkommen = sd(allbus_df$Einkommen) \n\nquartil25_einkommen = quantile(allbus_df$Einkommen, 0.25) \nquartil75_einkommen = quantile(allbus_df$Einkommen, 0.75)\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean_einkommen, color = \"red\", linewidth = 1) +   #, linetype = \"dashed\"\n  geom_vline(xintercept = mean_einkommen - sd_einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = mean_einkommen + sd_einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_einkommen, color = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = quartil25_einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quartil75_einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = range_einkommen[1], color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = range_einkommen[2], color = \"black\", linetype = \"dashed\", linewidth = 1) \n\n\n\n\nEs ist sehr deutlich zu erkennen, dass die Spannweite (die Distanz zwischen den beiden schwarzen gestrichelten Linien) wenig hilfreich ist, wenn man erfahren will, wo besonders viele Messwerte liegen. Der Interquartilsabstand scheint hier sehr viel aussagekräftiger zu sein, während die Standardabweichung unterhalb des Mittelwerts mehr Messwerte einzuschließen scheint als oberhalb davon. Dieses Ungleichgewicht wird im Abschnitt über Verteilungen noch eine Rolle spielen."
  },
  {
    "objectID": "Skript_4.2.html#geometrisches-mittel",
    "href": "Skript_4.2.html#geometrisches-mittel",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.5 Geometrisches Mittel",
    "text": "1.5 Geometrisches Mittel\nNicht unerwähnt bleiben sollte das geometrische Mittel, das bei der Berechnung des Mittelwerts von prozentualen Veränderungen angewendet wird. Dabei werden die einzelnen Messwerte multipliziert und die n-te Wurzel aus dem Ergebnis gezogen, wobei n die Gesamtzahl an Messwerten ist:\n\\[\nx_{Geom} = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}\n\\] ### Berechnung\nIn R gibt es dafür keine eigenständige Funktion, man kann aber die Gleichung umstellen und mit Hilfe einiger anderer eingebauter Funktionen eine simple Alternative erstellen, indem man einen kleinen Trick mit der Exponentialfunktion und dem natürlichen Logarithmus anwendet:\n$$\n\\[\\begin{aligned}\nx_{Geom}  & = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}} \\\\\n          & = e^{ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}} \\\\\n          & = e^{\\frac{1}{n}ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})} \\\\\n          & = e^{\\frac{1}{n} \\sum_{i=1}^{n} ln({x_{i})}  }\n\\end{aligned}\\]\n$$\nAuch, wenn die resultierende Formel wenig ansprechend aussieht, kann man bei genauerem Hinsehen das versteckte arithmetische Mittel erkennen und den ganzen Ausdruck in folgenden R-Code umsetzen:\n\ngeom_mean = function(vector){\n  exp(mean(log(vector)))\n}\n\n\nmin(allbus_df$Einkommen)\n\n[1] -41\n\ngeom_einkommen &lt;- geom_mean(allbus_df$Einkommen + 42) \ngeom_einkommen\n\n[1] 1996.667\n\n\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean_einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median_einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus_einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = geom_einkommen, color = \"yellow\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.4.html#lognormalverteilung",
    "href": "Skript_4.4.html#lognormalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "2.1 Lognormalverteilung",
    "text": "2.1 Lognormalverteilung\nWir haben diese Verteilung bereits angesprochen. Für Daten, die einen natürlichen Nullpunkt haben, ist die Lognormalverteilung ein möglicher Kandidat, weil sie nicht symmetrisch ist wie die Normalverteilung. Zur Normalisierung ist sie sehr nützlich. Allerdings sollte man daran denken, dass sie nur mit Werten funktioniert, die größer als Null sind. Man muss also ggf. die Messwerte vorher anpassen.\n\n2.1.1 Berechnung\nDie Formel ähnelt der für die Normalverteilung:\n\\[\nf(x;\\mu,\\sigma) = \\frac{1}{x \\cdot \\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln(x)-\\mu)^2}{2\\sigma^2}}\n\\]\n\n\n2.1.2 Visualisierung\nSchauen wir uns einmal an, wie unsere Einkommensdaten zur Dichtefunktion der Lognormalverteilung passen. Dazu addieren wir zunächst zu jedem Messwert die Zahl 42, damit alle Werte größer Null sind, logarithmieren dann die Messwerte und ermitteln anschließend das arithmetische Mittel und die Standardabweichung:\n\nlog.allbus.df = data.frame(LogEinkommen=log(allbus_df$Einkommen + 42)) \nlog.mean = mean(log.allbus.df$LogEinkommen) \nlog.sd = sd(log.allbus.df$LogEinkommen)\n\ncat(\"Mittelwert: \", log.mean, \"Standardabweichung: \", log.sd)\n\nMittelwert:  7.599235 Standardabweichung:  0.8405187\n\n\nDann plotten wir das Ergebnis in einem Histogramm, in das wir die theoretische Dichte der Lognormalverteilung eintragen, die unseren Parametern entspricht:\n\nggplot(data = allbus_df, aes(x=Einkommen + 42)) +\n  geom_histogram(aes(y=..density..), binwidth = 500, fill = \"gray\", color = \"black\") +\n  labs(\n    title = \"Nettoeinkommen und Lognormalverteilung\", x = \"Messwerte\", y = \"Dichte\") +\nstat_function(fun=dlnorm, args=list(meanlog=log.mean, sdlog=log.sd), colour=\"red\")  #meanlog=log.mean, sdlog=log.sd\n\n\n\n\nDie theoretische Verteilung wirkt schiefer als die empirische, aber insgesamt scheint die Übereinstimmung auch visuell größer zu sein als mit der Normalverteilung."
  },
  {
    "objectID": "Skript_4.4.html#weibull-verteilung",
    "href": "Skript_4.4.html#weibull-verteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "2.2 Weibull-Verteilung",
    "text": "2.2 Weibull-Verteilung\nDer zweite Kandidat, der sich bei stetigen, asymmetrischen Verteilungen anbieten kann, ist die Weibull-Verteilung. Sie ist sehr vielseitig, bezogen auf die Formen, die sie annehmen kann und wird deshalb oft für die Kalkulation der Lebensdauer von Maschinenteilen u.ä. verwendet. Allerdings kann sie auch bei der Untersuchung von Einkommensungleichheiten von Bedeutung sein.\n\n2.2.1 Berechnung\nDiese Verteilung ist abhängig von einem Streuungsparameter \\(1 / \\lambda\\) und dem Formparameter \\(k\\):\n$$ f(x;,k) = ()^{k-1} e{-(x/)k}\n$$ Durch den Parameter \\(k\\) kann man berücksichtigen, wie sich die Häufigkeit, mit der ein bestimmtes Ereignis auftritt, verändert. Wenn \\(k = 1\\), geht man davon aus, dass sich die Wahrscheinlichkeit, dass ein Ereignis eintritt, kaum verändert. Für \\(k &lt; 1\\) erwartet man, dass Ereignisse über die Zeit seltener auftreten und für \\(k &gt; 1\\), dass sie mit der Zeit zunehmen.\n\n\n2.2.2 Visualisierung\nAuf den Netto-Einkommens-Datensatz angewendet, ergibt sich ein Bild, das wie erwartet ähnlich wie die Lognormal-Verteilung wirkt, wenn auch immer noch mit deutlichen Abweichungen.\n\nfit.weibull = fitdist(allbus_df$Einkommen + 42, \"weibull\")\nggplot(data = allbus_df, aes(x=Einkommen + 42)) +\n  geom_histogram(aes(y=..density..), binwidth = 500, fill = \"grey\", color = \"black\") +\n  labs(\n    title = \"Netto-Einkommen und Weibull-Verteilung\",\n    x = \"Messwerte\",\n    y = \"Dichte\"\n  ) +\n  stat_function(fun=dweibull, args=list(scale=fit.weibull$estimate[\"scale\"], shape=fit.weibull$estimate[\"shape\"]), colour=\"red\")"
  },
  {
    "objectID": "Skript_4.4.html#weitere-verteilungen",
    "href": "Skript_4.4.html#weitere-verteilungen",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "2.3 Weitere Verteilungen",
    "text": "2.3 Weitere Verteilungen\nDamit verlassen wir das konkrete Einkommens-Beispiel und die Suche nach Alternativen zur Normalverteilung und schauen uns ein paar weitere Verteilungen an, die je nach Forschungsdesign ebenfalls wichtig sein können.\n\n2.3.1 Binomialverteilung\nDiese Verteilung basiert auf Zufallsexperimenten, die genau zwei Versuchsausgänge aufweisen, welche sich gegenseitig ausschließen und konstante Wahrscheinlichkeiten für die beiden Ausgänge haben. Die einzelnen Versuche sollen voneinander unabhängig sein.\nEin bekanntes Beispiel für so ein Zufallsexperiment ist der Münzwurf. Anwendungen in der KMW wären etwa Kaufentscheidungen (Ja/Nein) in der Werbewirkungsforschung oder allgemein Interview-Fragen, auf die es nur Ja/Nein-Antworten gibt.\n\n2.3.1.1 Berechnung und Visualisierung\nBei einer Eintrittswahrscheinlichkeit \\(p\\) für ein bestimmtes Ereignis berechnet sich die Wahrscheinlichkeit, dass das Ereignis nach \\(n\\) Wiederholungen \\(k\\) mal eintritt, mit folgender Formel:\n\\[\nf(k;n,p) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nDas folgende Beispiel zeigt, wie man in R die Wahrscheinlichkeit berechnet, bei zehn Münzwürfen genau 7 mal Kopf zu werfen:\n\nn = 10 # Festlegen der Gesamtzahl an Würfen \nkopf = 7 # Festlegen der Anzahl, wie oft Kopf geworfen werden soll \np_kopf = 0.5 # Festlegen der Wahrscheinlichkeit, Kopf zu werfen (hier: 50%)\nprob = dbinom(kopf, size=n, prob=p_kopf) \nprob\n\n[1] 0.1171875\n\n\n\n\n\n2.3.2 Hypergeometrische Verteilung\nDiese Verteilung kann vorliegen, wenn die Bedingung der Unabhängigkeit der einzelnen Versuche nicht einhaltbar ist.\n\n2.3.2.1 Berechnung\nEs wird wieder von zwei Ereignissen ausgegangen, die eintreten können. Angenommen, eine Grundgesamtheit setzt sich zusammen aus \\(N + M\\) Ereignissen, wobei \\(M\\) und \\(N\\) sich gegenseitig ausschließen. Aus dieser Grundgesamtheit wird nun eine Stichprobe der Größe \\(k\\) gezogen. Dann berechnet sich die Wahrscheinlichkeit, dass sich in der Stichprobe \\(x\\) mal das Ereignis \\(M\\) eintritt, folgendermaßen:\n\\[\nf(x, k, M, N) = \\frac{\\binom{M}{x} \\binom{N}{k-x}}{\\binom{N + M}{k}}\n\\]\nMachen wir uns das an einem Beispiel deutlich: In einer Stadt mit ca. 600000 Einwohnern sind ca. 2000 Menschen mit HIV infiziert. Wie groß ist die Wahrscheinlichkeit, dass von 50 Leuten, die man bei einem Ausflug in die Stadt zufällig trifft, kein einziger HIV hat?\n\nn = 600000 # Grundgesamtheit\nm = 2000   # Fälle, die eine bestimmte Merkmalsausprägung aufweisen\nk = 50     # Stichprobengröße\nx = 0      \nprob = dhyper(x, m, n, k) \nprob\n\n[1] 0.8467106\n\n\nDas heißt, die Wahrscheinlichkeit, dass mindestens eine Person, der man begegnet, HIV hat, beträgt ca. \\(1 - 0.8467\\), also ca. 15,33 Prozent.\n\n\n\n2.3.3 Poissonverteilung\nDiese Verteilung kann herangezogen werden, wenn es um das durchschnittliche Eintreten bestimmter Ereignisse innerhalb es festen Zeitintervalls geht. Dabei steht \\(\\lambda\\) für die Rate, mit der ein Ereignis durchschnittlich eintritt.\n\n2.3.3.1 Berechnung\nDie Poissonverteilung berechnet sich nach folgender Formel:\n\\[\nf(k;\\lambda) =  \\frac{\\lambda^k  }{k!}e^{-\\lambda}\n\\]\nDazu wieder ein Beispiel: Eine Nachrichtenagentur veröffentlicht zu einem bestimmten Thema normalerweise 3 Artikel pro Tag. Wie wahrscheinlich ist es, dass sie an einem Tag 7 Artikel zu demselben Thema veröffentlicht?\n\nx = 7 \nlambda = 3 \ndpois(x, lambda)\n\n[1] 0.02160403\n\n\n\n\n\n2.3.4 Gleichverteilung\nEine Zufallsvariable ist in einem gegebenen Intervall gleichmäßig verteilt. Diese Verteilung ist besonders zur Erzeugung von Zufallszahlen hilfreich.\n\n2.3.4.1 Berechnung\n\\[\nf(x;min,max) = \\begin{cases}\n\\frac{1}{max-min}, & min \\leq x \\leq max \\\\\n0, & \\text{sonst}\n\\end{cases}\n\\]\n\n# Fünf Zufallszahlen, die im voreingestellten Intervall von 0 bis 1 liegen:\nrunif(5) \n\n[1] 0.7934604 0.6085894 0.3025285 0.5038265 0.3501810\n\n# Drei Zufallszahlen, die im Intervall von -10 bis 10 liegen:\nrunif(3, min=-10, max=10) \n\n[1] -2.7544372 -1.2323549 -0.6117829\n\n\n\n\n\n2.3.5 Exponentialverteilung\nÄhnlich wie die Poissonverteilung, allerdings stetig. Sie ist ein Spezialfall der Weibull-Verteilung und wird z.B. bei der Untersuchung von Zeitabständen eingesetzt.\n\n2.3.5.1 Berechnung\nIm Gegensatz zur etwas vielseitigeren Weibull-Verteilung hat sie einen Parameter weniger, da \\(k\\) unveränderlich gleich 1 ist, geht also von weitgehend gleichmäßig auftretenden Ereignissen aus.\n\\[\nf(k;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda k}, & k \\geq 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\]\nBeispiel: Eine Nachrichtenagentur berichtet durchschnittlich alle 30 Tage über Proteste und Widerstandsaktionen in einem Land X. Wie wahrscheinlich ist es, dass zwischen den Nachrichten plötzlich nur noch maximal 5 Tage liegen?\n\nda = 30 # Durchschnittlicher Abstand in Tagen\nlambda = 1 / da\nx = 5 \nprob = pexp(x, rate=lambda)\nprob\n\n[1] 0.1535183"
  },
  {
    "objectID": "Autoren.html#hannah-marie-büttner",
    "href": "Autoren.html#hannah-marie-büttner",
    "title": "Das Team stellt sich vor",
    "section": "Hannah-Marie Büttner",
    "text": "Hannah-Marie Büttner\nVita\n\n\n\n\n\nHannah-Marie Büttner\n\n\nHannah-Marie Büttner ist als wissenschaftliche Mitarbeiterin der Universität Bremen am Zentrum für Medien-, Kommunikations- und Informationsforschung sowie am Institut für Informationsmanagement tätig. Sie hat einen B.Sc. Sozialwissenschaften an der Universität Trier und einen Research Master Social Sciences an der University of Amsterdam absolviert.Ihre Forschungsinteressen und methodischen Schwerpunkte liegen im Feld der Computational Social Science. Gegenwärtig forscht sie zu Protestbewegungen und deren Social Media Aktivismus, insbesondere auf der Plattform Telegram.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nSoziale Bewegungen\nSoziale Netzwerkanalyse\nComputerbasierte Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ifib, Institut für Informationsmanagement; Am Fallturm 1; TAB; 28359 Bremen\n🎓 Webseite\n✉️ E-Mail"
  },
  {
    "objectID": "Skript_3.4.html#einfache-häufigkeitstabelle",
    "href": "Skript_3.4.html#einfache-häufigkeitstabelle",
    "title": "Tabellen und Grafiken in R",
    "section": "2.1 Einfache Häufigkeitstabelle",
    "text": "2.1 Einfache Häufigkeitstabelle\nWir berechnen eine einfache Häufigkeitstabelle für die Variable geschlecht. Dies geschieht mit den Funktionen group_by und summarise. Die Funktion n übernimmt das eigentliche Auszählen der Häufigkeit einer Variablenausprägung.\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n())\ngeschlechterverteilung\n\n# A tibble: 2 × 2\n  geschlecht anzahl\n  &lt;fct&gt;       &lt;int&gt;\n1 MANN           15\n2 FRAU            5\n\n\nIm nächsten Schritt fügen wir den absoluten Zahlen relative Anteile hinzu, um das Geschlechterverhältnis besser zu verstehen.\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl))\ngeschlechterverteilung\n\n# A tibble: 2 × 3\n  geschlecht anzahl anteil\n  &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;\n1 MANN           15   0.75\n2 FRAU            5   0.25\n\n\nNun fügen wir den Anteilen von 1 noch Prozentanteile hinzu, indem with mit 100 multiplizieren und runden (was in erster Linie kosmetischer Natur ist).\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\ngeschlechterverteilung\n\n# A tibble: 2 × 4\n  geschlecht anzahl anteil prozent\n  &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 MANN           15   0.75      75\n2 FRAU            5   0.25      25"
  },
  {
    "objectID": "Skript_3.4.html#tabellen-speichern",
    "href": "Skript_3.4.html#tabellen-speichern",
    "title": "Tabellen und Grafiken in R",
    "section": "2.2 Tabellen speichern",
    "text": "2.2 Tabellen speichern\nJetzt schreiben wir die Daten in eine CSV-Datei, die wir bspw. später mit Excel oder einer anderen Tabellenkalkulation öffnen können.\n\nwrite_excel_csv2(geschlechterverteilung, file = \"geschlechterverteilung.csv\")"
  },
  {
    "objectID": "Skript_3.4.html#kreuztabellen",
    "href": "Skript_3.4.html#kreuztabellen",
    "title": "Tabellen und Grafiken in R",
    "section": "2.3 Kreuztabellen",
    "text": "2.3 Kreuztabellen\nWas, wenn wir zwei Variablen mit Blick auf ihre Werte in Beziehung setzen wollen? Das bezeichnet man als Kreuz- oder Kontigenztabelle. Man erzeugt eine Kreuztabelle mit dplyr, indem man nach mehreren Variablen gruppiert.\n\nbildung_und_geschlecht &lt;- sample_klein %&gt;% \n  group_by(bildung, geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\nbildung_und_geschlecht\n\n# A tibble: 9 × 5\n# Groups:   bildung [6]\n  bildung            geschlecht anzahl anteil prozent\n  &lt;fct&gt;              &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 VOLKS-,HAUPTSCHULE MANN            1  0.5        50\n2 VOLKS-,HAUPTSCHULE FRAU            1  0.5        50\n3 MITTLERE REIFE     MANN            4  0.571      57\n4 MITTLERE REIFE     FRAU            3  0.429      43\n5 FACHHOCHSCHULREIFE MANN            3  1         100\n6 HOCHSCHULREIFE     MANN            5  0.833      83\n7 HOCHSCHULREIFE     FRAU            1  0.167      17\n8 NOCH SCHUELER      MANN            1  1         100\n9 &lt;NA&gt;               MANN            1  1         100\n\n\nMöglicherweise möchte ich den Anteil anders berechnen und nicht die relative Geschlechterverteilung innerhalb eines Bildungsabschlusses in den Blick nehmen, sondern etwa die Verteilung der Bildungsabschlüsse jeweils für männliche und weibliche Studienteilnehmer anschauen. Dies lässt sich mit einer Neugruppierung der Daten durch group_by() erreichen.\n\ngeschlecht_und_bildung &lt;- sample_klein %&gt;% \n  group_by(geschlecht, bildung) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\ngeschlecht_und_bildung\n\n# A tibble: 9 × 5\n# Groups:   geschlecht [2]\n  geschlecht bildung            anzahl anteil prozent\n  &lt;fct&gt;      &lt;fct&gt;               &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 MANN       VOLKS-,HAUPTSCHULE      1 0.0667       7\n2 MANN       MITTLERE REIFE          4 0.267       27\n3 MANN       FACHHOCHSCHULREIFE      3 0.2         20\n4 MANN       HOCHSCHULREIFE          5 0.333       33\n5 MANN       NOCH SCHUELER           1 0.0667       7\n6 MANN       &lt;NA&gt;                    1 0.0667       7\n7 FRAU       VOLKS-,HAUPTSCHULE      1 0.2         20\n8 FRAU       MITTLERE REIFE          3 0.6         60\n9 FRAU       HOCHSCHULREIFE          1 0.2         20\n\n\nLag also im ersten Anlauf das Geschlecht der Gruppierung zugrunde, ist es in der zweiten Version der Bildungsabschluss."
  },
  {
    "objectID": "Skript_3.4.html#balkendiagramme",
    "href": "Skript_3.4.html#balkendiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.1 Balkendiagramme",
    "text": "3.1 Balkendiagramme\nDas vermutlich einfachste Diagramm, das man mit dem ggplot2-Paket erstellen kann, ist ein Balken- oder Säulendiagramm (barplot). Es zeigt die Häufigkeitsverteilung einer diskreten Variablen, indem es Säulen auf der x-Achse darstellt. Balkendiagramme sind besonders geeignet, um wenige Ausprägungen (bis ca. 15) zu veranschaulichen. Wenn es mehr Kategorien gibt, wird die Anschaulichkeit beeinträchtigt, und es empfiehlt sich, auf Liniendiagramme auszuweichen.\nDas nachstehende Beispiel zeigt die Häufigkeitsverteilung der Variable geschlecht im Datensatz (bzw. dem kleinen Sample).\n\nggplot(sample_klein, aes(geschlecht)) + \n  geom_bar()\n\n\n\n\nIn einem nächsten Schritt fügen wir eine Überschrift hinzu und formatieren die Balken und Achsen so, dass das Plot lesbarer ist.\n\nggplot(sample_klein, aes(bildung)) + \n  geom_bar() + \n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nFarben lassen sich in ggplot vielseitig einsetzen, um Kategorienunterschiede anzuzeigen. Dies geschieht mit den Argumenten ‘fill’ bzw. ‘color’.\n\nggplot(sample_klein, aes(bildung, fill = bildung)) + \n  geom_bar() +\n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nEs existieren zahlreiche Farbpaletten für ggplot, um unterschiedliche Arten von Beziehungen darzustellen. Das nachstehende Palette unterscheidet verschiedenen Kategorien. Für gradierte Variablen (‘viel’ - ‘wenig’) sind andere Paletten z.T. besser geeignet. Eine gute Auswahl an Palette enthält u.a. das Paket RColorBrewer.\n\nggplot(sample_klein, aes(bildung, fill = bildung)) + \n  geom_bar() + coord_flip() + \n  scale_fill_brewer(palette = \"Set1\") +  \n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nSchließlich lassen sich auch die Beschriftung und weitere Aspekte eines Plots anpassen (Achsenorientierung, Legende etc).\n\nggplot(sample_klein, aes(as_factor(fernsehkonsum))) + \n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  ggtitle(\"Fernsehkonsum pro Woche in Tagen\") + \n  xlab(\"Fernsehkonsum pro Woche in Tagen\") + ylab(\"Anzahl der Respondenten\")"
  },
  {
    "objectID": "Skript_3.4.html#histogramme",
    "href": "Skript_3.4.html#histogramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.2 Histogramme",
    "text": "3.2 Histogramme\nFür ein besseres Verständnis einer Verteilung sind oftmals die Häufigkeitsausprägungen einer kontinuierlichen Variable in gleich großen Gruppen interessant (sog. “bins”). Dabei hilft der Visualisierungstyp Histogramm. Ein Histogramm ist eine grafische Darstellung der Häufigkeitsverteilung kardinal skalierter Merkmale. Dabei werden Daten in Klassen unterteilt, die unterschiedliche Breiten haben können. Rechtecke mit der Breite der Klassen werden nebeneinander gezeichnet, wobei ihre Flächen die (relativen oder absoluten) Klassenhäufigkeiten repräsentieren. Die Höhe jedes Rechtecks zeigt die (relative oder absolute) Häufigkeitsdichte an, also die Häufigkeit geteilt durch die Breite der jeweiligen Klasse.\n\nggplot(sample_mittel, aes(alter)) + \n  geom_histogram()\n\n\n\n\nAuch hier lassen sich relevante Aspekte anpassen, etwa die Anzahl und Breite der Flächen.\n\nggplot(sample_mittel, aes(alter)) + \n  geom_histogram(bins = 40) + \n  ggtitle(\"Altersverteilung der Respondenten\") + \n  xlab(\"Alter\") + ylab(\"Anzahl der RespondentInnen\")\n\n\n\n\nHier können wir erstmalig die Farbe eines Elements gezielt einsetzen, um eine zusätzliche (also nach Vetrauen auf der x-Achse und der Anzahl der Respondenten auf der y-Achse eine dritte Variable) darzustellen, nämlich das Geschlecht der RespondentInnen.\n\nggplot(sample_gross, aes(vertrauen_polizei, fill = geschlecht)) + \n  geom_histogram(binwidth = 1, position = \"dodge\") + \n  ggtitle(\"Vertrauen in die Polizei nach Geschlecht\") + \n  xlab(\"Vetrauen (1-7)\") + ylab(\"Anzahl der RespondentInnen\") + labs(fill = \"Geschlecht\") \n\n\n\n\nNeben Blaken und Flächen beherrscht ggplot auch zahlreiche weitere Darstellungsformen (sog. geoms). Eine interessante Alternative zum klassischen Histogramm ist etwa das Dichte-Plot (density plot). Nachstehend verwenden wir zwei Flächen und einen Trasparenz-Effekt für die Darstellung."
  },
  {
    "objectID": "Skript_3.4.html#dichte-plots",
    "href": "Skript_3.4.html#dichte-plots",
    "title": "Tabellen und Grafiken in R",
    "section": "3.3 Dichte-Plots",
    "text": "3.3 Dichte-Plots\n\npolizei &lt;- sample_gross %&gt;% select(vertrauen_polizei, geschlecht) %&gt;% filter(!is.na(geschlecht))\nggplot(polizei, aes(vertrauen_polizei, fill = geschlecht)) + \n  geom_density(alpha = 0.5) + \n  ggtitle(\"Vertrauen in die Polizei nach Geschlecht\") + \n  xlab(\"Vetrauen (1-7)\") + ylab(\"Anteil der Respondenten\") + labs(fill = \"Geschlecht\")"
  },
  {
    "objectID": "Skript_3.4.html#liniendiagramme",
    "href": "Skript_3.4.html#liniendiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.4 Liniendiagramme",
    "text": "3.4 Liniendiagramme\nZu den klassischen Plot-Typen gehören neben Barplots und Histogrammen auch Linien-, Punkt- und Streudiagramme, sowie Boxplots.\nLiniendiagramme zeigen den Zusammenhang von zwei Variablen, in diesem Beispiel einer nominalen und einer ordinalen (oder, wenn wir großzügig sind [pseudo]metrischen) Variable, nämlich Wahlabsicht nach Partei und Vertrauen in die Presse.\n\nvertrauen_nach_partei &lt;- sample_gross %&gt;% \n  rename(Partei = wahlabsicht_partei) %&gt;% \n  group_by(Partei) %&gt;% \n  summarise(Vertrauenswürdigkeit = mean(vertrauen_zeitungswesen, na.rm = T))\n\nggplot(vertrauen_nach_partei, aes(Partei, Vertrauenswürdigkeit, group = 1)) + \n  geom_line() + geom_point(size = 3) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Vertrauen in die Presse nach Partei\")\n\n\n\n\nEs lassen sich auch problemlos mehrere Geoms kominieren (hier: Linie und Punkte). Im folgenden Beispiel lässt sich durch eine absteigende Sortierung der Ergebnisse ein klareres Resultat erzielen.\n\nvertrauen_nach_partei_sortiert &lt;- vertrauen_nach_partei %&gt;% \n  arrange(desc(Vertrauenswürdigkeit)) %&gt;% \n  mutate(Rang = row_number())\n\nggplot(vertrauen_nach_partei_sortiert, aes(reorder(Partei, Rang), Vertrauenswürdigkeit, group = 1)) + \n  geom_line() + geom_point(size = 3) + \n  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +\n  ggtitle(\"Verrauen in die Presse nach Partei\") + xlab(\"\")"
  },
  {
    "objectID": "Skript_3.4.html#streudiagramme",
    "href": "Skript_3.4.html#streudiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.5 Streudiagramme",
    "text": "3.5 Streudiagramme\nEin weiterer Diagrammtyp, der häufig zum Einsatz kommt, ist das sog. Streudiagramm (scatter plot). Ein Streudiagramm, auch als Punktwolkebekannt, ist eine visuelle Darstellung von beobachteten Wertepaaren zweier statistischer Merkmale. Diese Wertepaare werden in ein kartesisches Koordinatensystem eingetragen, was eine Ansammlung von Punkten ergibt. Die Darstellung der Punkte kann mit verschiedenen Symbolen erfolgen.\nMit diesem Plottypen lassen sich die Beziehung mehrerer Variablen (i.d.R. zwei, jeweils auf der x/y-Achse) anschaulich darstellen und sowohl (lineare oder nicht-lineare) Zusammenhänge indetifizieren als auch Cluster bilder.\nWir erzeugen zunächst einen Data Frame, der Informationen zu Geschlecht, Alter, Einkommen und Bildung der StudienteilnehmerInnen enthält.\n\neinkommen &lt;- daten %&gt;% \n  select(age, sex, educ, di01a) %&gt;% \n  rename(alter = age,\n         geschlecht = sex,\n         bildung = educ,\n         einkommen = di01a) %&gt;% \n  replace_with_na_all(condition = ~.x &lt; 0) %&gt;% \n  mutate(geschlecht = as_factor(geschlecht),\n         bildung = as_factor(bildung)) %&gt;% \n  drop_na() %&gt;% \n  slice_sample(n = 80)\neinkommen\n\n# A tibble: 80 × 4\n   alter     geschlecht bildung            einkommen\n   &lt;dbl+lbl&gt; &lt;fct&gt;      &lt;fct&gt;              &lt;dbl+lbl&gt;\n 1 41        FRAU       MITTLERE REIFE     2120     \n 2 27        MANN       HOCHSCHULREIFE     1920     \n 3 61        FRAU       VOLKS-,HAUPTSCHULE  650     \n 4 26        FRAU       HOCHSCHULREIFE     1800     \n 5 35        FRAU       VOLKS-,HAUPTSCHULE  900     \n 6 38        MANN       MITTLERE REIFE     2600     \n 7 58        MANN       HOCHSCHULREIFE     2700     \n 8 39        FRAU       HOCHSCHULREIFE     2300     \n 9 34        FRAU       HOCHSCHULREIFE     1500     \n10 56        FRAU       MITTLERE REIFE     1000     \n# ℹ 70 more rows\n\n\nWelche Beziehung lässt sich zwischen Alter (x-Achse) und dem Einkommen (y-Achse) der RespondentInnen feststellen?\n\nggplot(einkommen, aes(alter, einkommen)) +\n  geom_point()\n\n\n\n\nWir entwickeln dieses Beispiel nun noch etwas weiter.\n\nggplot(einkommen, aes(alter, einkommen)) +\n  geom_jitter(width = 1) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') + \n  ggtitle(\"Zusammenhang zwischen Alter und Nettoeinkommen\") + \n  xlab(\"Alter\") + ylab(\"Nettoeinkommen in Euro\")\n\n\n\n\nBei der Linie, die wir mit dem Befehl geom_smooth gezeichnet haben, handelt es sich um eine Regressionsgerade. Der graue Bereich um die Gerade zeigt den lokalen Standardfehler an. Auf Regressionmodelle gehen wir zum Abschluss des Moduls noch intensiv ein. Schon jetzt kann man aber erahnen, was uns die Regressionsgerade illustriert.\nWas, wenn wir mehr als drei Variablen (bzw. unterschiedliche Ausprägungen einer kategorialen Variable) darstellen wollen? Neben der Positionierung auf der x- und y-Achse und der Farbe können wir hier zusätzlich auch noch mit unterschiedlichen Formen arbeiten.\nDas nachstehende Beispiel ist nicht unbedingt besonders informativ, zeigt aber das Prinzip nachvollziehbar auf.\n\nggplot(einkommen, aes(alter, einkommen, color = bildung, shape = geschlecht)) +\n  geom_jitter(width = .5, height = .5, size = 2) +\n  ggtitle(\"Zusammenhang zwischen Alter und Nettoeinkommen \") + \n  xlab(\"Alter\") + ylab(\"Nettoeinkommen\") + \n  labs(color = \"Bildungsabschluss\") + labs(shape = \"Geschlecht\")"
  },
  {
    "objectID": "Skript_3.4.html#facettierte-plots",
    "href": "Skript_3.4.html#facettierte-plots",
    "title": "Tabellen und Grafiken in R",
    "section": "3.7 Facettierte Plots",
    "text": "3.7 Facettierte Plots\nIm letzten Abschnitt behandeln wir eine weitere nützliche Funktion von ggplot, um eine größere Zahl von Variablen zu visualisieren – die sog. Facettierung.\nDazu zählen wir zunächst wie zu Beginn des Kapitels mehrere Variablen aus, hier die Altersgruppe, das Geschlecht und die zuvor ermittelte Demokratiezufriedenheit als binäre Variable (eher zufrieden/eher unzufrieden).\n\ndemokratiezefriedenheit_gruppen &lt;- demokratiezefriedenheit %&gt;% \n  count(altersgruppe, geschlecht, zufriedenheit_demokratie_zusammengefasst)\n\nDas nun folgenden Plot hat die Altersgruppe auf der X-Achse, die Anzahl der RespondentInnen auf der Y-Achse und das Geschlecht als Füllfarbe. Zu diesen drei Variablen kommt noch eine weitere dazu (Demokratiezufriedenheit) nach der facettiert wird.\n\nggplot(demokratiezefriedenheit_gruppen, aes(altersgruppe, n, fill = geschlecht)) +\n  geom_bar(stat = \"identity\") + \n  facet_grid(cols = vars(zufriedenheit_demokratie_zusammengefasst)) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Demokratiezufriedenheit nach Alter und Geschlecht\") + \n  xlab(\"Altersgruppe\") + ylab(\"StudienteilnehmerInnen\")"
  },
  {
    "objectID": "Skript_7.1.html#das-überprüfen-von-zusammenhängen-bei-zwei-nominalskalierten-bis-ordinalskalierten-variablen",
    "href": "Skript_7.1.html#das-überprüfen-von-zusammenhängen-bei-zwei-nominalskalierten-bis-ordinalskalierten-variablen",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 1: Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "",
    "text": "Der Pearson Chi-Quadrat Test ist ein statistisches Verfahren, das verwendet wird, um zu überprüfen, ob es einen statistisch signifikanten Zusammenhang zwischen zwei nominalskalierten Variablen gibt; er kann auch bei ordinalskalierten Variablen angewendet werden. Der Chi-Quadrat-Tests testet die Nullhypothese, dass die beiden Variablen unabhängig voneinander sind. Mit dem Chi-Quadrat-Test wird dazu der Chi-Quadrat-Wert berechnet, indem die beiden untersuchten Variablen in einer sogenannten “Kreuztabelle” gegenüber gestellt und dann die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten verglichen werden.\nEin Beispiel für eine Zusammenhangshypothese zwischen zwei nominalskalierten Variablen, ist die Annahme, dass es einen Zusammenhang zwischen dem Geschlecht und der Religionszugehörigkeit gibt: Es gibt einen Zusammenhang zwischen dem Geschlecht der Befragten (sex) und der Konfessionszugehörigkeit (rd01). Diese Annahme können wir mit einem Chi-Quadrat-Test überprüfen.\nIn R kann der Pearson Chi-Quadrat Test mit der Funktion chisq.test() durchgeführt werden. Zuerst müssen wir dazu aber erst einmal die benötigen Pakete und Daten laden…"
  },
  {
    "objectID": "Skript_7.1.html#laden-der-pakete",
    "href": "Skript_7.1.html#laden-der-pakete",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.1 Laden der Pakete",
    "text": "2.1 Laden der Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, psych, conflicted, dplyr) \nconflicts_prefer(dplyr::filter)"
  },
  {
    "objectID": "Skript_7.1.html#laden-des-datensatzes",
    "href": "Skript_7.1.html#laden-des-datensatzes",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.2 Laden des Datensatzes",
    "text": "2.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nDie Variable Konfessionszugehörigkeit ist im Allbus-Datensatz mit dem Namen “rd01” benannt; wie benennen sie in “Konfession” um; außerdem filtern wir die für uns relevanten Fälle der Variablen heraus (und ignorieren damit die irrelevanten Fälle, z.B. -9=Keine Angabe):"
  },
  {
    "objectID": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen",
    "href": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.3 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen",
    "text": "2.3 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen\n\ndaten &lt;- daten %&gt;%\n  rename(Konfession = rd01)%&gt;%\n  filter(between(Konfession, 1, 6))%&gt;%\n  filter(between(sex, 1, 3))\n\nNun erstellen wir eine Kreuztabelle (auch Kontingenztafel genannt), um die Häufigkeiten der verschiedenen Kombinationen von Kategorien beider Variablen anzuzeigen. Das gibt uns einen ersten Überblick darüber, wie oft bestimmte Kombinationen vorkommen."
  },
  {
    "objectID": "Skript_7.1.html#erstellen-und-ausgeben-einer-kreuztabelle",
    "href": "Skript_7.1.html#erstellen-und-ausgeben-einer-kreuztabelle",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.4 Erstellen und Ausgeben einer Kreuztabelle",
    "text": "2.4 Erstellen und Ausgeben einer Kreuztabelle\n\nkreuztabelle &lt;- table(daten$sex, daten$Konfession)\nprint(kreuztabelle)\n\n   \n       1    2    3    4    5    6\n  1  575   53  578   42   49 1241\n  2  683   59  640   63   48 1123\n  3    1    0    0    0    0    2\n\n\nHier können Sie auch noch einmal nachschauen, wie Kreuztabellen erstellt werden"
  },
  {
    "objectID": "Skript_7.1.html#visualisieren-des-zusammenhangs-mit-hilfe-eines-gestapelten-balkendiagramms",
    "href": "Skript_7.1.html#visualisieren-des-zusammenhangs-mit-hilfe-eines-gestapelten-balkendiagramms",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.6 Visualisieren des Zusammenhangs mit Hilfe eines gestapelten Balkendiagramms",
    "text": "2.6 Visualisieren des Zusammenhangs mit Hilfe eines gestapelten Balkendiagramms\nEin gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) “aufeinandergestapelt”, wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den “as.factor”-Befehl nutzen:"
  },
  {
    "objectID": "Skript_7.1.html#erstellen-des-gestapelten-balkendiagramms",
    "href": "Skript_7.1.html#erstellen-des-gestapelten-balkendiagramms",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.7 Erstellen des gestapelten Balkendiagramms",
    "text": "2.7 Erstellen des gestapelten Balkendiagramms\n\nlibrary(ggplot2)\n\ndaten$Konfession &lt;- as.factor(daten$Konfession)\ndaten$sex &lt;- as.factor(daten$sex)\n\nggplot(daten, aes(x = Konfession, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Verteilung der Konfessionszugehörigkeit nach Geschlecht\",\n       x = \"Einkommensgruppe\", y = \"Anteil\") +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\")) # Farben anpassen\n\n\n\n\nDiese Visualisierung zeigt sehr schön, was wir auch schon in der Kreuztabelle ablesen konnten: Es gibt keinen deutlich sichtbaren Zusammenhang zwischen der Konfessionszugehörigkeit und dem Geschlecht - allenfalls lässt sich vermuten, dass mehr Frauen als Männer überhaupt einer Konfession angehören. Ob sich der Zusammenhang als statistisch bedeutsam erweist, können wir nun mit dem Chi-Quadrat Test prüfen:"
  },
  {
    "objectID": "Skript_7.1.html#überprüfen-des-zusammenhangs-mit-dem-chi-quadrat-test",
    "href": "Skript_7.1.html#überprüfen-des-zusammenhangs-mit-dem-chi-quadrat-test",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.8 Überprüfen des Zusammenhangs mit dem Chi-Quadrat Test",
    "text": "2.8 Überprüfen des Zusammenhangs mit dem Chi-Quadrat Test\nDer Chi-Quadrat Test ist ein statistisches Verfahren, das verwendet wird, um festzustellen, ob es einen signifikanten Unterschied zwischen den beobachteten und erwarteten Häufigkeiten in einer Kreuztabelle gibt. Da wir schon eine Kreuztabelle erstellt haben, können wir die Funktion chisq.test auf unsere Kreuztabelle anwenden - das geht ganz einfach:\n\nchisq.test(kreuztabelle)\n\n\n    Pearson's Chi-squared test\n\ndata:  kreuztabelle\nX-squared = 22.946, df = 10, p-value = 0.01095"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-1",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.9 Interpretation: Was sehen wir im Output?",
    "text": "2.9 Interpretation: Was sehen wir im Output?\nSchauen wir uns nun den R-Output an: Der Chi-Quadrat-Wert (X² oder X-squared) gibt uns an, wie gut die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten übereinstimmen. Ein höherer X²-Wert deutet auf eine größere Abweichung hin.\ndf zeigt die Anzahl der Freiheitsgrade (Degrees of Freedom) des Chi-Quadrat-Tests an. Sie hängt von der Anzahl der Kategorien in den Variablen ab und beeinflusst die Verteilung des Chi-Quadrat-Werts.\nDer p-Wert gibt die Wahrscheinlichkeit an, den beobachteten Chi-Quadrat-Wert zu erhalten, wenn die Nullhypothese wahr ist (d.h. wenn es keinen Zusammenhang zwischen den Variablen gibt). Der hier beobachtete p-Wert (von 0.01095) ist kleiner als .05 und damit signifikant. Das deutet darauf hin, dass der beobachtete Effekt überzufällig ist - Geschlecht und Konfessionszugehörigkeit hängen also statistisch zusammen.\nAllerdings ist der Chi-Quadrat-Test empfindlich gegenüber der Stichprobengröße: Bei sehr großen Stichproben können auch kleine Unterschiede signifikant werden. In solchen Fällen ist es ratsam, neben dem p-Wert auch die Effektstärke zu betrachten. Dazu ermitteln wir nun noch Cramér’s V. Das ist ein Maß für den Zusammenhang zwischen zwei kategorialen Variablen. Es reicht von 0 bis 1, wobei 0 keinen Zusammenhang und 1 einen vollständigen Zusammenhang anzeigt."
  },
  {
    "objectID": "Skript_7.1.html#berechnen-der-effektstärke-cramérs-v",
    "href": "Skript_7.1.html#berechnen-der-effektstärke-cramérs-v",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.4 Berechnen der Effektstärke Cramér’s V",
    "text": "3.4 Berechnen der Effektstärke Cramér’s V\nZuerst extrahieren wir den beobachteten Chi-Quadrat-Wert und speichern ihn in einem Datenobjekt chi_square ab. Dann berechnen wir die Effektstärke nach der Formel für Cramér’s V.\n\ncramers_v &lt;- cramersV(kreuztabelle)\npaste0(\"Cramér's V: \", cramers_v)\n\n[1] \"Cramér's V: 0.0471667326679284\"\n\n\nWie wir nun sehen, ist unser Wert für Cramér’s V mit 0.047 sehr gering. Das deutet auf einen sehr schwachen Zusammenhang zwischen den betrachteten Variablen hin. Der Zusammenhang ist so schwach, dass er in der praktischen Anwendung wahrscheinlich vernachlässigbar ist. Das deckt sich mit unserer “visuellen Inspektion” der Daten.\nKommen wir nun zur nächsten Analyse-Option: der Analyse von Zusammenhängen zwischen zwei ordinalskalierten Variablen:"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-2",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-2",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "2.11 Interpretation: Was sehen wir im Output?",
    "text": "2.11 Interpretation: Was sehen wir im Output?\nWie wir nun sehen, ist unser Wert für Cramér’s V mit 0.047 sehr gering. Das deutet auf einen sehr schwachen Zusammenhang zwischen den betrachteten Variablen hin. Der Zusammenhang ist so schwach, dass er in der praktischen Anwendung wahrscheinlich vernachlässigbar ist. Das deckt sich mit unserer “visuellen Inspektion” der Daten.\nKommen wir nun zur nächsten Analyse-Option: der Analyse von Zusammenhängen zwischen zwei ordinalskalierten Variablen:"
  },
  {
    "objectID": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen-1",
    "href": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.1 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen",
    "text": "3.1 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen\n\ndaten &lt;- daten %&gt;%\n  rename(Einkommensgruppe = di02a)%&gt;%\n  rename(Bildung = educ)%&gt;%\n  filter(between(Einkommensgruppe, 1, 25))%&gt;%\n  filter(between(Bildung, 1, 5))\n\nNun nutzen wir die Funktion cor.test() und spezifizieren bei “method” den Korrelationskoeefizienten, zuerst “spearman”, dann zum Vergleich auch “kendall”:"
  },
  {
    "objectID": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-spearmans-rho-und-ausgabe-der-werte",
    "href": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-spearmans-rho-und-ausgabe-der-werte",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.2 Berechnung der Rangkorrelation nach Spearman’s Rho und Ausgabe der Werte",
    "text": "3.2 Berechnung der Rangkorrelation nach Spearman’s Rho und Ausgabe der Werte\n\nresult_spearman &lt;- cor.test(daten$Einkommensgruppe, daten$Bildung, method = \"spearman\")\nprint(result_spearman)\n\n\n    Spearman's rank correlation rho\n\ndata:  daten$Einkommensgruppe and daten$Bildung\nS = 3725893693, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2799733"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-3",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-3",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.3 Interpretation: Was sehen wir im Output?",
    "text": "3.3 Interpretation: Was sehen wir im Output?\nSchauen wir uns nun den Output an: Der Wert rho gibt den berechneten Korrelationskoeffizienten wieder. Der Wert von 0,28 deutet darauf hin, dass es einen positiven Zusammenhang zwischen den beiden Variablen Bildungsabschluss und Einkommensgruppe gibt.\nDer sehr kleine p-Wert (kleiner als 2.2e-16, was nahezu null ist), zeigt an, dass der beobachtete Zusammenhang zwischen den Variablen statistisch höchst signifikant ist.\nDer S-Wert (3725893693) repräsentiert die Summe der quadrierten Unterschiede zwischen den Rängen der beiden Variablen. Er ist Teil der Berechnung des Spearman’s Rho und wird für die Interpretation eigentlich nicht unbedingt benötigt.\nSchauen wir uns nun zum Vergleich das Ergebnis mit Kendall’s Tau an:"
  },
  {
    "objectID": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-kendalls-tau-und-ausgabe-der-werte",
    "href": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-kendalls-tau-und-ausgabe-der-werte",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.4 Berechnung der Rangkorrelation nach Kendall’s Tau und Ausgabe der Werte",
    "text": "3.4 Berechnung der Rangkorrelation nach Kendall’s Tau und Ausgabe der Werte\n\nresult_kendall &lt;- cor.test(daten$Einkommensgruppe, daten$Bildung, method = \"kendall\")\nprint(result_kendall)\n\n\n    Kendall's rank correlation tau\n\ndata:  daten$Einkommensgruppe and daten$Bildung\nz = 16.019, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2202662"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-4",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-4",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.5 Interpretation: Was sehen wir im Output?",
    "text": "3.5 Interpretation: Was sehen wir im Output?\nDer z-Wert gibt an, wie viele Standardabweichungen die beobachtete Korrelation von der erwarteten Korrelation entfernt ist. Ein höherer z-Wert deutet darauf hin, dass die beobachtete Korrelation signifikant von Null abweicht. In unserem Fall ist der z-Wert sehr hoch (16.343), was darauf hindeutet, dass die beobachtete Korrelation sehr weit von Null entfernt ist.\nDer p-Wert ist die Wahrscheinlichkeit mit der die beobachtete Korrelation zufällig ist. Ein kleiner p-Wert deutet darauf hin, dass die beobachtete Korrelation sehr unwahrscheinlich ist, wenn kein Zusammenhang bestehen würde. Da unser p-Wert extrem klein ist (&lt; 2.2e-16), können wir von einem Zusammenhang ausgehen. Beide Rangkorrelationskoeffizienten liefern uns also ein ähnliches Ergebnis und zeigen einen (positiven) Zusammenhang zwischen dem Bildungsabschluss und der Einkommensgruppe."
  },
  {
    "objectID": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen-2",
    "href": "Skript_7.1.html#erzeugen-eines-teildatensatzes-umbenennen-und-filtern-der-variablen-2",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.1 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen",
    "text": "4.1 Erzeugen eines Teildatensatzes, Umbenennen und Filtern der Variablen\n\ndaten_neu &lt;- daten %&gt;%\n  filter(between(Einkommensgruppe, 1, 25))%&gt;%\n  mutate(sex = as.numeric(sex)) %&gt;%\n  filter(between(sex, 1, 3))\n\nNun erstellen wir wieder eine Kreuztabelle, um die Häufigkeiten der verschiedenen Kombinationen von Kategorien beider Variablen anzuzeigen. Damit erhalten wir einen ersten Überblick über die Verteilung der Daten entlang der Vergleichskategorie:"
  },
  {
    "objectID": "Skript_7.1.html#erstellen-und-ausgeben-einer-kreuztabelle-1",
    "href": "Skript_7.1.html#erstellen-und-ausgeben-einer-kreuztabelle-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.2 Erstellen und Ausgeben einer Kreuztabelle",
    "text": "4.2 Erstellen und Ausgeben einer Kreuztabelle\n\nkreuztabelle_2 &lt;- table(daten_neu$sex, daten_neu$Einkommensgruppe)\nprint(kreuztabelle_2)\n\n   \n      1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n  1   6   2  10  27  15  22  26  42  64  48  70  68 119 163 154 113  79  71 101\n  2  13  23  36  64  42  82  68 134 127  88 112 111 158 167 159 102  51  47  63\n   \n     20  21  22  23  24  25\n  1  77  48  35  32  17  16\n  2  27  16  10   7   5   6"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-5",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-5",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.3 Interpretation: Was sehen wir im Output?",
    "text": "4.3 Interpretation: Was sehen wir im Output?\nIm Datensatz entsprechen höhere Werte bei der variable Einkommensgruppe einem höheren Einkommen (1=bis unter 200 Euro; 2= 200 bis unter 300 Euro; 25=7.500 bis unter 10.000 Euro). Wenn wir uns nun die Kreuztabelle anschauen, können wir schon einen ersten Trend erkennen: Offenbar sind mehr Männer in höheren Einkommensgruppen als Frauen; dafür sind weniger Männer in niedrigen Einkommensgruppen. Hier deutet sich also an, dass Geschlecht und Einkommensgruppe einen Zusammenhang aufweisen.\nMit einem gestapelten Balkendiagramm können wir diese Tendenz auch visualisieren:"
  },
  {
    "objectID": "Skript_7.1.html#visualisieren-des-zusammenhangs-mit-hilfe-eines-gestapelten-balkendiagramms-1",
    "href": "Skript_7.1.html#visualisieren-des-zusammenhangs-mit-hilfe-eines-gestapelten-balkendiagramms-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.4 Visualisieren des Zusammenhangs mit Hilfe eines gestapelten Balkendiagramms",
    "text": "4.4 Visualisieren des Zusammenhangs mit Hilfe eines gestapelten Balkendiagramms\nEin gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) “aufeinandergestapelt”, wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den “as.factor”-Befehl nutzen:"
  },
  {
    "objectID": "Skript_7.1.html#erstellen-des-gestapelten-balkendiagramms-1",
    "href": "Skript_7.1.html#erstellen-des-gestapelten-balkendiagramms-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.5 Erstellen des gestapelten Balkendiagramms",
    "text": "4.5 Erstellen des gestapelten Balkendiagramms\n\nlibrary(ggplot2)\n\ndaten_neu$Einkommensgruppe &lt;- as.factor(daten_neu$Einkommensgruppe)\ndaten_neu$sex &lt;- as.factor(daten$sex)\n\nggplot(daten_neu, aes(x = Einkommensgruppe, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Verteilung der Einkommensgruppe nach Geschlecht\",\n       x = \"Einkommensgruppe\", y = \"Anteil\") +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"green\")) # Farben anpassen\n\n\n\n\nDiese Visualisierung zeigt sehr schön, was wir auch schon anhand der Kreuztabelle ablesen konnten: Es sind mehr Frauen als Männer in den unteren Einkommensgruppen zu finden; es sind mehr Männer als Frauen in höheren Einkommensgruppen zu finden.\nNun prüfen wir mit einem Chi-Quadrat Test, ob der beobachtete Zusammenhang auch statistisch signifikant ist. Dazu nutzen wir wieder die chisq.test-Funktion, die wir auf unsere Kreuztabelle anwenden:"
  },
  {
    "objectID": "Skript_7.1.html#überprüfung-des-zusammenhangs-mit-dem-chi-quadrat-test",
    "href": "Skript_7.1.html#überprüfung-des-zusammenhangs-mit-dem-chi-quadrat-test",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "5.4 Überprüfung des Zusammenhangs mit dem Chi-Quadrat Test",
    "text": "5.4 Überprüfung des Zusammenhangs mit dem Chi-Quadrat Test\n\nchisq.test(kreuztabelle_2)\n\n\n    Pearson's Chi-squared test\n\ndata:  kreuztabelle_2\nX-squared = 299.01, df = 24, p-value &lt; 2.2e-16\n\n\nSchauen wir uns nun den R-Output an: Der Chi-Quadrat-Wert (X² oder X-squared) gibt uns an, wie gut die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten übereinstimmen. Ein höherer X²-Wert deutet auf eine größere Abweichung hin. Der Chi-Quadrat-Statistikwert von 299.01 ist also weit von dem erwarteten Wert entfernt.\ndf zeigt die Anzahl der Freiheitsgrade (Degrees of Freedom) des Chi-Quadrat-Tests an. Sie hängt von der Anzahl der Kategorien in den Variablen ab und beeinflusst die Verteilung des Chi-Quadrat-Werts.\nDer p-Wert gibt die Wahrscheinlichkeit an, den beobachteten Chi-Quadrat-Wert zu erhalten, wenn die Nullhypothese wahr ist (d.h. wenn es keinen Zusammenhang zwischen den Variablen gibt). Der hier beobachtete p-Wert (&lt; 2.2e-16) ist extrem klein und damit höchst signifikant. Das deutet darauf hin, dass der beobachtete Effekt überzufällig ist - Geschlecht und Konfessionszugehörigkeit hängen also auch statistisch zusammen.\nDa der Chi-Quadrat-Test empfindlich gegenüber der Stichprobengröße ist, berechnen wir nun noch die Effektstärke mittels Cramér’s V. Hier gilt ein Wert von 0,1 als klein, einer von 0,3 als mittel und ein Wert von 0,5 als groß.\n\ncramers_v &lt;- cramersV(kreuztabelle_2)\npaste0(\"Cramér's V: \", cramers_v)\n\n[1] \"Cramér's V: 0.3084421864275\"\n\n\nMit Cramér’s V können wir nun also zusätzlich noch aussagen, dass der Effekt von Geschlecht auf die Einkommensklasse bei 0,3 liegt und damit mittelstark ist. Wir haben es also mit einem signifikanten und substantiellen Einfluss des Geschlechts auf die Einkommensklasse zu tun. Da wir diesen Befund mit Hilfe aktueller ALLBUS-Daten ermittelt haben, haben Sie hiermit einen guten Indikator für die aktuelle Gender Equality in Deutschland vor Augen. ES GIBT NOCH VIEL ZU TUN!"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-6",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-6",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.7 Interpretation: Was sehen wir im Output?",
    "text": "4.7 Interpretation: Was sehen wir im Output?\nSchauen wir uns nun den R-Output an: Der Chi-Quadrat-Wert (X² oder X-squared) gibt uns an, wie gut die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten übereinstimmen. Ein höherer X²-Wert deutet auf eine größere Abweichung hin. Der Chi-Quadrat-Statistikwert von 299.01 ist also weit von dem erwarteten Wert entfernt.\ndf zeigt die Anzahl der Freiheitsgrade (Degrees of Freedom) des Chi-Quadrat-Tests an. Sie hängt von der Anzahl der Kategorien in den Variablen ab und beeinflusst die Verteilung des Chi-Quadrat-Werts.\nDer p-Wert gibt die Wahrscheinlichkeit an, den beobachteten Chi-Quadrat-Wert zu erhalten, wenn die Nullhypothese wahr ist (d.h. wenn es keinen Zusammenhang zwischen den Variablen gibt). Der hier beobachtete p-Wert (&lt; 2.2e-16) ist extrem klein und damit höchst signifikant. Das deutet darauf hin, dass der beobachtete Effekt überzufällig ist - Geschlecht und Konfessionszugehörigkeit hängen also auch statistisch zusammen.\nDa der Chi-Quadrat-Test empfindlich gegenüber der Stichprobengröße ist, berechnen wir nun noch die Effektstärke mittels Cramér’s V. Hier gilt ein Wert von 0,1 als klein, einer von 0,3 als mittel und ein Wert von 0,5 als groß.\n\n# Schritt 1: Extrahieren des beobachteten Chi-Quadrat-Werts\nchi_square &lt;- chisq.test(kreuztabelle_2)$statistic\n\n# Schritt 2: Berechnen der Effektstärke (Cramér's V)\nn &lt;- sum(kreuztabelle_2)  # Gesamtanzahl der Beobachtungen\nk &lt;- nrow(kreuztabelle_2)  # Anzahl der Kategorien in 'sex'\nr &lt;- ncol(kreuztabelle_2)  # Anzahl der Kategorien in 'Konfession'\n\nV &lt;- sqrt(chi_square / (n * min(k-1, r-1)))\n\n# Schritt 3: Ausgabe der beobachteten Chi-Quadrat-Wert und Cramér's V \ncat(\"Beobachteter Chi-Quadrat-Wert:\", chi_square, \"\\n\")\n\nBeobachteter Chi-Quadrat-Wert: 299.0143 \n\ncat(\"Cramér's V:\", V, \"\\n\")\n\nCramér's V: 0.3084422"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-7",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output-7",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.8 Interpretation: Was sehen wir im Output?",
    "text": "4.8 Interpretation: Was sehen wir im Output?\nMit Cramér’s V können wir nun also zusätzlich noch aussagen, dass der Effekt von Geschlecht auf die Einkommensklasse bei 0,3 liegt und damit mittelstark ist. Wir haben es also mit einem signifikanten und substantiellen Einfluss des Geschlechts auf die Einkommensklasse zu tun. Da wir diesen Befund mit Hilfe aktueller ALLBUS-Daten ermittelt haben, haben Sie hiermit einen guten Indikator für die aktuelle Gender Equality in Deutschland vor Augen. ES GIBT NOCH VIEL ZU TUN!"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-was-sehen-wir-im-streudiagramm",
    "href": "Skript_7.2.html#interpretation-was-sehen-wir-im-streudiagramm",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.1 Interpretation: Was sehen wir im Streudiagramm?",
    "text": "15.1 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen positiven und linearen Zusammenhang zwischen Alter und Fernsehnutzung nahe: mit zunehmendem Alter steigt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt."
  },
  {
    "objectID": "Skript_7.2.html#durchführung-der-einfachen-linearen-regression-über-die-funktion-lm",
    "href": "Skript_7.2.html#durchführung-der-einfachen-linearen-regression-über-die-funktion-lm",
    "title": "Korrelation & Regression",
    "section": "3.4 Durchführung der einfachen linearen Regression über die Funktion lm",
    "text": "3.4 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regression prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: TV_Konsum), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (TV_Konsum) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll. Zum Schluss lassen wir uns das Modell ausgeben.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n3.4.1 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(TV_Konsum ~ Alter, data = daten) \nprint(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nCoefficients:\n(Intercept)        Alter  \n     64.644        2.193  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn nun mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n3.4.2 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten” die abhängige Variable “TV_Konsum” durch die unabhängige Variable “Alter” zu erklären.\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nDas Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\nMit St.error wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (8,8) Prozent der Varianz der TV-Nutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Intensität der Fernsehnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (das kommt in der Realität aber fast nicht vor).\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n3.4.3 Inhaltliche Interpretation des Outputs: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen positiven Einfluss auf die tägliche Fernsehnutzung in Minuten. Je älter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Konsum um 2,19 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Das Alter beeinflusst die Dauer der Fernsehnutzung, R2 = .08, F(1, 4927) = 474.1, p = .000.\n\n\n\n\n\n3.4.4 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine lineare Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (TV_Konsum) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\n\n3.4.5 Beispiel 1: Vorhersage für die tägliche Internetnutzung\nWir lassen uns mit Hilfe unseres Modells zunächst die tägliche Internetnutzung in Minuten bei einem Alter von 25 und 75 vorhersagen.\n\npredict.lm(model, data.frame(Alter = 25))\n\n       1 \n119.4635 \n\npredict.lm(model, data.frame(Alter = 75))\n\n       1 \n229.1028 \n\n\n\n\n3.4.6 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von 119 Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von 229 Minuten auf.\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe",
    "href": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.3 Interpretation des Outputs: Was sehen wir in der Ausgabe?",
    "text": "15.3 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten” die abhängige Variable “TV_Konsum” durch die unabhängige Variable “Alter” zu erklären.\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nDas Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\nMit St.error wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\nAuch beide **R-Werte* (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (8,8) Prozent der Varianz der TV-Nutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Intensität der Fernsehnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (das kommt in der Realität aber fast nicht vor).\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)"
  },
  {
    "objectID": "Skript_7.2.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-jetzt-also-alles",
    "href": "Skript_7.2.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-jetzt-also-alles",
    "title": "Korrelation & Regression",
    "section": "2.5 Inhaltliche Interpretation des Outputs: Was bedeutet das jetzt also alles?",
    "text": "2.5 Inhaltliche Interpretation des Outputs: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen positiven Einfluss auf die tägliche Fernsehnutzung in Minuten. Je älter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Konsum um 2,19 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Das Alter beeinflusst die Dauer der Fernsehnutzung, R2 = .08, F(1, 4927) = 474.1, p = .000."
  },
  {
    "objectID": "Skript_7.2.html#vorhersage-von-werten-auf-basis-des-modells",
    "href": "Skript_7.2.html#vorhersage-von-werten-auf-basis-des-modells",
    "title": "Korrelation & Regression",
    "section": "2.6 Vorhersage von Werten auf Basis des Modells",
    "text": "2.6 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine lineare Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (TV_Konsum) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\nWir lassen uns mit Hilfe unseres Modells zunächst die tägliche Internetnutzung in Minuten bei einem Alter von 25 und 75 vorhersagen.\n\npredict.lm(model, data.frame(Alter = 25))\n\n       1 \n119.4635 \n\npredict.lm(model, data.frame(Alter = 75))\n\n       1 \n229.1028 \n\n\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von 119 Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von 229 Minuten auf.\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(Alter = c(25, 75)))\n\n       1        2 \n119.4635 229.1028 \n\n\nIn unserem zweiten Beispiel betrachten wir andere Altersgruppen. Wir lassen uns hier mit Hilfe unseres Modells die tägliche Internetnutzung in Minuten bei einem Alter von 20, 30, 40, 50, 60 und 70 vorhersagen.\n\npredict.lm(model, data.frame(Alter = c(20, 30, 40, 50, 60, 70)))\n\n       1        2        3        4        5        6 \n108.4996 130.4274 152.3553 174.2832 196.2110 218.1389 \n\n\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable TV-Konsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen TV-Konsum von (X) Minuten.\nWeil wir im Datensatz aber n=4929 Fälle haben (und die Ausgabe sonst zu unübersichtlich wird), begrenzen ich die Ausgabe auf die ersten 100 Zeilen (Fälle) des Datensatzes, indem ich zusätzlich die Funktion head() verwende, und die Anzahl der gewünschten Fälle in der Klammer mit 100 festlege:\n\nhead(fitted(model), 100)\n\n       1        2        3        4        5        6        7        8 \n183.0543 180.8615 259.8019 237.8740 200.5966 115.0779 132.6202 213.7533 \n       9       10       11       12       13       14       15       16 \n176.4760 189.6327 251.0307 121.6563 147.9697 183.0543 163.3192 172.0904 \n      17       18       19       20       21       22       23       24 \n121.6563 246.6451 169.8976 169.8976 224.7173 200.5966 119.4635 183.0543 \n      25       26       27       28       29       30       31       32 \n183.0543 176.4760 196.2110 172.0904 189.6327 191.8255 191.8255 150.1625 \n      33       34       35       36       37       38       39       40 \n244.4523 233.4884 237.8740 112.8851 233.4884 183.0543 174.2832 119.4635 \n      41       42       43       44       45       46       47       48 \n207.1750 187.4399 222.5245 178.6687 213.7533 185.2471 132.6202 237.8740 \n      49       50       51       52       53       54       55       56 \n211.5605 209.3678 115.0779 246.6451 154.5481 189.6327 112.8851 147.9697 \n      57       58       59       60       61       62       63       64 \n215.9461 200.5966 169.8976 204.9822 121.6563 224.7173 172.0904 147.9697 \n      65       66       67       68       69       70       71       72 \n152.3553 174.2832 189.6327 156.7409 185.2471 132.6202 185.2471 213.7533 \n      73       74       75       76       77       78       79       80 \n264.1874 202.7894 187.4399 233.4884 187.4399 191.8255 196.2110 194.0183 \n      81       82       83       84       85       86       87       88 \n176.4760 200.5966 194.0183 207.1750 174.2832 143.5842 119.4635 161.1265 \n      89       90       91       92       93       94       95       96 \n165.5120 161.1265 174.2832 150.1625 172.0904 110.6924 196.2110 233.4884 \n      97       98       99      100 \n110.6924 196.2110 185.2471 178.6687"
  },
  {
    "objectID": "Skript_7.2.html#beispiel-vorhersage-des-modells-für-die-tägliche-internetnutzung-in-minuten-bei-alter-von-25-und-75",
    "href": "Skript_7.2.html#beispiel-vorhersage-des-modells-für-die-tägliche-internetnutzung-in-minuten-bei-alter-von-25-und-75",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.6 Beispiel: Vorhersage des Modells für die tägliche Internetnutzung in Minuten bei Alter von 25 und 75",
    "text": "15.6 Beispiel: Vorhersage des Modells für die tägliche Internetnutzung in Minuten bei Alter von 25 und 75\n\npredict.lm(model, data.frame(Alter = 25))\n\n       1 \n119.4635 \n\npredict.lm(model, data.frame(Alter = 75))\n\n       1 \n229.1028"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-1",
    "href": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-1",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.7 Interpretation des Outputs: Was sehen wir in der Ausgabe?",
    "text": "15.7 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von 119 Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von 229 Minuten auf.\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:"
  },
  {
    "objectID": "Skript_7.2.html#beispiel-2-vorhersage-des-modells-für-die-tägliche-internetnutzung-in-minuten-bei-alter-von-20-30-40-50-60-und-70",
    "href": "Skript_7.2.html#beispiel-2-vorhersage-des-modells-für-die-tägliche-internetnutzung-in-minuten-bei-alter-von-20-30-40-50-60-und-70",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.8 Beispiel 2: Vorhersage des Modells für die tägliche Internetnutzung in Minuten bei Alter von 20, 30, 40, 50, 60 und 70",
    "text": "15.8 Beispiel 2: Vorhersage des Modells für die tägliche Internetnutzung in Minuten bei Alter von 20, 30, 40, 50, 60 und 70\n\npredict.lm(model, data.frame(Alter = c(20, 30, 40, 50, 60, 70)))\n\n       1        2        3        4        5        6 \n108.4996 130.4274 152.3553 174.2832 196.2110 218.1389"
  },
  {
    "objectID": "Skript_7.2.html#vorhersage-und-residuen-berechnen",
    "href": "Skript_7.2.html#vorhersage-und-residuen-berechnen",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.9 Vorhersage und Residuen berechnen",
    "text": "15.9 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable TV-Konsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen TV-Konsum von (X) Minuten.\nWeil wir im Datensatz aber n=4929 Fälle haben (und die Ausgabe sonst zu unübersichtlich wird), begrenzen ich die Ausgabe auf die ersten 100 Zeilen (Fälle) des Datensatzes, indem ich zusätzlich die Funktion head() verwende, und die Anzahl der gewünschten Fälle in der Klammer mit 100 festlege:"
  },
  {
    "objectID": "Skript_7.2.html#vorhersagewerte-für-jede-beobachtung-anzeigen",
    "href": "Skript_7.2.html#vorhersagewerte-für-jede-beobachtung-anzeigen",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.10 Vorhersagewerte für jede Beobachtung anzeigen",
    "text": "15.10 Vorhersagewerte für jede Beobachtung anzeigen\n\nhead(fitted(model), 100)\n\n       1        2        3        4        5        6        7        8 \n183.0543 180.8615 259.8019 237.8740 200.5966 115.0779 132.6202 213.7533 \n       9       10       11       12       13       14       15       16 \n176.4760 189.6327 251.0307 121.6563 147.9697 183.0543 163.3192 172.0904 \n      17       18       19       20       21       22       23       24 \n121.6563 246.6451 169.8976 169.8976 224.7173 200.5966 119.4635 183.0543 \n      25       26       27       28       29       30       31       32 \n183.0543 176.4760 196.2110 172.0904 189.6327 191.8255 191.8255 150.1625 \n      33       34       35       36       37       38       39       40 \n244.4523 233.4884 237.8740 112.8851 233.4884 183.0543 174.2832 119.4635 \n      41       42       43       44       45       46       47       48 \n207.1750 187.4399 222.5245 178.6687 213.7533 185.2471 132.6202 237.8740 \n      49       50       51       52       53       54       55       56 \n211.5605 209.3678 115.0779 246.6451 154.5481 189.6327 112.8851 147.9697 \n      57       58       59       60       61       62       63       64 \n215.9461 200.5966 169.8976 204.9822 121.6563 224.7173 172.0904 147.9697 \n      65       66       67       68       69       70       71       72 \n152.3553 174.2832 189.6327 156.7409 185.2471 132.6202 185.2471 213.7533 \n      73       74       75       76       77       78       79       80 \n264.1874 202.7894 187.4399 233.4884 187.4399 191.8255 196.2110 194.0183 \n      81       82       83       84       85       86       87       88 \n176.4760 200.5966 194.0183 207.1750 174.2832 143.5842 119.4635 161.1265 \n      89       90       91       92       93       94       95       96 \n165.5120 161.1265 174.2832 150.1625 172.0904 110.6924 196.2110 233.4884 \n      97       98       99      100 \n110.6924 196.2110 185.2471 178.6687"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-2",
    "href": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-2",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.11 Interpretation des Outputs: Was sehen wir in der Ausgabe?",
    "text": "15.11 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nIn der Ausgabe kann ich nun die laut Modell prognostizierte Fernsehnutzung für meine Befragten entsprechend ihres Alters sehen - Befragte(r) 10 hat so z.B. eine prognostizierte Fernsehnutzung von 189 Minuten.\nNun haben wir im Rahmen unserer Befragung die TV-Nutzung der Befragten aber ja schon erhoben. Wozu dient diese Prognose dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nhead(residuals(model), 100)\n\n&lt;labelled&lt;double&gt;[100]&gt;: FERNSEHGESAMTDAUER PRO TAG IN MINUTEN\n           1            2            3            4            5            6 \n  26.9456805  -90.8615329 -124.8018509 -177.8739847  -20.5966124  -70.0779345 \n           7            8            9           10           11           12 \n-102.6202274  -33.7533320    3.5240404  -69.6326793  198.9692956 -111.6562944 \n          13           14           15           16           17           18 \n -27.9697337   56.9456805   46.6807600  -82.0903864  -31.6562944 -126.6451312 \n          19           20           21           22           23           24 \n -79.8975998  -19.8975998   75.2827349 1059.4033876    0.5364922   56.9456805 \n          25           26           27           28           29           30 \n  -3.0543195  -26.4759596  -76.2110391    7.9096136  -39.6326793  108.1745341 \n          31           32           33           34           35           36 \n -71.8254659  -90.1625203 -124.4523446    6.5115885 -117.8739847  426.1148521 \n          37           38           39           40           41           42 \n -83.4884115  146.9456805  125.7168270  -29.4635078  -17.1749722   82.5601073 \n          43           44           45           46           47           48 \n -12.5244785  -28.6687462 -138.7533320  -65.2471061  -12.6202274  -27.8739847 \n          49           50           51           52           53           54 \n   8.4394546  150.6322412   94.9220655  -66.6451312  -34.5480935  -39.6326793 \n          55           56           57           58           59           60 \n -52.8851479 -132.9697337  -65.9461186  -20.5966124  -49.8975998  -24.9821856 \n          61           62           63           64           65           66 \n -31.6562944  -14.7172651    7.9096136    2.0302663  -92.3553069    5.7168270 \n          67           68           69           70           71           72 \n -69.6326793  -36.7408801   -5.2471061  -72.6202274   84.7528939  -93.7533320 \n          73           74           75           76           77           78 \n-174.1874241  -82.7893990  126.5601073    6.5115885   52.5601073  -71.8254659 \n          79           80           81           82           83           84 \n  13.7889609  -44.0182525  -26.4759596  -20.5966124   75.9817475  -87.1749722 \n          85           86           87           88           89           90 \n   5.7168270    6.4158395  -89.4635078   18.8735466  -45.5120266    3.8735466 \n          91           92           93           94           95           96 \n   5.7168270  149.8374797  -67.0903864  -80.6923613   68.7889609  246.5115885 \n          97           98           99          100 \n -80.6923613  -16.2110391  -35.2471061  -58.6687462 \n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n   -10       TNZ: FILTER\n    -9      KEINE ANGABE"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-3",
    "href": "Skript_7.2.html#interpretation-des-outputs-was-sehen-wir-in-der-ausgabe-3",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.12 Interpretation des Outputs: Was sehen wir in der Ausgabe?",
    "text": "15.12 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nFür unseren Fall Nummer 65 beträgt die Abweichung der Prognose von der Beobachtung -92 Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 8 Prozent nicht besonders groß ist."
  },
  {
    "objectID": "Skript_7.2.html#vorhersage-und-residuen-grafisch-darstellen",
    "href": "Skript_7.2.html#vorhersage-und-residuen-grafisch-darstellen",
    "title": "Korrelation & Regression",
    "section": "2.8 Vorhersage und Residuen grafisch darstellen",
    "text": "2.8 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten$vorhersage &lt;- predict(model) \ndaten$residuen &lt;- as.numeric(residuals(model))\n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten, aes(Alter, TV_Konsum)) + \n1  geom_point(aes(color = residuen)) +\n2  scale_color_gradient2(low = \"tan4\", mid = \"white\", high = \"darkgreen\") +\n3  guides(color = \"none\") +\n4  geom_point(aes(y = vorhersage), shape = 1) +\n5  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") +\n6  geom_segment(aes(xend = Alter, yend = vorhersage), alpha = .2) +\n7  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Fernsehnutzung\") +\n8  xlab(\"Alter\") + ylab(\"tägliche Fernsehnutzung (Minuten)\")\n\n\n1\n\nFestlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n\n2\n\nFestlegung der Farbe für die Residuen\n\n3\n\nUnterdrückt eine Legende an der Seite (ist obligatorisch)\n\n4\n\ngibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n\n5\n\ngibt die Regressionsgerade als Linie aus\n\n6\n\nzeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein\n\n7\n\nTitel\n\n8\n\nAchsen-Beschriftung"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-der-vorhersagewerte-und-residuen-für-zusätzliche-plots",
    "href": "Skript_7.2.html#berechnung-der-vorhersagewerte-und-residuen-für-zusätzliche-plots",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.14 Berechnung der Vorhersagewerte und Residuen für zusätzliche Plots",
    "text": "15.14 Berechnung der Vorhersagewerte und Residuen für zusätzliche Plots\n\ndaten$vorhersage &lt;- predict(model) \ndaten$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:"
  },
  {
    "objectID": "Skript_7.2.html#beobachtete-und-vorhergesagte-werte-sowie-residuen-gemeinsam-plotten",
    "href": "Skript_7.2.html#beobachtete-und-vorhergesagte-werte-sowie-residuen-gemeinsam-plotten",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.15 Beobachtete und vorhergesagte Werte sowie Residuen gemeinsam plotten",
    "text": "15.15 Beobachtete und vorhergesagte Werte sowie Residuen gemeinsam plotten\n\n# Note: Erzeugt einen Error, hab es fürs rendern erstmal rausgenommen (Patrick)\n\n#ggplot(daten, aes(Alter, TV_Konsum)) + \n#  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n#  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n#  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n#  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n#  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n#  geom_segment(aes(xend = Alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n#  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Fernsehnutzung\") + # Titel\n#  xlab(\"Alter\") + ylab(\"tägliche Fernsehnutzung (Minuten)\") # Achsen-Beschriftung\n\n\nvorhersagen &lt;- predict(model)\nresiduen &lt;- residuals(model)\n\n# Residualplot erstellen\nplot(vorhersagen, residuen, \n     xlab = \"Vorhersagen\", ylab = \"Residuen\",\n     main = \"Residualplot\", pch = 16, col = \"blue\")\nabline(h = 0, col = \"red\", lty = 2)  # Hinzufügen einer Linie bei y = 0 für Referenz"
  },
  {
    "objectID": "Skript_7.2.html#standardisierung-der-b-koeffizienten-beta-koeffizienten",
    "href": "Skript_7.2.html#standardisierung-der-b-koeffizienten-beta-koeffizienten",
    "title": "Korrelation & Regression",
    "section": "2.9 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)",
    "text": "2.9 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438           NA     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928       0.2963     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\nExkurs: Zusatzfunktionen zur schöneren Ergebnisdarstellung\n\n\n\n\n\nFunktion zur Modellzusammenfassung als data frame\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    64.6        NA         5.69       11.4 1.49e- 29\n2 Alter           2.19        0.296     0.101      21.8 1.93e-100\n\n\nFunktion, um weitere Modellstatistiken in einem Befehl zu berechnen\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0878        0.0876  123.      474. 1.93e-100     1 -30693. 61393. 61412.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nFunktion, um Rohdaten um Modellvorhersagen zu erweitern\n\naugment(model)\n\n# A tibble: 4,929 × 8\n   TV_Konsum Alter     .fitted  .resid     .hat .sigma      .cooksd .std.resid\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 210       54           183.   26.9  0.000203   123. 0.00000491       0.220 \n 2  90       53           181.  -90.9  0.000203   123. 0.0000559       -0.742 \n 3 135       89           260. -125.   0.00104    123. 0.000541        -1.02  \n 4  60       79           238. -178.   0.000633   123. 0.000668        -1.45  \n 5 180       62           201.  -20.6  0.000249   123. 0.00000351      -0.168 \n 6  45       23           115.  -70.1  0.000842   123. 0.000138        -0.572 \n 7  30       31           133. -103.   0.000553   123. 0.000194        -0.838 \n 8 180       68           214.  -33.8  0.000340   123. 0.0000129       -0.276 \n 9 180       51           176.    3.52 0.000208   123. 0.0000000861     0.0288\n10 120       57           190.  -69.6  0.000210   123. 0.0000339       -0.568 \n# ℹ 4,919 more rows\n\n\n\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#und-zum-abschluss-noch-ein-paar-nützliche-zusatzfunktionen-zur-schöneren-ergebnisdarstellung-durch-das-paket-broom",
    "href": "Skript_7.2.html#und-zum-abschluss-noch-ein-paar-nützliche-zusatzfunktionen-zur-schöneren-ergebnisdarstellung-durch-das-paket-broom",
    "title": "Korrelation & Regression",
    "section": "3.6 Und zum Abschluss noch ein paar nützliche Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom:",
    "text": "3.6 Und zum Abschluss noch ein paar nützliche Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom:\n\n3.6.1 Funktion zur Modellzusammenfassung als data frame\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    64.6        NA         5.69       11.4 1.49e- 29\n2 Alter           2.19        0.296     0.101      21.8 1.93e-100\n\n\n\n\n3.6.2 Funktion, um weitere Modellstatistiken in einem Befehl zu berechnen\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0878        0.0876  123.      474. 1.93e-100     1 -30693. 61393. 61412.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n3.6.3 Funktion, um Rohdaten um Modellvorhersagen zu erweitern\n\naugment(model)\n\n# A tibble: 4,929 × 8\n   TV_Konsum Alter     .fitted  .resid     .hat .sigma      .cooksd .std.resid\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 210       54           183.   26.9  0.000203   123. 0.00000491       0.220 \n 2  90       53           181.  -90.9  0.000203   123. 0.0000559       -0.742 \n 3 135       89           260. -125.   0.00104    123. 0.000541        -1.02  \n 4  60       79           238. -178.   0.000633   123. 0.000668        -1.45  \n 5 180       62           201.  -20.6  0.000249   123. 0.00000351      -0.168 \n 6  45       23           115.  -70.1  0.000842   123. 0.000138        -0.572 \n 7  30       31           133. -103.   0.000553   123. 0.000194        -0.838 \n 8 180       68           214.  -33.8  0.000340   123. 0.0000129       -0.276 \n 9 180       51           176.    3.52 0.000208   123. 0.0000000861     0.0288\n10 120       57           190.  -69.6  0.000210   123. 0.0000339       -0.568 \n# ℹ 4,919 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#funktion-um-weitere-modellstatistiken-in-einem-befehl-zu-berechnen",
    "href": "Skript_7.2.html#funktion-um-weitere-modellstatistiken-in-einem-befehl-zu-berechnen",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.18 Funktion, um weitere Modellstatistiken in einem Befehl zu berechnen",
    "text": "15.18 Funktion, um weitere Modellstatistiken in einem Befehl zu berechnen\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0878        0.0876  123.      474. 1.93e-100     1 -30693. 61393. 61412.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.2.html#funktion-um-rohdaten-um-modellvorhersagen-zu-erweitern",
    "href": "Skript_7.2.html#funktion-um-rohdaten-um-modellvorhersagen-zu-erweitern",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "15.19 Funktion, um Rohdaten um Modellvorhersagen zu erweitern",
    "text": "15.19 Funktion, um Rohdaten um Modellvorhersagen zu erweitern\n\naugment(model)\n\n# A tibble: 4,929 × 8\n   TV_Konsum Alter     .fitted  .resid     .hat .sigma      .cooksd .std.resid\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 210       54           183.   26.9  0.000203   123. 0.00000491       0.220 \n 2  90       53           181.  -90.9  0.000203   123. 0.0000559       -0.742 \n 3 135       89           260. -125.   0.00104    123. 0.000541        -1.02  \n 4  60       79           238. -178.   0.000633   123. 0.000668        -1.45  \n 5 180       62           201.  -20.6  0.000249   123. 0.00000351      -0.168 \n 6  45       23           115.  -70.1  0.000842   123. 0.000138        -0.572 \n 7  30       31           133. -103.   0.000553   123. 0.000194        -0.838 \n 8 180       68           214.  -33.8  0.000340   123. 0.0000129       -0.276 \n 9 180       51           176.    3.52 0.000208   123. 0.0000000861     0.0288\n10 120       57           190.  -69.6  0.000210   123. 0.0000339       -0.568 \n# ℹ 4,919 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.3.html#laden-der-notwendigen-pakete-und-der-daten",
    "href": "Skript_7.3.html#laden-der-notwendigen-pakete-und-der-daten",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "0.1 Laden der notwendigen Pakete und der Daten",
    "text": "0.1 Laden der notwendigen Pakete und der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom, um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ALLBUS-Datensatzes, die wir im letzten Schritt laden.\n\n#Laden der notwendigen Pakete\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLoading required package: pacman\n\np_load(tidyverse, lm.beta, lmtest, performance, easystats, haven, broom, see, haven, sandwich)\ntheme_set(theme_classic())\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999)"
  },
  {
    "objectID": "Skript_7.3.html#erinnerung-einfache-lineare-regression-mit-alter-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "href": "Skript_7.3.html#erinnerung-einfache-lineare-regression-mit-alter-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "0.2 Erinnerung: Einfache lineare Regression mit Alter als UV und TV-Nutzung als AV mit lm (=linear models)",
    "text": "0.2 Erinnerung: Einfache lineare Regression mit Alter als UV und TV-Nutzung als AV mit lm (=linear models)"
  },
  {
    "objectID": "Skript_7.3.html#erinnerung-voraussetzungen-der-einfachen-linearen-regression",
    "href": "Skript_7.3.html#erinnerung-voraussetzungen-der-einfachen-linearen-regression",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.1 Erinnerung: Voraussetzungen der einfachen linearen Regression:",
    "text": "2.1 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen-1-und-2-metrisches-skalenniveau-linearität-des-zusammenhangs",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen-1-und-2-metrisches-skalenniveau-linearität-des-zusammenhangs",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.2 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs",
    "text": "2.2 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt. Hier können Sie noch einmal nachschauen, wie das ging:\nLink LINK EINFÜGEN\n\n2.2.0.1 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß - unabhängig davon, wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-Package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde. Wir müssen diese Funktion lediglich auf unser geschätzes Regressionsmodell anwenden:\n\ncheck_heteroscedasticity(model)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift - wie im vorliegenden Fall - ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). Dann liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen. Es ist dann auch unbedingt geboten, diese Limitation anzugeben.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))"
  },
  {
    "objectID": "Skript_7.3.html#interpretation-was-sehen-wir-im-plot",
    "href": "Skript_7.3.html#interpretation-was-sehen-wir-im-plot",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "4.1 Interpretation: Was sehen wir im Plot?",
    "text": "4.1 Interpretation: Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei mittleren Werten erkennbar ist, weil wir einen leicht zur Mitte geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\nIm Beispiel sehen wir aber auch, dass die Abweichung bei uns zwar vorhanden, aber nicht allzu “dramatisch” ist. Die Voraussetzung ist zwar verletzt, und das müssen wir auch berücksichtgen, aber die Regressionsanalyse ist sehr robust gegen die Verletzung ihrer Voraussetzungen. Wir können den Fehler zudem korrigieren! Und das werden wir nun auch tun :)"
  },
  {
    "objectID": "Skript_7.3.html#was-tun-bei-heteroskedastizität-der-residuen-berechnung-von-hc-standard-errors",
    "href": "Skript_7.3.html#was-tun-bei-heteroskedastizität-der-residuen-berechnung-von-hc-standard-errors",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "4.2 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!",
    "text": "4.2 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value              Pr(&gt;|t|)    \n(Intercept) 64.64384    5.29202  12.215 &lt; 0.00000000000000022 ***\nAlter        2.19279    0.10197  21.504 &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen-3-unabhängigkeit-der-residuen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen-3-unabhängigkeit-der-residuen",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen",
    "text": "2.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.582).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0.588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen-4-normalverteilung-der-residuen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen-4-normalverteilung-der-residuen",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen",
    "text": "2.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist."
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen-5-ausreißer-im-modell",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen-5-ausreißer-im-modell",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell",
    "text": "2.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer in den Daten sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, können wir wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können."
  },
  {
    "objectID": "Skript_7.3.html#add-on-visuelle-inspektion-der-modellgüte-bzw.-der-modellannahmen",
    "href": "Skript_7.3.html#add-on-visuelle-inspektion-der-modellgüte-bzw.-der-modellannahmen",
    "title": "Prüfung der Voraussetzungn bei der linearen Regression",
    "section": "2.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen",
    "text": "2.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr gehaltvolle Funktion, die uns eine visuelle Inspektion der Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion können wir uns dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\nHier der Link zur Dokumentation des Performance-Packages mit weiteren Informationen: Link\n\n#check_model(model)\n#model_performance(model)"
  },
  {
    "objectID": "Skript_7.2.html#die-korrelationsanalyse-analyselogik-ziel-und-einsatzgebiete",
    "href": "Skript_7.2.html#die-korrelationsanalyse-analyselogik-ziel-und-einsatzgebiete",
    "title": "Teil 2: Korrelation & Regression",
    "section": "",
    "text": "Die Korrelationsanalyse ist eine schnelle und einfache statistische Methode, um den Zusammenhang zwischen zwei oder mehr metrischen Variablen zu untersuchen. Ziel der Korrelationsanalyse ist, die Richtung (positiv oder negativ) und die Stärke eines linearen Zusammenhangs zwischen den Variablen zu bestimmen. Als statistischer Kennwert wird ein Korrelationskoeffizient, zum Beispiel der Pearson-Korrelationskoeffizient (auch Pearson’s r) ermittelt. Der Korrelationskoeffizient gibt an, wie sehr die Werte einer Variable mit den Werten einer anderen Variable zusammenhängen, d.h. korrelieren. Er nimmt Werte zwischen -1 und 1 an. Ein hoher Korrelationskoeffizient (nahe 1 oder -1) deutet auf einen starken Zusammenhang hin, während ein niedriger Koeffizient (nahe 0) auf einen schwachen oder keinen Zusammenhang hindeutet. Zudem deutet ein positiver Korrelationskoeffizient auf einen positiven Zusammenhang hin, während ein negativer Koeffizient auf einen negativen Zusammenhang verweist.\nDer Zusammenhang zwischen den untersuchten Variablen wird dabei analysiert, ohne dass eine Richtung spezifiziert wird - es muss also nicht die eine Variable als unabhängig, und die andere als abhängige Variable festgelegt werden. Die Korrelationsanalyse kann vielfältig eingesetzt werden. Sie ist allerdings etwas empfindlich gegenüber Ausreißern und setzt einen linearen Zusammenhang voraus.\nVorsicht! Lassen Sie sich nicht verwirren: Der Pearson-Korrelationskoeffizient und die Rangkorrelation nach Pearson sind unterschiedliche Kennwerte, die nur ähnlich klingen. Der Pearson-Korrelationskoeffizient misst den linearen Zusammenhang zwischen den tatsächlichen Messwerten, während die Rangkorrelation nach Pearson (auch als Spearman-Korrelationskoeffizient bezeichnet) den monotonen Zusammenhang zwischen den Rängen der Daten untersucht. Daher ist die Rangkorrelation besser geeignet, wenn der Zusammenhang nicht-linear ist oder wenn Ausreißer in den Daten vorhanden sind. Nun soll es aber um die Analyse eines linearen Zusammenhangs zwischen zwei metrischen Variablen gehen. Dazu widmen wir uns nun der Korrelationsanalyse mittels Pearson-Korrelationskoeffizient.\nMit dieser Methode wollen wir untersuchen, ob es einen Zusammenhang zwischen der metrischen Variable tägliche Fernsehnutzung in Minuten (lm02) sowie der quasi-metrischen Variable Vertrauen ins Fernsehen (pt09) gibt. Wir vermuten, dass es einen positiven Zusammenhang gibt: Wer ein höheres Vertrauen in das Fernsehen hat, schaut auch mehr fern.\nUm das zu prüfen, müssen wir zunächst wieder unsere Pakete und Allbus-Daten laden und aufbereiten. Los geht’s!\n\n\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, dplyr) \ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nDann erzeugen wir einen neuen Teildatensatz, benennen die benötigten Variablen in “TV_Konsum” und “TV_Vertrauen” um und selektieren die fehlende Werte (z.B. -9=Keine Angabe) mittels Filter-Funktion.\n\n\n\n\ndaten_neu &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(TV_Vertrauen= pt09)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(TV_Vertrauen, 1, 7))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\nkorrelation &lt;- daten_neu %&gt;% \n  summarize(correlation = cor(TV_Konsum, TV_Vertrauen, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.114\n\n\nSchauen wir uns den (sehr übersichtlichen) Output an: Wie erhalten einen Korrelationskoeffizient von 0,113903. Das bedeutet, dass zwischen den beiden Variablen TV_Konsum und TV_Vertrauen ein schwacher positiver linearer Zusammenhang besteht. Höhere Werte in einer Variable gehen also tendenziell mit höheren Werten in der anderen Variable einher. Jedoch ist die Korrelation relativ niedrig, was darauf hindeutet, dass der Zusammenhang zwischen den beiden Variablen nicht besonders stark ist.\nUm die Korrelation zwischen den Variablen “TV_Konsum” und “TV_Vertrauen” zu visualisieren, können wir mit dem ggplot2-Paket ein Scatterplot erstellen:\n\n\n\n\nscatterplot &lt;- ggplot(daten_neu, aes(x = TV_Konsum, y = TV_Vertrauen)) +\n  geom_point(color = \"blue\") +\n  labs(x = \"TV Konsum\", y = \"TV Vertrauen\", title = \"Korrelation zwischen TV Konsum und Vertrauen\")\nprint(scatterplot)\n\n\n\n\nHier können wir den oben bereits ermittelten Befund noch einmal grafisch inspizieren. Der leichte positive Zusammenhang zwischen dem Vertrauen in die Insitution Fernsehen und der täglichen Fernsehnutzung zeigt sich ganz schön.\nFühren Sie zur Übung nun noch eine Korrelationsanalyse durch, um den Zusammenhang zwischen der täglichen Fernsehnutzung in Minuten (umbenannt in “TV_Konsum”) und der quasi-metrisch gemessenen Einschätzung, ob die Entwicklung der Kriminalität in Deutschland zu- oder abgenommen hat (cf03; 1=hat stark zugenommen; 5=hat stark abgenommen). Entsprechend der Kultivierungsforschung können wir vermuten, dass es einen negativen Zusammenhang geben sollte: Mehr Fernsehkonsum sollte mit einer negativen Kriminalitätsprognose korrelieren. Dazu müssen Sie zuerst wieder einen Teildatensatz mit den benötigten Variablen erzeugen, diese ggf. umbennen und filtern. Dann ermitteln Sie die Korrelation beider Variablen.\n\n\n\n\ndaten_neu2 &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(Kriminalitätsprognose= cf03)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(Kriminalitätsprognose, 1, 5))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\n\n\n\nkorrelation &lt;- daten_neu2 %&gt;% \n  summarize(correlation = cor(TV_Konsum, Kriminalitätsprognose, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1      -0.196\n\n\nBetrachten wir den Output: Der Korrelationskoeffizient von -0,196 deutet auf einen schwachen negativen linearen Zusammenhang zwischen der “täglichen Fernsehnutzung in Minuten” und der Einschätzung zur Entwicklung der Kriminalität in Deutschland hin.\nACHTUNG! Zur Interpretation müssen Sie nun aber auch berücksichtigen, wie die Werte der Variable gelabelt sind: Ein höherer Wert bedeutet, dass man eine Abnahme der Kriminalitätsentwicklung vermutet (1=hat stark zugenommen; 5=hat stark abgenommen). Der negative Korrelationskoeffizient bedeutet dann, dass höhere Werte in der “täglichen Fernsehnutzung in Minuten” tendenziell mit niedrigeren Werten in der Einschätzung zur Kriminalitätsentwicklung korrelieren. Oder anders gesagt: Personen, die mehr Fernsehen schauen, tendieren eher zur Annahme, dass die Kriminalität in Deutschland weniger zugenommen oder sogar abgenommen hat.\nSo oder so ist zu beachten, dass der Korrelationskoeffizient nur den linearen Zusammenhang erfasst und keine Aussagen zur Kausalitäten des Zusammenhangs macht. Korrelation ist nicht Kausalität!\nZur grafischen Illustration erstellen wir nun wieder ein Scatterplot:\n\n\n\n\nscatterplot &lt;- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalitätsprognose)) +\n  geom_point(color = \"red\") +\n  labs(x = \"TV Konsum\", y = \"Kriminalitätsprognose\", title = \"Korrelation zwischen TV Konsum und Kriminalitätsprognose\")\nprint(scatterplot)"
  },
  {
    "objectID": "Skript_7.2.html#die-analyse-von-zusammenhängen-bei-zwei-metrischen-variablen-mit-hilfe-der-korrelationsanalyse",
    "href": "Skript_7.2.html#die-analyse-von-zusammenhängen-bei-zwei-metrischen-variablen-mit-hilfe-der-korrelationsanalyse",
    "title": "Einführung in die Analyse von Zusammenhängen zwischen Variablen - Teil 2: Korrelation & Regression",
    "section": "",
    "text": "Mit dieser Methode wollen wir untersuchen, ob es einen Zusammenhang zwischen der metrischen Variable tägliche Fernsehnutzung in Minuten (lm02) sowie der quasi-metrischen Variable Vertrauen ins Fernsehen (pt09) gibt. Wir vermuten, dass es einen positiven Zusammenhang gibt: Wer ein höheres Vertrauen in das Fernsehen hat, schaut auch mehr fern.\nUm das zu prüfen, müssen wir zunächst wieder unsere Pakete und Allbus-Daten laden und aufbereiten. Los geht’s!\n\n\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, dplyr) \ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nDann erzeugen wir einen neuen Teildatensatz, benennen die benötigten Variablen in “TV_Konsum” und “TV_Vertrauen” um und selektieren die fehlende Werte (z.B. -9=Keine Angabe) mittels Filter-Funktion.\n\n\n\n\ndaten_neu &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(TV_Vertrauen= pt09)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(TV_Vertrauen, 1, 7))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\nkorrelation &lt;- daten_neu %&gt;% \n  summarize(correlation = cor(TV_Konsum, TV_Vertrauen, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.114\n\n\nSchauen wir uns den (sehr übersichtlichen) Output an: Wie erhalten einen Korrelationskoeffizient von 0,113903. Das bedeutet, dass zwischen den beiden Variablen TV_Konsum und TV_Vertrauen ein schwacher positiver linearer Zusammenhang besteht. Höhere Werte in einer Variable gehen also tendenziell mit höheren Werten in der anderen Variable einher. Jedoch ist die Korrelation relativ niedrig, was darauf hindeutet, dass der Zusammenhang zwischen den beiden Variablen nicht besonders stark ist.\nUm die Korrelation zwischen den Variablen “TV_Konsum” und “TV_Vertrauen” zu visualisieren, können wir mit dem ggplot2-Paket ein Scatterplot erstellen:\n\n\n\n\nscatterplot &lt;- ggplot(daten_neu, aes(x = TV_Konsum, y = TV_Vertrauen)) +\n  geom_point(color = \"blue\") +\n  labs(x = \"TV Konsum\", y = \"TV Vertrauen\", title = \"Korrelation zwischen TV Konsum und Vertrauen\")\nprint(scatterplot)\n\n\n\n\nHier können wir den oben bereits ermittelten Befund noch einmal grafisch inspizieren. Der leichte positive Zusammenhang zwischen dem Vertrauen in die Insitution Fernsehen und der täglichen Fernsehnutzung zeigt sich ganz schön.\nFühren Sie zur Übung nun noch eine Korrelationsanalyse durch, um den Zusammenhang zwischen der täglichen Fernsehnutzung in Minuten (umbenannt in “TV_Konsum”) und der quasi-metrisch gemessenen Einschätzung, ob die Entwicklung der Kriminalität in Deutschland zu- oder abgenommen hat (cf03; 1=hat stark zugenommen; 5=hat stark abgenommen). Entsprechend der Kultivierungsforschung können wir vermuten, dass es einen negativen Zusammenhang geben sollte: Mehr Fernsehkonsum sollte mit einer negativen Kriminalitätsprognose korrelieren. Dazu müssen Sie zuerst wieder einen Teildatensatz mit den benötigten Variablen erzeugen, diese ggf. umbennen und filtern. Dann ermitteln Sie die Korrelation beider Variablen.\n\n\n\n\ndaten_neu2 &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(Kriminalitätsprognose= cf03)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(Kriminalitätsprognose, 1, 5))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\n\n\n\nkorrelation &lt;- daten_neu2 %&gt;% \n  summarize(correlation = cor(TV_Konsum, Kriminalitätsprognose, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1      -0.196\n\n\nBetrachten wir den Output: Der Korrelationskoeffizient von -0,196 deutet auf einen schwachen negativen linearen Zusammenhang zwischen der “täglichen Fernsehnutzung in Minuten” und der Einschätzung zur Entwicklung der Kriminalität in Deutschland hin.\nACHTUNG! Zur Interpretation müssen Sie nun aber auch berücksichtigen, wie die Werte der Variable gelabelt sind: Ein höherer Wert bedeutet, dass man eine Abnahme der Kriminalitätsentwicklung vermutet (1=hat stark zugenommen; 5=hat stark abgenommen). Der negative Korrelationskoeffizient bedeutet dann, dass höhere Werte in der “täglichen Fernsehnutzung in Minuten” tendenziell mit niedrigeren Werten in der Einschätzung zur Kriminalitätsentwicklung korrelieren. Oder anders gesagt: Personen, die mehr Fernsehen schauen, tendieren eher zur Annahme, dass die Kriminalität in Deutschland weniger zugenommen oder sogar abgenommen hat.\nSo oder so ist zu beachten, dass der Korrelationskoeffizient nur den linearen Zusammenhang erfasst und keine Aussagen zur Kausalitäten des Zusammenhangs macht. Korrelation ist nicht Kausalität!\nZur grafischen Illustration erstellen wir nun wieder ein Scatterplot:\n\n\n\n\nscatterplot &lt;- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalitätsprognose)) +\n  geom_point(color = \"red\") +\n  labs(x = \"TV Konsum\", y = \"Kriminalitätsprognose\", title = \"Korrelation zwischen TV Konsum und Kriminalitätsprognose\")\nprint(scatterplot)"
  },
  {
    "objectID": "Skript_7.2.html#die-regressionsanalyse-analyselogik-ziel-und-einsatzgebiete",
    "href": "Skript_7.2.html#die-regressionsanalyse-analyselogik-ziel-und-einsatzgebiete",
    "title": "Teil 2: Korrelation & Regression",
    "section": "",
    "text": "Die lineare Regression untersucht den Zusammenhang zwischen einer abhängigen Variable und mindestens einer unabhängigen Variable. Sie versucht, eine mathematische Beziehung zwischen den Variablen zu modellieren, die durch eine Linie (in einfachen linearen Regressionen) oder eine Ebene (in multiplen linearen Regressionen) repräsentiert wird. Die dahinterliegende “mathematische Idee” der linearen Regression besteht darin, die bestmögliche Anpassung der Daten an das Modell zu erreichen, um die Vorhersage der abhängigen Variable basierend auf den unabhängigen Variablen zu ermöglichen. Dazu wird ein Regressionskoeffizient für jede unabhängige Variable geschätzt, um ihren Einfluss auf die abhängige Variable zu quantifizieren.\nZiel der Regressionsanalyse ist es also, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?"
  },
  {
    "objectID": "Skript_7.2.html#die-einfache-lineare-regression",
    "href": "Skript_7.2.html#die-einfache-lineare-regression",
    "title": "Teil 2: Korrelation & Regression",
    "section": "",
    "text": "Die einfache lineare Regression wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen einer abhängigen metrischen Variable und einer unabhängigen metrischen Variable besteht. Da sie zwei metrische Variablen integriert, wird sie auch als bivariate Regression bezeichnet. In diesem Teilkapitel lernen wir die einfache lineare Regression auf Grundlage der Allbus-Daten kennen. In der nächsten Sitzung gehen wir dann näher auf die notwendige Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen auch noch die multiple lineare Regression kennen.\nUm die Durchführung der linearen Regression an einem Beispiel nachzuvollziehen, stellen wir zunächst eine gerichtete Hypothese auf, die den Einfluss einer unabhängigen metrischen Variable auf eine abhängige metrische Variable spezifiziert. Hierzu schauen wir uns den Zusammenhang zwischen dem Alter (age) sowie der täglichen Fernsehnutzung in Minuten (lm02) an. Dazu vermuten wir: Das Alter erklärt die Intensität der täglichen Fernsehnutzung in Minuten.\n\n\n\nGrafik: Kann das Alter die Fernsehnutzung erklären? (Picture generated by midjourney)\n\n\n\n\nWie immer besteht der erste Schritt nun darin, die benötigten Pakete sowie den Datensatz zu laden. Für die lineare Regression kommen zwei neue Pakete hinzu: Wir laden das Paket broom, um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können sowie das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt. Das Paket “see” kommt außerdem dazu, weil es uns eine toolbox für die Visualisierung der Zusammenhänge bereitstellt.\n\n\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, dplyr, broom, lm.beta, performance, see) \n\n\n\n\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n\n\nFür den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(Alter = age)%&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(TV_Konsum, 0, 1500))\n\n\n1theme_set(theme_minimal())\n\n2options(scipen = 999)\n\n\n1\n\nVisualisierungshintergrund der Grafiken in ggplot festlegen\n\n2\n\nAnzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\n\n\n\n\n\n\n\n\nMit Hilfe der Regression wollen wir nun die oben formulierte Annahme prüfen, dass das Alter die täglichen Fernsehnutzung in Minuten (umbenannt in TV_Konsum) erklären kann. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse grundsätzlich eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden, dazu aber später mehr).\nBevor wir die Regressionsanalyse durchführen, verschaffen wir uns zunächst einmal einen Überblick über die Daten - dazu visualisieren wir den Zusammenhang zwischen dem Alter sowie der täglichen Fernsehnutzung. Mit Hilfe der Grafik können wir auch die erste Voraussetzung prüfen, nämlich dass es einen linearen Zusammenhang zwischen beiden Variablen gibt.\n\n\n\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und TV_Konsum. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Alter) und x (=TV_Konsum) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\n\n\nggplot(daten, aes(Alter, TV_Konsum)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und TV_Konsum\") + \n  xlab(\"TV_Konsum\") + ylab(\"Alter\")\n\n\n\n\n\n\n\nDie grafische Darstellung legt uns einen positiven und linearen Zusammenhang zwischen Alter und Fernsehnutzung nahe: mit zunehmendem Alter steigt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\n\n\n\n\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regression prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: TV_Konsum), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (TV_Konsum) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll. Zum Schluss lassen wir uns das Modell ausgeben.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n\nmodel &lt;- lm(TV_Konsum ~ Alter, data = daten) \nprint(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nCoefficients:\n(Intercept)        Alter  \n     64.644        2.193  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn nun mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten” die abhängige Variable “TV_Konsum” durch die unabhängige Variable “Alter” zu erklären.\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nDas Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\nMit St.error wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (8,8) Prozent der Varianz der TV-Nutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Intensität der Fernsehnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (das kommt in der Realität aber fast nicht vor).\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\nDer Output zeigt uns: Das Alter hat einen positiven Einfluss auf die tägliche Fernsehnutzung in Minuten. Je älter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Konsum um 2,19 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Das Alter beeinflusst die Dauer der Fernsehnutzung, R2 = .08, F(1, 4927) = 474.1, p = .000.\n\n\n\n\n\n\nDa bei der Regression eine lineare Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (TV_Konsum) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\n\n\nWir lassen uns mit Hilfe unseres Modells zunächst die tägliche Internetnutzung in Minuten bei einem Alter von 25 und 75 vorhersagen.\n\npredict.lm(model, data.frame(Alter = 25))\n\n       1 \n119.4635 \n\npredict.lm(model, data.frame(Alter = 75))\n\n       1 \n229.1028 \n\n\n\n\n\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von 119 Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von 229 Minuten auf.\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\n\n\n\nIn unserem zweiten Beispiel betrachten wir andere Altersgruppen. Wir lassen uns hier mit Hilfe unseres Modells die tägliche Internetnutzung in Minuten bei einem Alter von 20, 30, 40, 50, 60 und 70 vorhersagen.\n\npredict.lm(model, data.frame(Alter = c(20, 30, 40, 50, 60, 70)))\n\n       1        2        3        4        5        6 \n108.4996 130.4274 152.3553 174.2832 196.2110 218.1389 \n\n\n\n\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable TV-Konsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen TV-Konsum von (X) Minuten.\nWeil wir im Datensatz aber n=4929 Fälle haben (und die Ausgabe sonst zu unübersichtlich wird), begrenzen ich die Ausgabe auf die ersten 100 Zeilen (Fälle) des Datensatzes, indem ich zusätzlich die Funktion head() verwende, und die Anzahl der gewünschten Fälle in der Klammer mit 100 festlege:\n\n\n\n\nhead(fitted(model), 100)\n\n       1        2        3        4        5        6        7        8 \n183.0543 180.8615 259.8019 237.8740 200.5966 115.0779 132.6202 213.7533 \n       9       10       11       12       13       14       15       16 \n176.4760 189.6327 251.0307 121.6563 147.9697 183.0543 163.3192 172.0904 \n      17       18       19       20       21       22       23       24 \n121.6563 246.6451 169.8976 169.8976 224.7173 200.5966 119.4635 183.0543 \n      25       26       27       28       29       30       31       32 \n183.0543 176.4760 196.2110 172.0904 189.6327 191.8255 191.8255 150.1625 \n      33       34       35       36       37       38       39       40 \n244.4523 233.4884 237.8740 112.8851 233.4884 183.0543 174.2832 119.4635 \n      41       42       43       44       45       46       47       48 \n207.1750 187.4399 222.5245 178.6687 213.7533 185.2471 132.6202 237.8740 \n      49       50       51       52       53       54       55       56 \n211.5605 209.3678 115.0779 246.6451 154.5481 189.6327 112.8851 147.9697 \n      57       58       59       60       61       62       63       64 \n215.9461 200.5966 169.8976 204.9822 121.6563 224.7173 172.0904 147.9697 \n      65       66       67       68       69       70       71       72 \n152.3553 174.2832 189.6327 156.7409 185.2471 132.6202 185.2471 213.7533 \n      73       74       75       76       77       78       79       80 \n264.1874 202.7894 187.4399 233.4884 187.4399 191.8255 196.2110 194.0183 \n      81       82       83       84       85       86       87       88 \n176.4760 200.5966 194.0183 207.1750 174.2832 143.5842 119.4635 161.1265 \n      89       90       91       92       93       94       95       96 \n165.5120 161.1265 174.2832 150.1625 172.0904 110.6924 196.2110 233.4884 \n      97       98       99      100 \n110.6924 196.2110 185.2471 178.6687 \n\n\n\n\n\nIn der Ausgabe kann ich nun die laut Modell prognostizierte Fernsehnutzung für meine Befragten entsprechend ihres Alters sehen - Befragte(r) 10 hat so z.B. eine prognostizierte Fernsehnutzung von 189 Minuten.\nNun haben wir im Rahmen unserer Befragung die TV-Nutzung der Befragten aber ja schon erhoben. Wozu dient diese Prognose dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nhead(residuals(model), 100)\n\n&lt;labelled&lt;double&gt;[100]&gt;: FERNSEHGESAMTDAUER PRO TAG IN MINUTEN\n           1            2            3            4            5            6 \n  26.9456805  -90.8615329 -124.8018509 -177.8739847  -20.5966124  -70.0779345 \n           7            8            9           10           11           12 \n-102.6202274  -33.7533320    3.5240404  -69.6326793  198.9692956 -111.6562944 \n          13           14           15           16           17           18 \n -27.9697337   56.9456805   46.6807600  -82.0903864  -31.6562944 -126.6451312 \n          19           20           21           22           23           24 \n -79.8975998  -19.8975998   75.2827349 1059.4033876    0.5364922   56.9456805 \n          25           26           27           28           29           30 \n  -3.0543195  -26.4759596  -76.2110391    7.9096136  -39.6326793  108.1745341 \n          31           32           33           34           35           36 \n -71.8254659  -90.1625203 -124.4523446    6.5115885 -117.8739847  426.1148521 \n          37           38           39           40           41           42 \n -83.4884115  146.9456805  125.7168270  -29.4635078  -17.1749722   82.5601073 \n          43           44           45           46           47           48 \n -12.5244785  -28.6687462 -138.7533320  -65.2471061  -12.6202274  -27.8739847 \n          49           50           51           52           53           54 \n   8.4394546  150.6322412   94.9220655  -66.6451312  -34.5480935  -39.6326793 \n          55           56           57           58           59           60 \n -52.8851479 -132.9697337  -65.9461186  -20.5966124  -49.8975998  -24.9821856 \n          61           62           63           64           65           66 \n -31.6562944  -14.7172651    7.9096136    2.0302663  -92.3553069    5.7168270 \n          67           68           69           70           71           72 \n -69.6326793  -36.7408801   -5.2471061  -72.6202274   84.7528939  -93.7533320 \n          73           74           75           76           77           78 \n-174.1874241  -82.7893990  126.5601073    6.5115885   52.5601073  -71.8254659 \n          79           80           81           82           83           84 \n  13.7889609  -44.0182525  -26.4759596  -20.5966124   75.9817475  -87.1749722 \n          85           86           87           88           89           90 \n   5.7168270    6.4158395  -89.4635078   18.8735466  -45.5120266    3.8735466 \n          91           92           93           94           95           96 \n   5.7168270  149.8374797  -67.0903864  -80.6923613   68.7889609  246.5115885 \n          97           98           99          100 \n -80.6923613  -16.2110391  -35.2471061  -58.6687462 \n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n   -10       TNZ: FILTER\n    -9      KEINE ANGABE\n\n\n\n\n\nFür unseren Fall Nummer 65 beträgt die Abweichung der Prognose von der Beobachtung -92 Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 8 Prozent nicht besonders groß ist.\n\n\n\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\n\n\n\ndaten$vorhersage &lt;- predict(model) \ndaten$residuen &lt;- as.numeric(residuals(model))\n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\n\n\n\nggplot(daten, aes(Alter, TV_Konsum)) + \n1  geom_point(aes(color = residuen)) +\n2  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n3  guides(color = \"none\") +\n4  geom_point(aes(y = vorhersage), shape = 1) +\n5  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") +\n6  geom_segment(aes(xend = Alter, yend = vorhersage), alpha = .2) +\n7  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Fernsehnutzung\") +\n8  xlab(\"Alter\") + ylab(\"tägliche Fernsehnutzung (Minuten)\")\n\n\n1\n\nFestlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n\n2\n\nFestlegung der Farbe für die Residuen\n\n3\n\nUnterdrückt eine Legende an der Seite (ist obligatorisch)\n\n4\n\ngibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n\n5\n\ngibt die Regressionsgerade als Linie aus\n\n6\n\nzeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein\n\n7\n\nTitel\n\n8\n\nAchsen-Beschriftung\n\n\n\n\n\n\n\n\nvorhersagen &lt;- predict(model)\nresiduen &lt;- residuals(model)\n\n1plot(vorhersagen, residuen,\n     xlab = \"Vorhersagen\", ylab = \"Residuen\",\n     main = \"Residualplot\", pch = 16, col = \"blue\")\n2abline(h = 0, col = \"red\", lty = 2)\n\n\n1\n\nResidualplot erstellen\n\n2\n\nHinzufügen einer Linie bei y = 0 für Referenz\n\n\n\n\n\n\n\n\n\n\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438           NA     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928       0.2963     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n\n\n\n\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    64.6        NA         5.69       11.4 1.49e- 29\n2 Alter           2.19        0.296     0.101      21.8 1.93e-100\n\n\n\n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1    0.0878        0.0876  123.      474. 1.93e-100     1 -30693. 61393. 61412.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\n\naugment(model)\n\n# A tibble: 4,929 × 8\n   TV_Konsum Alter     .fitted  .resid     .hat .sigma      .cooksd .std.resid\n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1 210       54           183.   26.9  0.000203   123. 0.00000491       0.220 \n 2  90       53           181.  -90.9  0.000203   123. 0.0000559       -0.742 \n 3 135       89           260. -125.   0.00104    123. 0.000541        -1.02  \n 4  60       79           238. -178.   0.000633   123. 0.000668        -1.45  \n 5 180       62           201.  -20.6  0.000249   123. 0.00000351      -0.168 \n 6  45       23           115.  -70.1  0.000842   123. 0.000138        -0.572 \n 7  30       31           133. -103.   0.000553   123. 0.000194        -0.838 \n 8 180       68           214.  -33.8  0.000340   123. 0.0000129       -0.276 \n 9 180       51           176.    3.52 0.000208   123. 0.0000000861     0.0288\n10 120       57           190.  -69.6  0.000210   123. 0.0000339       -0.568 \n# ℹ 4,919 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.4.html",
    "href": "Skript_7.4.html",
    "title": "Die multiple Regression",
    "section": "",
    "text": "Eine Halle voller Maschinen, Bild generiert von Midjourney\nIn diesem Teilkapitel lernen wir nun - wie angekündigt - die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren."
  },
  {
    "objectID": "Skript_7.4.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.4.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Teilkapitel lernen wir nun - wie angekündigt - die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren."
  },
  {
    "objectID": "Skript_7.4.html#vorbereitung-und-laden-der-daten",
    "href": "Skript_7.4.html#vorbereitung-und-laden-der-daten",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "2 Vorbereitung und Laden der Daten",
    "text": "2 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis Allbus-Datensatzes, den wir entsprechend einlesen:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, lm.beta, lmtest, performance, easystats, haven, broom, see, haven, sandwich)\n\nInstalliere Paket nach 'C:/Users/Patrick Zerrer/AppData/Local/R/win-library/4.3'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeit 'report'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES' nicht öffnen\n\n\nPaket 'report' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'easystats' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Patrick Zerrer\\AppData\\Local\\Temp\\Rtmpe8EHBL\\downloaded_packages\n\n\n\neasystats installed\n\n\nWarning: Paket 'easystats' wurde unter R Version 4.3.1 erstellt\n\ntheme_set(theme_classic())\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\ntheme_set(theme_minimal())\noptions(scipen = 999)"
  },
  {
    "objectID": "Skript_7.4.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen",
    "href": "Skript_7.4.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "3 Erzeugen eines Teildatensatzes & Umbenennen der Variablen",
    "text": "3 Erzeugen eines Teildatensatzes & Umbenennen der Variablen\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die TV-Nutzung (lm02); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Variablen Bildung (educ) und Vertrauen ins Fernsehen (pt09). Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl zunächst wieder um. Außerdem filtern wir auch wieder die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(TV_Vertrauen= pt09)%&gt;%\n  rename(Alter = age)%&gt;%\n  rename(Bildung = educ)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(TV_Vertrauen, 1, 7))%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(Bildung, 1, 5))"
  },
  {
    "objectID": "Skript_7.4.html#erinnerung-einfache-lineare-regression-mit-alter-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "href": "Skript_7.4.html#erinnerung-einfache-lineare-regression-mit-alter-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "title": "Die multiple Regression",
    "section": "1 Erinnerung: Einfache lineare Regression mit Alter als UV und TV-Nutzung als AV mit lm (=linear models)",
    "text": "1 Erinnerung: Einfache lineare Regression mit Alter als UV und TV-Nutzung als AV mit lm (=linear models)\nAus den letzten Sitzungen wissen wir bereits, dass das Alter einen signifikanten, positiven Einfluss auf die TV-Nutzung hat. Wir wissen aber auch, dass die TV-Nutzung nicht alleine über das Alter erklärt werden kann (weil unser R2 recht gering war).\n\nmodel &lt;- lm(TV_Konsum ~ Alter, data = daten) \nsummary(lm.beta(model))\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-243.01  -67.15  -22.38   34.77 1294.77 \n\nCoefficients:\n            Estimate Standardized Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  59.5226           NA     7.2929   8.162 0.000000000000000474 ***\nAlter         2.2555       0.2970     0.1299  17.364 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 124.4 on 3117 degrees of freedom\nMultiple R-squared:  0.0882,    Adjusted R-squared:  0.0879 \nF-statistic: 301.5 on 1 and 3117 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Skript_7.4.html#multiple-lineare-regression-mit-alter-bildung-sowie-vertrauen-ins-fernsehen-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "href": "Skript_7.4.html#multiple-lineare-regression-mit-alter-bildung-sowie-vertrauen-ins-fernsehen-als-uv-und-tv-nutzung-als-av-mit-lm-linear-models",
    "title": "Die multiple Regression",
    "section": "2 Multiple lineare Regression mit Alter, Bildung sowie Vertrauen ins Fernsehen als UV und TV-Nutzung als AV mit lm (=linear models)",
    "text": "2 Multiple lineare Regression mit Alter, Bildung sowie Vertrauen ins Fernsehen als UV und TV-Nutzung als AV mit lm (=linear models)\nMit der multiplen linearen Regression wollen wir nun prüfen, welche weiteren unabhängigen Variablen die TV-Nutzung erklären könnten. Dazu bringen wir Vertrauen in das Fernsehen als möglicherweise zusätzlichen erklärenden Faktor in unser Regressionsmodell ein.\nAls Funktion können wir weiterhin lm nutzen; die zusätzlichen Variablen können wir in der Klammer ganz simpel ergänzen, indem wir sie mit einem “+” hinten anhängen:\n\nmodel2 &lt;- lm(TV_Konsum ~ Alter + Bildung + TV_Vertrauen, data = daten) \nsummary(lm.beta(model2))\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter + Bildung + TV_Vertrauen, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-253.27  -65.90  -19.06   33.03 1243.54 \n\nCoefficients:\n              Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  173.13362           NA   12.70427  13.628 &lt;0.0000000000000002 ***\nAlter          1.56297      0.20579    0.13805  11.322 &lt;0.0000000000000002 ***\nBildung      -26.27887     -0.23361    1.98888 -13.213 &lt;0.0000000000000002 ***\nTV_Vertrauen   5.25668      0.05248    1.71576   3.064              0.0022 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 120.9 on 3115 degrees of freedom\nMultiple R-squared:  0.1383,    Adjusted R-squared:  0.1375 \nF-statistic: 166.7 on 3 and 3115 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n2.1 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?\nDer Output zeigt uns: Das Alter hat weiterhin einen positiven Einfluss auf die tägliche Fernsehnutzung in Minuten. Je älter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Konsum um 2,08 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p=.000 statistisch höchst signifikant.\nAuch die zweite Variable, die wir ins Regressionsmodell eingebracht haben, das Vertrauen in das Fernsehen, hat einen signifikanten, positiven Effekt auf den TV-Konsum der Befragten: Mit jedem Skalenpunkt steigt der TV-Konsum um 5,1 Minuten an. Der Einfluss des TV-Vertrauens ist hoch signifikant.\nSchließlich hat auch der Bildungsabschluss einen höchst signifikanten Einfluss auf die Intensität der Fernsehnutzung - dieser ist allerdings negativ, so dass der TV-Konsum mit steigender Messeinheit des Bildungsniveaus sinkt. Um das zu inhaltlich interpretieren, ist es sinnvoll, sich die Skalierung der Variable genauer anzuschauen: Je höher der Wert, desto höher der Abschluss (z.B. Bildung: 1=ohne Abschluss; 3=Mittlere Reife; 5=Abitur). Wie unser Regressionsmodell zeigt, geht also ein höherer Abschluss mit einem niedrigeren TV-Konsum einher.\nIm Output sehen wir nun auch, dass sich unser R2 (im Vergleich zur einfachen Regression oben) deutlich verbessert hat - es liegt jetzt bei 13,8 Prozent Varianzaufklärung. Das ist schon ordentlich, aber es muss immer noch weitere Faktoren geben, die die TV-Nutzung substantiell erklären können. Welche das sein können, können Sie ja mal selbst ausprobieren. Fügen Sie dazu einfach weitere - theoretisch plausible! - Kandidaten in das Regressionsmodell ein, indem sie sie durch ein “+”-Zeichen in ihre Modellfunktion integrieren (Achtung, vorher müssen Sie die zusätzlichen Variablen natürlich in ihrem Datenobjekt definieren und ggf. aufbereiten).\n\n\n2.2 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie die Voraussetzungen der Regressionaanalyse prüfen, haben Sie ja in den letzten Teilkapiteln gelernt - hier können Sie das noch einmal nachlesen:\nLink\nZusätzlich müssen Sie bei einer multiplen Regresssion allerdings noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabhängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n         Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n        Alter 1.19 [1.15, 1.25]         1.09      0.84     [0.80, 0.87]\n      Bildung 1.13 [1.09, 1.18]         1.06      0.88     [0.85, 0.92]\n TV_Vertrauen 1.06 [1.03, 1.12]         1.03      0.94     [0.90, 0.97]\n\n\n\n2.2.1 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5 (sogar alle bei 1); wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”).\n\n\n\n2.3 Vorhersage des multivariaten Modells für die tägliche TV-Nutzung durch Alter, Bildung und Vertrauen in das Fernsehen\nAuch für die Kombination verschiedener Merkmale bzw. unabhängiger Variablen können wir uns über die predict.lm-Funktion Prognosen erstellen lassen. So können wir beispielsweise vergleichen, wie sich der Fernsehkonsum bestimmter “Idealtypen” von Befragten unterscheiden würde:\n\npredict.lm(model2, data.frame(Alter = c(25, 25), Bildung = c(0,5), TV_Vertrauen = c(7, 1)))\n\n        1         2 \n249.00453  86.07011 \n\n\n\n2.3.1 Inhaltliche Interpretation: Was bedeutet der Output?\nEine Person, die 25 Jahre alt ist und keinen Schulabschluss, aber dafür sehr großes Vertrauen in das Fernsehen hat, hat einen prognostizierten täglichen TV-Konsum von 249 Minuten. Eine Person, die ebenfalls 25 Jahre alt ist und Abitur hat, dem Fernsehen aber “überhaupt kein Vertrauen” entgegenbringt, hat einen täglichen prognostizierten Fernsehkonsum von 86 Minuten.\n\n\n\n2.4 Multiple Regression mit Dummy-Codierung der kategorialen Variable “sex”\nEine Besonderheit schauen wir uns nun noch zum Schluss an: Die lineare Regression ist ein Verfahren für metrische Daten. Sie untersucht den Zusammenhang zwischen einer metrischen abhängigen Variable und mindestens einer metrischen unabhängigen Variable. In den bisherigen Regressionsanalysen haben wir mit metrischen und quasi-metrischen Variablen gearbeitet, die die Voraussetzung erfüllen, dass eine Regression gerechnet werden kann. Das ist auch der übliche Fall. Es gibt aber die “Ausnahme”, dass auch kategorische (v.a. binäre) Variablen bei der Regressionsanalyse grundsätzlich eingesetzt werden können, wenn diese durch eine Dummy-Coding passend gemacht werden. Und diesen Fall schauen wir un nun am Beispiel der Variable “Geschlecht” an. Dazu müssen wir die Variable aber als binär codiert bzw. dichotom betrachten - d.h. wir behandeln sie so, als hätten wir hier nur 2 Ausprägungen.\n\n2.4.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Vertrauen ins TV, Geschlecht und TV-Konsum\nWir wollen das biologische Geschlecht (sex) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei “sex” haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist. Es handelt sich vielmehr um eine kategoriale Variable (mit 3 Ausprägungen). Dennoch können wir die Variable mit einem “Trick” in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Hier wollen uns nun mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl aber erst einmal umcodieren und dadurch in eine dichotome VAriable verwandeln.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n2.4.2 Dummy-Codierung der Variable sex\n\ndaten &lt;- daten %&gt;%\n filter(between(sex, 1, 2)) %&gt;%\n  mutate(sex_d = case_when(sex == 1 ~ 0, sex == 2 ~ 1))\n\n\n\n2.4.3 Prüfung der Umcodierung zur Dummy-Variablen\n\nhäufigkeitstabelle &lt;- table(daten$sex_d)\nprint(häufigkeitstabelle)\n\n\n   0    1 \n1546 1569 \n\n\nDie Dummy-Codierung war erfolgreich - wir haben nun 1546 Befragte mit der Ausprägung 0 (männlich) sowie 1569 Befragte mit der Ausprägung 2 (weiblich).\n\n\n2.4.4 Regressionsmodell zum Zusammenhang von Alter, Bildung, TV-Vertrauen, Geschlecht und TV-Konsum spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable sex_d ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel3 &lt;- lm(TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, data = daten) \nsummary(lm.beta(model3))\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, \n    data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-255.89  -64.84  -18.97   33.01 1245.99 \n\nCoefficients:\n              Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  170.54603           NA   12.96665  13.153 &lt;0.0000000000000002 ***\nAlter          1.56629      0.20625    0.13830  11.325 &lt;0.0000000000000002 ***\nBildung      -26.32891     -0.23397    1.99103 -13.224 &lt;0.0000000000000002 ***\nTV_Vertrauen   5.34102      0.05330    1.71809   3.109              0.0019 ** \nsex_d          4.60020      0.01766    4.34277   1.059              0.2896    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 121 on 3110 degrees of freedom\nMultiple R-squared:  0.1386,    Adjusted R-squared:  0.1375 \nF-statistic: 125.1 on 4 and 3110 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n\n2.4.5 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?\nGender hat keinen signifikanten Einfluss auf den TV-Konsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy-Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (4,6) Minuten höheren TV-Konsum als Männer (wobei dieser Befund statistisch ja nicht signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)"
  },
  {
    "objectID": "Skript_7.4.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-ergebnis",
    "href": "Skript_7.4.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-ergebnis",
    "title": "Die multiple Regression",
    "section": "2.1 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?",
    "text": "2.1 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?\nDer Output zeigt uns: Das Alter hat weiterhin einen positiven Einfluss auf die tägliche Fernsehnutzung in Minuten. Je älter ein Nutzer ist, desto mehr nutzt er das Fernsehen. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Konsum um 2,08 Messeinheiten (hier: Minuten) zu. Dieser Zusammenhang ist mit p=.000 statistisch höchst signifikant.\nAuch die zweite Variable, die wir ins Regressionsmodell eingebracht haben, das Vertrauen in das Fernsehen, hat einen signifikanten, positiven Effekt auf den TV-Konsum der Befragten: Mit jedem Skalenpunkt steigt der TV-Konsum um 5,1 Minuten an. Der Einfluss des TV-Vertrauens ist hoch signifikant.\nSchließlich hat auch der Bildungsabschluss einen höchst signifikanten Einfluss auf die Intensität der Fernsehnutzung - dieser ist allerdings negativ, so dass der TV-Konsum mit steigender Messeinheit des Bildungsniveaus sinkt. Um das zu inhaltlich interpretieren, ist es sinnvoll, sich die Skalierung der Variable genauer anzuschauen: Je höher der Wert, desto höher der Abschluss (z.B. Bildung: 1=ohne Abschluss; 3=Mittlere Reife; 5=Abitur). Wie unser Regressionsmodell zeigt, geht also ein höherer Abschluss mit einem niedrigeren TV-Konsum einher.\nIm Output sehen wir nun auch, dass sich unser R2 (im Vergleich zur einfachen Regression oben) deutlich verbessert hat - es liegt jetzt bei 13,8 Prozent Varianzaufklärung. Das ist schon ordentlich, aber es muss immer noch weitere Faktoren geben, die die TV-Nutzung substantiell erklären können. Welche das sein können, können Sie ja mal selbst ausprobieren. Fügen Sie dazu einfach weitere - theoretisch plausible! - Kandidaten in das Regressionsmodell ein, indem sie sie durch ein “+”-Zeichen in ihre Modellfunktion integrieren (Achtung, vorher müssen Sie die zusätzlichen Variablen natürlich in ihrem Datenobjekt definieren und ggf. aufbereiten)."
  },
  {
    "objectID": "Skript_7.4.html#zusätzliche-voraussetzungsprüfung-bei-der-multiplen-linearen-regression-liegt-multikollinearität-vor",
    "href": "Skript_7.4.html#zusätzliche-voraussetzungsprüfung-bei-der-multiplen-linearen-regression-liegt-multikollinearität-vor",
    "title": "Die multiple Regression",
    "section": "2.2 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?",
    "text": "2.2 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie die Voraussetzungen der Regressionaanalyse prüfen, haben Sie ja in den letzten Teilkapiteln gelernt - hier könnt ihr das noch einmal nachlesen.\nZusätzlich müsst ihr bei einer multiplen Regresssion allerdings noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabhängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n         Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n        Alter 1.19 [1.15, 1.25]         1.09      0.84     [0.80, 0.87]\n      Bildung 1.13 [1.09, 1.18]         1.06      0.88     [0.85, 0.92]\n TV_Vertrauen 1.06 [1.03, 1.12]         1.03      0.94     [0.90, 0.97]\n\n\nDie VIF-Werte liegen zwischen 0 und 5 (sogar alle bei 1); wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.4.html#multiple-regression-mit-dummy-codierung-der-kategorialen-variable-sex",
    "href": "Skript_7.4.html#multiple-regression-mit-dummy-codierung-der-kategorialen-variable-sex",
    "title": "Die multiple Regression",
    "section": "2.4 Multiple Regression mit Dummy-Codierung der kategorialen Variable “sex”",
    "text": "2.4 Multiple Regression mit Dummy-Codierung der kategorialen Variable “sex”\nEine Besonderheit schauen wir uns nun noch zum Schluss an: Die lineare Regression ist ein Verfahren für metrische Daten. Sie untersucht den Zusammenhang zwischen einer metrischen abhängigen Variable und mindestens einer metrischen unabhängigen Variable. In den bisherigen Regressionsanalysen haben wir mit metrischen und quasi-metrischen Variablen gearbeitet, die die Voraussetzung erfüllen, dass eine Regression gerechnet werden kann. Das ist auch der übliche Fall. Es gibt aber die “Ausnahme”, dass auch kategorische (v.a. binäre) Variablen bei der Regressionsanalyse grundsätzlich eingesetzt werden können, wenn diese durch eine Dummy-Coding passend gemacht werden. Und diesen Fall schauen wir un nun am Beispiel der Variable “Geschlecht” an. Dazu müssen wir die Variable aber als binär codiert bzw. dichotom betrachten - d.h. wir behandeln sie so, als hätten wir hier nur 2 Ausprägungen.\n\n2.4.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Vertrauen ins TV, Geschlecht und TV-Konsum\nWir wollen das biologische Geschlecht (sex) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei “sex” haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist. Es handelt sich vielmehr um eine kategoriale Variable (mit 3 Ausprägungen). Dennoch können wir die Variable mit einem “Trick” in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Hier wollen uns nun mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl aber erst einmal umcodieren und dadurch in eine dichotome VAriable verwandeln.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\ndaten &lt;- daten %&gt;%\n filter(between(sex, 1, 2)) %&gt;%\n  mutate(sex_d = case_when(sex == 1 ~ 0, sex == 2 ~ 1))\n\n\nhäufigkeitstabelle &lt;- table(daten$sex_d)\nprint(häufigkeitstabelle)\n\n\n   0    1 \n1546 1569 \n\n\nDie Dummy-Codierung war erfolgreich - wir haben nun 1546 Befragte mit der Ausprägung 0 (männlich) sowie 1569 Befragte mit der Ausprägung 2 (weiblich)."
  },
  {
    "objectID": "Skript_7.4.html#vorbereitung-der-daten-zum-zusammenhang-von-alter-bildung-vertrauen-ins-tv-geschlecht-und-tv-konsum",
    "href": "Skript_7.4.html#vorbereitung-der-daten-zum-zusammenhang-von-alter-bildung-vertrauen-ins-tv-geschlecht-und-tv-konsum",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "9 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Vertrauen ins TV, Geschlecht und TV-Konsum",
    "text": "9 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Vertrauen ins TV, Geschlecht und TV-Konsum\nWir wollen das biologische Geschlecht (sex) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei “sex” haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist. Es handelt sich vielmehr um eine kategoriale Variable (mit 3 Ausprägungen). Dennoch können wir die Variable mit einem “Trick” in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Hier wollen uns nun mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl aber erst einmal umcodieren und dadurch in eine dichotome VAriable verwandeln.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten."
  },
  {
    "objectID": "Skript_7.4.html#dummy-codierung-der-variable-sex",
    "href": "Skript_7.4.html#dummy-codierung-der-variable-sex",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "10 Dummy-Codierung der Variable sex",
    "text": "10 Dummy-Codierung der Variable sex\n\ndaten &lt;- daten %&gt;%\n filter(between(sex, 1, 2)) %&gt;%\n  mutate(sex_d = case_when(sex == 1 ~ 0, sex == 2 ~ 1))"
  },
  {
    "objectID": "Skript_7.4.html#prüfung-der-umcodierung-zur-dummy-variablen",
    "href": "Skript_7.4.html#prüfung-der-umcodierung-zur-dummy-variablen",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "11 Prüfung der Umcodierung zur Dummy-Variablen",
    "text": "11 Prüfung der Umcodierung zur Dummy-Variablen\n\nhäufigkeitstabelle &lt;- table(daten$sex_d)\nprint(häufigkeitstabelle)\n\n\n   0    1 \n1546 1569 \n\n\nDie Dummy-Codierung war erfolgreich - wir haben nun 1546 Befragte mit der Ausprägung 0 (männlich) sowie 1569 Befragte mit der Ausprägung 2 (weiblich)."
  },
  {
    "objectID": "Skript_7.4.html#regressionsmodell-zum-zusammenhang-von-alter-bildung-tv-vertrauen-geschlecht-und-tv-konsum-spezifizieren-und-anzeigen-lassen",
    "href": "Skript_7.4.html#regressionsmodell-zum-zusammenhang-von-alter-bildung-tv-vertrauen-geschlecht-und-tv-konsum-spezifizieren-und-anzeigen-lassen",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "12 Regressionsmodell zum Zusammenhang von Alter, Bildung, TV-Vertrauen, Geschlecht und TV-Konsum spezifizieren und anzeigen lassen",
    "text": "12 Regressionsmodell zum Zusammenhang von Alter, Bildung, TV-Vertrauen, Geschlecht und TV-Konsum spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable sex_d ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel3 &lt;- lm(TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, data = daten) \nsummary(lm.beta(model3))\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, \n    data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-255.89  -64.84  -18.97   33.01 1245.99 \n\nCoefficients:\n              Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  170.54603           NA   12.96665  13.153 &lt;0.0000000000000002 ***\nAlter          1.56629      0.20625    0.13830  11.325 &lt;0.0000000000000002 ***\nBildung      -26.32891     -0.23397    1.99103 -13.224 &lt;0.0000000000000002 ***\nTV_Vertrauen   5.34102      0.05330    1.71809   3.109              0.0019 ** \nsex_d          4.60020      0.01766    4.34277   1.059              0.2896    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 121 on 3110 degrees of freedom\nMultiple R-squared:  0.1386,    Adjusted R-squared:  0.1375 \nF-statistic: 125.1 on 4 and 3110 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Skript_7.4.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-ergebnis-1",
    "href": "Skript_7.4.html#inhaltliche-interpretation-des-outputs-was-bedeutet-das-ergebnis-1",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "13 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?",
    "text": "13 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?\nGender hat keinen signifikanten Einfluss auf den TV-Konsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy-Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (4,6) Minuten höheren TV-Konsum als Männer (wobei dieser Befund statistisch ja nicht signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)"
  },
  {
    "objectID": "Skript_7.3.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen",
    "href": "Skript_7.3.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "1.1 Erzeugen eines Teildatensatzes & Umbenennen der Variablen",
    "text": "1.1 Erzeugen eines Teildatensatzes & Umbenennen der Variablen\nFür den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(Alter = age)%&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(TV_Konsum, 0, 1500))"
  },
  {
    "objectID": "Skript_7.3.html#modellierung-der-linearen-regression",
    "href": "Skript_7.3.html#modellierung-der-linearen-regression",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "1.2 Modellierung der linearen Regression:",
    "text": "1.2 Modellierung der linearen Regression:\nDann erstellen wir das linear model:\n\nmodel &lt;- lm(TV_Konsum ~ Alter, data = daten) \n1summary(model)\n2summary(lm.beta(model))\n\n\n1\n\nklassischer Output mit relevanten Kennzahlen\n\n2\n\nklassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\n\n\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438           NA     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928       0.2963     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022\n\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Fernsehnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten positiven Einfluss auf die Internetnutzung. Je älter eine Person ist, desto mehr nutzt er/sie das Fernsehen. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable TV-Nutzung um “den Estimate-Wert” in Messeinheiten (hier: -2.19 Minuten) zu. Dieser Zusammenhang ist mit p &gt;.05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern ihr euch, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Da habt ihr Recht: Eigentlich müssten wir erst die Voraussetzungen prüfen, dann erst das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen."
  },
  {
    "objectID": "Skript_7.2.html#laden-der-pakete-und-des-datensatzes",
    "href": "Skript_7.2.html#laden-der-pakete-und-des-datensatzes",
    "title": "Korrelation & Regression",
    "section": "1.1 Laden der Pakete und des Datensatzes",
    "text": "1.1 Laden der Pakete und des Datensatzes\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, dplyr) \ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nDann erzeugen wir einen neuen Teildatensatz, benennen die benötigten Variablen in “TV_Konsum” und “TV_Vertrauen” um und selektieren die fehlende Werte (z.B. -9=Keine Angabe) mittels Filter-Funktion."
  },
  {
    "objectID": "Skript_7.2.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen-und-filtern-der-fälle",
    "href": "Skript_7.2.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen-und-filtern-der-fälle",
    "title": "Korrelation & Regression",
    "section": "1.2 Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der Fälle",
    "text": "1.2 Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der Fälle\n\ndaten_neu &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(TV_Vertrauen= pt09)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(TV_Vertrauen, 1, 7))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\nkorrelation &lt;- daten_neu %&gt;% \n  summarize(correlation = cor(TV_Konsum, TV_Vertrauen, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.114\n\n\nSchauen wir uns den (sehr übersichtlichen) Output an: Wie erhalten einen Korrelationskoeffizient von 0,113903. Das bedeutet, dass zwischen den beiden Variablen TV_Konsum und TV_Vertrauen ein schwacher positiver linearer Zusammenhang besteht. Höhere Werte in einer Variable gehen also tendenziell mit höheren Werten in der anderen Variable einher. Jedoch ist die Korrelation relativ niedrig, was darauf hindeutet, dass der Zusammenhang zwischen den beiden Variablen nicht besonders stark ist.\nUm die Korrelation zwischen den Variablen “TV_Konsum” und “TV_Vertrauen” zu visualisieren, können wir mit dem ggplot2-Paket ein Scatterplot erstellen:"
  },
  {
    "objectID": "Skript_7.2.html#visualisierung-der-korrelation-mit-hilfe-eines-scatterplots",
    "href": "Skript_7.2.html#visualisierung-der-korrelation-mit-hilfe-eines-scatterplots",
    "title": "Korrelation & Regression",
    "section": "1.6 Visualisierung der Korrelation mit Hilfe eines Scatterplots",
    "text": "1.6 Visualisierung der Korrelation mit Hilfe eines Scatterplots\n\nscatterplot &lt;- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalitätsprognose)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"TV Konsum\", y = \"Kriminalitätsprognose\", title = \"Korrelation zwischen TV Konsum und Kriminalitätsprognose\")\nprint(scatterplot)"
  },
  {
    "objectID": "Skript_7.2.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen-und-filtern-der-fälle-1",
    "href": "Skript_7.2.html#erzeugen-eines-teildatensatzes-umbenennen-der-variablen-und-filtern-der-fälle-1",
    "title": "Korrelation & Regression",
    "section": "1.4 Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der Fälle",
    "text": "1.4 Erzeugen eines Teildatensatzes, Umbenennen der Variablen und Filtern der Fälle\n\ndaten_neu2 &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(Kriminalitätsprognose= cf03)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(Kriminalitätsprognose, 1, 5))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-ausgabe-des-korrelationskoeffizienten",
    "href": "Skript_7.2.html#berechnung-und-ausgabe-des-korrelationskoeffizienten",
    "title": "Korrelation & Regression",
    "section": "1.5 Berechnung und Ausgabe des Korrelationskoeffizienten",
    "text": "1.5 Berechnung und Ausgabe des Korrelationskoeffizienten\n\nkorrelation &lt;- daten_neu2 %&gt;% \n  summarize(correlation = cor(TV_Konsum, Kriminalitätsprognose, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1      -0.196\n\n\nBetrachten wir den Output: Der Korrelationskoeffizient von -0,196 deutet auf einen schwachen negativen linearen Zusammenhang zwischen der “täglichen Fernsehnutzung in Minuten” und der Einschätzung zur Entwicklung der Kriminalität in Deutschland hin.\nACHTUNG! Zur Interpretation müssen Sie nun aber auch berücksichtigen, wie die Werte der Variable gelabelt sind: Ein höherer Wert bedeutet, dass man eine Abnahme der Kriminalitätsentwicklung vermutet (1=hat stark zugenommen; 5=hat stark abgenommen). Der negative Korrelationskoeffizient bedeutet dann, dass höhere Werte in der “täglichen Fernsehnutzung in Minuten” tendenziell mit niedrigeren Werten in der Einschätzung zur Kriminalitätsentwicklung korrelieren. Oder anders gesagt: Personen, die mehr Fernsehen schauen, tendieren eher zur Annahme, dass die Kriminalität in Deutschland weniger zugenommen oder sogar abgenommen hat.\nSo oder so ist zu beachten, dass der Korrelationskoeffizient nur den linearen Zusammenhang erfasst und keine Aussagen zur Kausalitäten des Zusammenhangs macht. Korrelation ist nicht Kausalität!\nZur grafischen Illustration erstellen wir nun wieder ein Scatterplot:"
  },
  {
    "objectID": "Skript_7.2.html#visualisierung-der-korrelation-mit-hilfe-eines-scatterplots-1",
    "href": "Skript_7.2.html#visualisierung-der-korrelation-mit-hilfe-eines-scatterplots-1",
    "title": "Korrelation & Regression",
    "section": "1.6 Visualisierung der Korrelation mit Hilfe eines Scatterplots",
    "text": "1.6 Visualisierung der Korrelation mit Hilfe eines Scatterplots\n\nscatterplot &lt;- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalitätsprognose)) +\n  geom_point(color = \"red\") +\n  labs(x = \"TV Konsum\", y = \"Kriminalitätsprognose\", title = \"Korrelation zwischen TV Konsum und Kriminalitätsprognose\")\nprint(scatterplot)"
  },
  {
    "objectID": "Skript_7.2.html#vorbereitung-und-laden-der-daten",
    "href": "Skript_7.2.html#vorbereitung-und-laden-der-daten",
    "title": "Korrelation & Regression",
    "section": "3.1 Vorbereitung und Laden der Daten",
    "text": "3.1 Vorbereitung und Laden der Daten\nWie immer besteht der erste Schritt nun darin, die benötigten Pakete sowie den Datensatz zu laden. Für die lineare Regression kommen zwei neue Pakete hinzu: Wir laden das Paket broom, um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können sowie das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt. Das Paket “see” kommt außerdem dazu, weil es uns eine toolbox für die Visualisierung der Zusammenhänge bereitstellt.\n\n3.1.1 Laden der benötigten Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, dplyr, broom, lm.beta, performance, see) \n\n\n\n3.1.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n\n3.1.3 Erzeugen eines Teildatensatzes & Umbenennen der Variablen\nFür den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(Alter = age)%&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(TV_Konsum, 0, 1500))\n\n\n1theme_set(theme_minimal())\n\n2options(scipen = 999)\n\n\n1\n\nVisualisierungshintergrund der Grafiken in ggplot festlegen\n\n2\n\nAnzeige der p-Werte als Zahlen mit Nachkommastellen einstellen"
  },
  {
    "objectID": "Skript_7.2.html#ziel-der-analyse",
    "href": "Skript_7.2.html#ziel-der-analyse",
    "title": "Korrelation & Regression",
    "section": "3.2 Ziel der Analyse",
    "text": "3.2 Ziel der Analyse\nMit Hilfe der Regression wollen wir nun die oben formulierte Annahme prüfen, dass das Alter die täglichen Fernsehnutzung in Minuten (umbenannt in TV_Konsum) erklären kann. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse grundsätzlich eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden, dazu aber später mehr).\nBevor wir die Regressionsanalyse durchführen, verschaffen wir uns zunächst einmal einen Überblick über die Daten - dazu visualisieren wir den Zusammenhang zwischen dem Alter sowie der täglichen Fernsehnutzung. Mit Hilfe der Grafik können wir auch die erste Voraussetzung prüfen, nämlich dass es einen linearen Zusammenhang zwischen beiden Variablen gibt."
  },
  {
    "objectID": "Skript_7.2.html#grafische-prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#grafische-prüfung-der-voraussetzungen",
    "title": "Korrelation & Regression",
    "section": "2.2 Grafische Prüfung der Voraussetzungen",
    "text": "2.2 Grafische Prüfung der Voraussetzungen\nBevor wir die Regressionsanalyse durchführen, verschaffen wir uns zunächst einmal einen Überblick über die Daten - dazu visualisieren wir den Zusammenhang zwischen dem Alter sowie der täglichen Fernsehnutzung. Mit Hilfe der Grafik können wir auch die erste Voraussetzung prüfen, nämlich dass es einen linearen Zusammenhang zwischen beiden Variablen gibt.\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und TV_Konsum. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Alter) und x (=TV_Konsum) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten, aes(Alter, TV_Konsum)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\", color = \"lightgreen\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und TV_Konsum\") + \n  xlab(\"TV_Konsum\") + ylab(\"Alter\")\n\n\n\n\nDie grafische Darstellung legt uns einen positiven und linearen Zusammenhang zwischen Alter und Fernsehnutzung nahe: mit zunehmendem Alter steigt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt."
  },
  {
    "objectID": "Skript_7.2.html#beispiel-2-vorhersage-für-die-tägliche-internetnutzung",
    "href": "Skript_7.2.html#beispiel-2-vorhersage-für-die-tägliche-internetnutzung",
    "title": "Korrelation & Regression",
    "section": "3.5 Beispiel 2: Vorhersage für die tägliche Internetnutzung",
    "text": "3.5 Beispiel 2: Vorhersage für die tägliche Internetnutzung\nIn unserem zweiten Beispiel betrachten wir andere Altersgruppen. Wir lassen uns hier mit Hilfe unseres Modells die tägliche Internetnutzung in Minuten bei einem Alter von 20, 30, 40, 50, 60 und 70 vorhersagen.\n\npredict.lm(model, data.frame(Alter = c(20, 30, 40, 50, 60, 70)))\n\n       1        2        3        4        5        6 \n108.4996 130.4274 152.3553 174.2832 196.2110 218.1389 \n\n\n\n3.5.1 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable TV-Konsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen TV-Konsum von (X) Minuten.\nWeil wir im Datensatz aber n=4929 Fälle haben (und die Ausgabe sonst zu unübersichtlich wird), begrenzen ich die Ausgabe auf die ersten 100 Zeilen (Fälle) des Datensatzes, indem ich zusätzlich die Funktion head() verwende, und die Anzahl der gewünschten Fälle in der Klammer mit 100 festlege:\n\n\n3.5.2 Vorhersagewerte für jede Beobachtung anzeigen\n\nhead(fitted(model), 100)\n\n       1        2        3        4        5        6        7        8 \n183.0543 180.8615 259.8019 237.8740 200.5966 115.0779 132.6202 213.7533 \n       9       10       11       12       13       14       15       16 \n176.4760 189.6327 251.0307 121.6563 147.9697 183.0543 163.3192 172.0904 \n      17       18       19       20       21       22       23       24 \n121.6563 246.6451 169.8976 169.8976 224.7173 200.5966 119.4635 183.0543 \n      25       26       27       28       29       30       31       32 \n183.0543 176.4760 196.2110 172.0904 189.6327 191.8255 191.8255 150.1625 \n      33       34       35       36       37       38       39       40 \n244.4523 233.4884 237.8740 112.8851 233.4884 183.0543 174.2832 119.4635 \n      41       42       43       44       45       46       47       48 \n207.1750 187.4399 222.5245 178.6687 213.7533 185.2471 132.6202 237.8740 \n      49       50       51       52       53       54       55       56 \n211.5605 209.3678 115.0779 246.6451 154.5481 189.6327 112.8851 147.9697 \n      57       58       59       60       61       62       63       64 \n215.9461 200.5966 169.8976 204.9822 121.6563 224.7173 172.0904 147.9697 \n      65       66       67       68       69       70       71       72 \n152.3553 174.2832 189.6327 156.7409 185.2471 132.6202 185.2471 213.7533 \n      73       74       75       76       77       78       79       80 \n264.1874 202.7894 187.4399 233.4884 187.4399 191.8255 196.2110 194.0183 \n      81       82       83       84       85       86       87       88 \n176.4760 200.5966 194.0183 207.1750 174.2832 143.5842 119.4635 161.1265 \n      89       90       91       92       93       94       95       96 \n165.5120 161.1265 174.2832 150.1625 172.0904 110.6924 196.2110 233.4884 \n      97       98       99      100 \n110.6924 196.2110 185.2471 178.6687 \n\n\n\n\n3.5.3 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nIn der Ausgabe kann ich nun die laut Modell prognostizierte Fernsehnutzung für meine Befragten entsprechend ihres Alters sehen - Befragte(r) 10 hat so z.B. eine prognostizierte Fernsehnutzung von 189 Minuten.\nNun haben wir im Rahmen unserer Befragung die TV-Nutzung der Befragten aber ja schon erhoben. Wozu dient diese Prognose dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nhead(residuals(model), 100)\n\n&lt;labelled&lt;double&gt;[100]&gt;: FERNSEHGESAMTDAUER PRO TAG IN MINUTEN\n           1            2            3            4            5            6 \n  26.9456805  -90.8615329 -124.8018509 -177.8739847  -20.5966124  -70.0779345 \n           7            8            9           10           11           12 \n-102.6202274  -33.7533320    3.5240404  -69.6326793  198.9692956 -111.6562944 \n          13           14           15           16           17           18 \n -27.9697337   56.9456805   46.6807600  -82.0903864  -31.6562944 -126.6451312 \n          19           20           21           22           23           24 \n -79.8975998  -19.8975998   75.2827349 1059.4033876    0.5364922   56.9456805 \n          25           26           27           28           29           30 \n  -3.0543195  -26.4759596  -76.2110391    7.9096136  -39.6326793  108.1745341 \n          31           32           33           34           35           36 \n -71.8254659  -90.1625203 -124.4523446    6.5115885 -117.8739847  426.1148521 \n          37           38           39           40           41           42 \n -83.4884115  146.9456805  125.7168270  -29.4635078  -17.1749722   82.5601073 \n          43           44           45           46           47           48 \n -12.5244785  -28.6687462 -138.7533320  -65.2471061  -12.6202274  -27.8739847 \n          49           50           51           52           53           54 \n   8.4394546  150.6322412   94.9220655  -66.6451312  -34.5480935  -39.6326793 \n          55           56           57           58           59           60 \n -52.8851479 -132.9697337  -65.9461186  -20.5966124  -49.8975998  -24.9821856 \n          61           62           63           64           65           66 \n -31.6562944  -14.7172651    7.9096136    2.0302663  -92.3553069    5.7168270 \n          67           68           69           70           71           72 \n -69.6326793  -36.7408801   -5.2471061  -72.6202274   84.7528939  -93.7533320 \n          73           74           75           76           77           78 \n-174.1874241  -82.7893990  126.5601073    6.5115885   52.5601073  -71.8254659 \n          79           80           81           82           83           84 \n  13.7889609  -44.0182525  -26.4759596  -20.5966124   75.9817475  -87.1749722 \n          85           86           87           88           89           90 \n   5.7168270    6.4158395  -89.4635078   18.8735466  -45.5120266    3.8735466 \n          91           92           93           94           95           96 \n   5.7168270  149.8374797  -67.0903864  -80.6923613   68.7889609  246.5115885 \n          97           98           99          100 \n -80.6923613  -16.2110391  -35.2471061  -58.6687462 \n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n   -10       TNZ: FILTER\n    -9      KEINE ANGABE\n\n\n\n\n3.5.4 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nFür unseren Fall Nummer 65 beträgt die Abweichung der Prognose von der Beobachtung -92 Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 8 Prozent nicht besonders groß ist.\n\n\n3.5.5 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\n\n3.5.6 Berechnung der Vorhersagewerte und Residuen für zusätzliche Plots\n\ndaten$vorhersage &lt;- predict(model) \ndaten$residuen &lt;- as.numeric(residuals(model))\n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\n\n3.5.7 Beobachtete und vorhergesagte Werte sowie Residuen gemeinsam plotten\n\nggplot(daten, aes(Alter, TV_Konsum)) + \n1  geom_point(aes(color = residuen)) +\n2  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n3  guides(color = \"none\") +\n4  geom_point(aes(y = vorhersage), shape = 1) +\n5  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") +\n6  geom_segment(aes(xend = Alter, yend = vorhersage), alpha = .2) +\n7  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Fernsehnutzung\") +\n8  xlab(\"Alter\") + ylab(\"tägliche Fernsehnutzung (Minuten)\")\n\n\n1\n\nFestlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n\n2\n\nFestlegung der Farbe für die Residuen\n\n3\n\nUnterdrückt eine Legende an der Seite (ist obligatorisch)\n\n4\n\ngibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n\n5\n\ngibt die Regressionsgerade als Linie aus\n\n6\n\nzeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein\n\n7\n\nTitel\n\n8\n\nAchsen-Beschriftung\n\n\n\n\n\n\n\n\nvorhersagen &lt;- predict(model)\nresiduen &lt;- residuals(model)\n\n1plot(vorhersagen, residuen,\n     xlab = \"Vorhersagen\", ylab = \"Residuen\",\n     main = \"Residualplot\", pch = 16, col = \"blue\")\n2abline(h = 0, col = \"red\", lty = 2)\n\n\n1\n\nResidualplot erstellen\n\n2\n\nHinzufügen einer Linie bei y = 0 für Referenz\n\n\n\n\n\n\n\n\n\n3.5.8 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438           NA     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928       0.2963     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Skript_7.1.html#interpretation-was-sehen-wir-im-output",
    "href": "Skript_7.1.html#interpretation-was-sehen-wir-im-output",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.3 Interpretation: Was sehen wir im Output?",
    "text": "3.3 Interpretation: Was sehen wir im Output?\nSchauen wir uns nun den R-Output an: Der Chi-Quadrat-Wert (X² oder X-squared) gibt uns an, wie gut die beobachteten Häufigkeiten mit den erwarteten Häufigkeiten übereinstimmen. Ein höherer X²-Wert deutet auf eine größere Abweichung hin.\ndf zeigt die Anzahl der Freiheitsgrade (Degrees of Freedom) des Chi-Quadrat-Tests an. Sie hängt von der Anzahl der Kategorien in den Variablen ab und beeinflusst die Verteilung des Chi-Quadrat-Werts.\nDer p-Wert gibt die Wahrscheinlichkeit an, den beobachteten Chi-Quadrat-Wert zu erhalten, wenn die Nullhypothese wahr ist (d.h. wenn es keinen Zusammenhang zwischen den Variablen gibt). Der hier beobachtete p-Wert (von 0.01095) ist kleiner als .05 und damit signifikant. Das deutet darauf hin, dass der beobachtete Effekt überzufällig ist - Geschlecht und Konfessionszugehörigkeit hängen also statistisch zusammen.\nAllerdings ist der Chi-Quadrat-Test empfindlich gegenüber der Stichprobengröße: Bei sehr großen Stichproben können auch kleine Unterschiede signifikant werden. In solchen Fällen ist es ratsam, neben dem p-Wert auch die Effektstärke zu betrachten. Dazu ermitteln wir nun noch Cramér’s V. Das ist ein Maß für den Zusammenhang zwischen zwei kategorialen Variablen. Es reicht von 0 bis 1, wobei 0 keinen Zusammenhang und 1 einen vollständigen Zusammenhang anzeigt."
  },
  {
    "objectID": "Home.html#inhalte-des-online-lehrbuchs",
    "href": "Home.html#inhalte-des-online-lehrbuchs",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "Inhalte des Online-Lehrbuchs",
    "text": "Inhalte des Online-Lehrbuchs\nWir hoffen mit diesem Buch einen umfassenden Überblick über die statistischen Verfahren zu bieten. Unter dem Reiter Der Kurs finden sich die einzelnen Kapitel des Buches. Darin behandeln wir eine Einführung in R, den Umgang mit Datensätzen und Daten sowie erste Schritte der Datenanalyse. Die statistischen Verfahren wiederum umfassen die Faktorenanalyse, die Mittelwertvergleiche t-Test und Varianzanalyse sowie die Analyse von Zusammenhängen mittels Korrelation und Regression.\nDie einzelnen Kapitel umfassen dabei jeweils kurz die theoretischen Grundlagen und gehen anschließend dezidiert auf die Umsetzung der Verfahren in R ein."
  },
  {
    "objectID": "Home.html#aufbau-der-kapitel-umgang-mit-dem-buch",
    "href": "Home.html#aufbau-der-kapitel-umgang-mit-dem-buch",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "Aufbau der Kapitel & Umgang mit dem Buch",
    "text": "Aufbau der Kapitel & Umgang mit dem Buch\nDie Kapitel enthalten neben der textlichen Erklärung der Verfahren jeweils auch ein kurzes Video, welches eine Einführung in das zu behandelnde Thema bietet.\nDes Weiteren enthalten alle Skripte den benötigten Code in farblich abgesetzten Felder. Innerhalb dieser Codefehler haben wir oftmals zusätzliche Erklärungen zu den Codezeilen hinzugefügt. Hier könnt ihr jeweils über die Zahlen hovern um euch die Erklärungen anzeigen zu lassen.\n\nDie Codefelder könnt ihr als Ganzes direkt in eure Zwischenablage kopieren. Geht dafür einfach mit der Maus über das Codefeld und klickt anschließend auf das kleine Clipboard.\n\nAlternativ könnt ihr unter dem Reiter XX auch alle Skripte der einzelnen Kapitel als Ganzes in Form von Quarto-Dokumenten downloaden."
  },
  {
    "objectID": "index.html#inhalte-des-online-lehrbuchs",
    "href": "index.html#inhalte-des-online-lehrbuchs",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "Inhalte des Online-Lehrbuchs",
    "text": "Inhalte des Online-Lehrbuchs\nWir hoffen mit diesem Buch einen umfassenden Überblick über die statistischen Verfahren zu bieten. Unter dem Reiter Der Kurs finden sich die einzelnen Kapitel des Buches. Darin behandeln wir eine Einführung in R, den Umgang mit Datensätzen und Daten sowie erste Schritte der Datenanalyse. Die statistischen Verfahren wiederum umfassen die Faktorenanalyse, die Mittelwertvergleiche t-Test und Varianzanalyse sowie die Analyse von Zusammenhängen mittels Korrelation und Regression.\nDie einzelnen Kapitel umfassen dabei jeweils kurz die theoretischen Grundlagen und gehen anschließend dezidiert auf die Umsetzung der Verfahren in R ein."
  },
  {
    "objectID": "index.html#aufbau-der-kapitel-umgang-mit-dem-buch",
    "href": "index.html#aufbau-der-kapitel-umgang-mit-dem-buch",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "Aufbau der Kapitel & Umgang mit dem Buch",
    "text": "Aufbau der Kapitel & Umgang mit dem Buch\nDie Kapitel enthalten neben der textlichen Erklärung der Verfahren jeweils auch ein kurzes Video, welches eine Einführung in das zu behandelnde Thema bietet.\nDes Weiteren enthalten alle Skripte den benötigten Code in farblich abgesetzten Felder. Innerhalb dieser Codefehler haben wir oftmals zusätzliche Erklärungen zu den Codezeilen hinzugefügt. Hier könnt ihr jeweils über die Zahlen hovern um euch die Erklärungen anzeigen zu lassen.\n\nDie Codefehler könnt ihr als Ganzes direkt in eure Zwischenablage kopieren. Geht dafür einfach mit der Maus über das Codefeld und klickt anschließend auf das kleine Clipboard.\n\nAlternativ könnt ihr unter dem Reiter XX auch alle Skripte der einzelnen Kapitel als Ganzes in Form von Quarto-Dokumenten downloaden."
  },
  {
    "objectID": "Skript_1.1.html#schaltflächen",
    "href": "Skript_1.1.html#schaltflächen",
    "title": "Einführung in R und RStudio",
    "section": "4.1 Schaltflächen",
    "text": "4.1 Schaltflächen\nInnerhalb von RStudio unterscheiden wir 4 Schaltflächen welche sich beliebig via Drag and Drop verschieben oder auch minimieren lassen.\n\n\n\nBild Programmoberfläche\n\n\n1 Ist der Bereich in welchen wir Skripte, Markdown und Quarto-Dokumente bearbeiten und ausführen können (siehe auch Markdown und Quarto)\n2 Ist die Konsole. Dies ist der Bereich in welchem wir weiterhin oldschool R angezeigt bekommen. Dieser Bereich kann sehr hilfreich sein, wenn man kurz Befehle benötigt, welche nicht im Skript auftauchen sollen (beispielsweise eine kurze Hilfe zu Funktionen mittels ?)\n3 In diesem Bereich findet sich alles zu den innerhalb von R geladenen Datensätzen. Unter Environment finden sich die Datensätze (Data), die Historie der genutzten Befehle (History), eine Schnittstelle zu Datenbanken (Connection) sowie R-interne Tutorials (Tutorial).\n4 In diesem Bereich finden sich verschiedene Reiter, welche die Organisation der Arbeit mit R erleichtern. Unter File befinden sich alle innerhalb des Ordners oder Projektes befindlichen Dateien. Unter Plots kann man sich die in R erstellten Grafiken anzeigen lassen. Packages zeigt alle in R installierten Pakete an. Hier lassen sich mittels install auch neue Pakete installieren oder über update die aktuellen Pakete updaten. Bei Help können wir eine Hilfeseite aufrufen. Es kann wahlweise direkt innerhalb der Seite nach Hilfen gesucht werden oder mit dem Befehl ?Name der Funktion bzw. ??Name des Packages innerhalb der Konsole. Möchte man nach einem Befehl aus einem Paket suchen nutzt man den Suchbefehle?Name des Packages::Name der Funktion. Unter Viewer finden sich gerenderte Dokumente (beispielsweise ein gerendertes Markdown oder Quarto Dokument) und unter Presentation gerenderte Shiny-Dokumente."
  },
  {
    "objectID": "Skript_1.1.html#menüleiste",
    "href": "Skript_1.1.html#menüleiste",
    "title": "Einführung in R und RStudio",
    "section": "4.2 Menüleiste",
    "text": "4.2 Menüleiste\nZusätzlich zu den Schaltflächen findet sich oben links eine Menüleiste:\n\n\n\nBild Menüleiste\n\n\n\nUnter File können neue Dateien erstellt, geöffnet und gespeichert werden.\nEdit bietet Möglichkeiten der Dateibearbeitung (bspw. Kopieren, Ausschneiden, Rückgängig etc.) falls ihr keine Short-Cuts nutzen wollt.\nCode gibt eine Übersicht über Funktionen innerhalb des Markdown-Dokumentes (bspw. Codechunks einfügen).\nUnter View können die einzelnen Schaltflächen und deren Aufteilung geändert werden. Wahlweise geht dies auch via Drag & Drop.\nPlots vereinfacht den Umgang mit in r erstellten Grafiken. Wahlweise könnt ihr hier auch den Reiter Plots in Schaltfläche 4 nutzen.\nSession hier kann eine R-Session neu gestartet oder beendet werden (siehe auch 1.6 Die Session).\nBuild, Debug und Profile beinhaltet Sonderanwendungen, wie beispielsweise das Debugging von Funktionen oder Fragen nach der Speed-Optimierung von R-Code.\nTools hat viele hilfreiche Funktionen. Hier können unter anderem Pakete installiert und hilfreiche Keyboard Shortcuts ausgegeben werden. Am bedeutsamsten ist hier jedoch der Bereich Global Options, in welchem unter anderem grundlegende Einstellungen zum Speicherort von R und den Paketen, zur Aufteilung und Aussehen von RStudio und zur Funktionalität von R Markdown getroffen werden können.\n\nAuch der Reiter Help kann sehr hilfreich sein. Hier finden sich eine Hilfeseite (siehe auch Reiter Help in Schaltfläche 4), Möglichkeiten der besseren Zugänglichkeit (Accessibility), Cheat Sheets für die Arbeit mit R und erneut eine Übersicht über Shortcuts für die Arbeit mit R."
  },
  {
    "objectID": "Skript_1.1.html#kurz-exkurs-das-tidyverse",
    "href": "Skript_1.1.html#kurz-exkurs-das-tidyverse",
    "title": "Einführung in R und RStudio",
    "section": "5.1 Kurz-Exkurs: das tidyverse",
    "text": "5.1 Kurz-Exkurs: das tidyverse\nIn R selbst findet sich eine Vielzahl von Befehlen. Zusätzlich wird R von den Nutzern immer weiter entwickelt und es kommen neue Funktionen in Form von Paketen hinzu. Eines der meist genutzten Pakete(-universen) stellt dabei das tidyverse dar. Mit den Befehlen und Funktionen dieses Paketes kommt eine etwas andere Programmiersprache, welche uns jedoch die Arbeit mit R erleichtert. Gerade auch bei Fragen des Datenmanagementes ist das tidyverse hilfreich, denn hier kommt ein zweiter Fun-Fact über R:💡 R kann nicht nur manchmal etwas dumm sein (wir erinnern uns, es denkt nicht mit), es ist auch recht vergesslich. Wir haben oben bereits gelernt, dass wir in Befehlen immer den Datensatz und die Variable spezifizieren müssen. Dies stellt kein Problem bei einzelnen Befehlen dar, ist jedoch bei einer Vielzahl von Befehlen etwas nervig. Hier kommt uns die tidyverse Logik zu Nutze, in welcher einmal zu Beginn des Dokumentes der Datensatz spezifziert wird und anschließend alle weiteren Schritte durch eine Pipe %&gt;% verbunden werden. Die Pipe (Shortcut Windows: Ctrl + Shift + M; MAC: Cmd + Shift + M) bedeutet so viel wie “und dann”. Also im Prinzip sagen wir R, nehme diesen Datensatz und dann mache die folgenden Dinge, wobei wir so viele Schritte wie wir möchten jeweils mit Pipes verbinden können. Wir nutzen in unseren Skripten hauptsächlich die tidyverse-Logik, erklären diese daher grundlegender in Kapitel 3.1, wenn wir uns mit den ersten Schritten des Datenmanagements beschäftigen."
  },
  {
    "objectID": "Skript_1.1.html#die-installation-von-paketen",
    "href": "Skript_1.1.html#die-installation-von-paketen",
    "title": "Einführung in R und RStudio",
    "section": "6.1 Die Installation von Paketen",
    "text": "6.1 Die Installation von Paketen\nPakete können wahlweise R-intern über CRAN (das steht für Comprehensive R Archive Network und ist das zentrale Software-Repository) oder direkt von GitHub (für sehr neue Pakete, welche noch nicht auf CRAN sind) installiert werden. Im Normalfall installieren wir jedoch direkt von CRAN, da hier eine Vielzahl von Paketen und Funktionen vorhanden sind und die Installation sehr simpel ist.\nWenn wir ein R-Paket von CRAN installieren möchten nutzen wir die Funktion install.packages(), in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen. Für das Paket tidyverse wäre der Befehl wie folgt:\n\ninstall.packages(\"tidyverse\")\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und bestätigt die Anwendung wiederum mit Install.\n\n\n\nScreenshot RStudio\n\n\nEinige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden. Grundsätzlich ist es jedoch immer ratsam, einmal zu checken, ob die Pakete in der aktuellen Version installiert sind, da ansonsten die Funktionen der Pakete nicht funktionieren können."
  },
  {
    "objectID": "Skript_1.1.html#das-laden-von-paketen",
    "href": "Skript_1.1.html#das-laden-von-paketen",
    "title": "Einführung in R und RStudio",
    "section": "6.2 Das Laden von Paketen",
    "text": "6.2 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet. Dies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen):\n\nlibrary(tidyverse)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio\n\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link\n\n\n📖 Julia Niemann-Lenz (2021). R Kompendium für die kommunikationswissenschaftliche Statistik- und Datenanalyse-Ausbildung. Link"
  },
  {
    "objectID": "Skript_1.3.html#yaml-header",
    "href": "Skript_1.3.html#yaml-header",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6.1 YAML-Header",
    "text": "6.1 YAML-Header\nInnerhalb des YAML Headers, welcher jeweils von --- umgeben ist, legen wir die Dokumentstruktur fest.\n\n\n\nYAML Header in einem Quarto Dokument\n\n\nDies beinhaltet beispielsweise den Titel des Dokumentes title:, die Autoren author, sowie Spezifikationen zur Dokumentstruktur, wie beispielsweise das Outputformat format: oder auch in Quarto Spezifikationen zum Umgang mit den Codechunks execute: echo : true auf Gesamtdokumentebene."
  },
  {
    "objectID": "Skript_1.3.html#text",
    "href": "Skript_1.3.html#text",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6.2 Text",
    "text": "6.2 Text\nIn Markdown und Quarto-Dokumenten können wir Text einbinden und diesen beliebig formatieren. Dazu können wir wahlweise die Source-Variante oder die Visual-Variante nutzen. In der Source-Variante variieren wir Text mittels Syntax. Typische Syntaxbefehle sind:\n\n*kursiv*: jeweils einen Stern vor und nach einem Wort um dieses kursiv zu schreiben\n**fett**: jeweils zwei Sterne vor und nach einem Wort um dieses fett zu schreiben\n#: Rauten für Überschriften, wobei eine Raute die erste Überschrift signalisiert, zwei Rauten die zweite usw.\n![Bildunterschrift](Link des Bildes): um Bilder einzufügen\n[Linktext](url): Um Links einzufügen\n\nMöchten wir übrigens die oben genutzten Symbole im Text nutzen, so können wir mit einem  vor dem jeweiligen Symbol die Formatierung umgehen.\nWahlweise können wir auch den Visual-Modus nutzen, indem wir oben in der Dokumentleiste von Sourceauf Visual umstellen. In diesem Modus erhalten wir ein Word-ähnliches Interface und können Formatoptionen durch Klicken auf die jeweilige Formatierung umsetzen:\n\n\n\nFormatierungsoptionen im Visual Modus"
  },
  {
    "objectID": "Skript_1.3.html#code-chunks",
    "href": "Skript_1.3.html#code-chunks",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6.3 Code Chunks",
    "text": "6.3 Code Chunks\nInnerhalb von Markdown und Quarto können wir Codebefehle direkt in unser Dokument innerhalb von sogenannten Codechunks integrieren. Hier können wir alle Arten von Code schreiben sowie diese mit Hilfe von # direkt kommentieren (alles hinter einer Raute wird dabei nicht ausgeführt). Codechunks beginnen mit drei Backticks und einem r in geschweiften Klammern und enden wieder mit drei Backticks:\n\n```{r}\n# Dies ist ein Code Chunk\n```\n\nUm die Codechunks zu erzeugen können wir einfach auf Code -&gt; Insert Codechunk gehen, auf das +C-Symbol in der Dokumentmenüleiste oder den Shortcut Alt + Strg + I nutzen. Innerhalb der Chunks können wir Code schreiben und ausführen. Dies geschieht für eine einzelne Codezeile mit dem Shortcut Strg + Enter und für den gesamten Codechunk mit dem Shortcut Strg + Shift + Enter. Wahlweise könnt ihr auch den kleinen grünen Pfeil in der rechten oberen Ecke des Chunks, Code -&gt; Run Selected Lines oder den Punkt Run in der Quarto-Dokumentleiste auswählen.\nZusätzlich können wir hinter dem {r} angeben, wie R mit dem Code des Chunks umgehen soll. Wir können beispielsweise auswählen, ob R den Code ausführen soll (eval = T/F) ob der Codebereich in unserem Enddokument aufgeführt sein soll (echo = T) oder wir lediglich die Ergebnisse angezeigt wollen (echo = F) oder ob wir beispielsweise Warnungen (warnings = T/F) oder Messages (message = T/F) in unserem Output-Dokument wünschen:\n\n```{r, echo = T}\n# Dies ist ein Code Chunk\n```\n\nWahlweise können wir diese Optionen auch für das Gesamtdokument im YAML-Header festlegen. Dafür nutzen wir den Zusatz execute: und geben anschließend alle unsere Dokumentoptionen (für einen Überblick siehe hier) an.\n\n\n\nCodeoptionen im Quarto Header\n\n\nAchtung: wir nutzen hier : statt = und schreiben true und false statt TRUE/T und FALSE/F."
  },
  {
    "objectID": "Autoren.html#michael-linke",
    "href": "Autoren.html#michael-linke",
    "title": "Das Team stellt sich vor",
    "section": "Michael Linke",
    "text": "Michael Linke\nVita\n\n\n\n\n\nMichael Linke\n\n\nMichael Linke ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen und hat einen B.A. in Kunstgeschichte und Politikwissenschaft sowie einen M.Sc. in Informatik für Geistes- und Sozialwissenschaftler. Seine Forschungsinteressen liegen im Bereich Maschinelles Lernen in den Sozialwissenschaften, Netzwerkanalyse und Protestforschung.\nForschungsschwerpunkte\n\nMaschinelles Lernen, vor allem Deep Learning\nProtest Event Analysis\nPolitische Kommunikation\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite"
  },
  {
    "objectID": "Skript_6.4.html#datenmanagement-1",
    "href": "Skript_6.4.html#datenmanagement-1",
    "title": "Die Varianzanalyse",
    "section": "0.2 Datenmanagement",
    "text": "0.2 Datenmanagement\nUm mit dem Datensatz zu arbeiten benötigen wir einige grundlegende Schritte des Datenmanagements für ausführliche Erklärungen siehe hier. Für unsere Varianzanalyse möchten wir uns anschauen, wie sich der Gesundheitszustand (und das Alter) der Befragten auf ihr Vertrauen in das Gesundheitswesen auswirkt. Wir nutzen dazu die folgenden Variablen:\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\nAusprägungen\n\n\n\n\nhs01\nGesundheitszustand der Befragten\n-42 = Datenfehler\n-11 = TNZ Split\n-9 = Keine Angabe\n1 = Sehr Gut\n2 = Gut\n3 = Zufriedenstellend\n4 = Weniger Gut\n5 = Schlecht\n\n\nage\nAlter der Befragten\n-32 = Nicht Generierbar\n\n\npt01\nVertrauen in das Gesundheitswesen\n-42 = Datenfehler\n-11 = TNZ Split\n-9 = Keine Angabe\n1 = Gar kein Vertrauen\n…\n7 = Großes Vertrauen\n\n\n\nInnerhalb unseres Datenmanagements schließen wir fehlerhafte und fehlende Werte der Variablen sex, agef und pt12 aus und benennen falls nötig die Variablen um:\n\ndaten &lt;- daten %&gt;%\n  filter(., hs01 &gt; 0 & pt01 &gt; 0 & age &gt; 0) %&gt;% \n  rename(., gesund = hs01,\n            trustges = pt01) %&gt;% \n  mutate(gesund = haven::as_factor(gesund),\n         trustges = as.numeric(trustges),\n         agef = dicho(age, as.num = T) %&gt;% factor(levels = c(0,1), labels = c(\"jung\", \"alt\")))"
  },
  {
    "objectID": "Skript_6.4.html#interpretation-des-outputs",
    "href": "Skript_6.4.html#interpretation-des-outputs",
    "title": "Die Varianzanalyse",
    "section": "3.3 Interpretation des Outputs",
    "text": "3.3 Interpretation des Outputs\nAls Output erhalten wir eine Tabelle, deren einzelne Bestandteile im Folgenden näher erläutert und interpretiert werden.\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Variable Gesundheitszustand(gesund). Die vier bedeutet, dass hier insgesamt eine Gruppe mit 4 anderen (= fünf Ausprägungen) verglichen wurde.\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied des Gesundheitszustandes im Bezug auf das Vertrauen in das Gesundheitswesen ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Die Effektstärke beträgt 0.006 und entspricht damit einem sehr kleinen Effekt (Cohen, 1988). Im Datenbeispiel hat demnach der Gesundheitszustand (und damit auch unser Gesamtmodell) eine Erklärkraft von lediglich 0.6 Prozent für Unterschiede im Vertrauen auf das Gesundheitssystem. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests."
  },
  {
    "objectID": "Skript_6.4.html#visualisierung-der-gruppenunterschiede-mittels-fehlerbalken",
    "href": "Skript_6.4.html#visualisierung-der-gruppenunterschiede-mittels-fehlerbalken",
    "title": "Die Varianzanalyse",
    "section": "3.6 Visualisierung der Gruppenunterschiede mittels Fehlerbalken",
    "text": "3.6 Visualisierung der Gruppenunterschiede mittels Fehlerbalken\nOftmals lohnt es sich, die Ergebnisse der Varianzanalyse auch graphisch darzustellen. Hierzu nutzen wir Fehlerbalkendiagramme:\n\ndaten %&gt;% \n1  ggline(x= \"gesund\",\n         y = \"trustges\",\n2         add = \"mean_ci\",\n3         title = \"Vertrauen in das Gesundheitswesen\",\n         xlab = \"Gesundheitszustand\",\n         ylab = \"Vertrauen in das Gesundheitswesen\") +\n4  rotate_x_text(45)\n\n\n1\n\nWir wählen auf der x-Achse die Variable gesund aus und auf der y-Achse die Variable trustges.\n\n2\n\nWir fügen für die jeweiligen Faktorstufen der Variablen gesund die Mittelwerte sowie Konfidenzintervalle hinzu.\n\n3\n\nWir bennenen unsere Grafik um.\n\n4\n\nDa die Labels unserer Faktorstufen sehr lang sind, drehen wir diese um 45 Grad. Dies geschieht mit der Funktion rotate_x_text() aus dem Paket ggpubr.\n\n\n\n\n\n\n\nDie Punkte in der Grafik visualisieren die Mittelwerte der einzelnen Gesundheitszustände mit Bezug auf die Variable Vertrauen in das Gesundheitswesen. Um die Punkte sind jeweils mit Strichen die Konfidenzintervalle eingezeichnet. Wenn sich die Bereich der Konfidenzintervalle nicht überschneiden, besteht ein signifikanter Mittelwertunterschied.\nDaher können wir anhand der Grafik ablesen, dass das Vertrauen in das Gesundheitswesen am höchsten bei Personen mit einem sehr guten Gesundheitszustand ist und sukkzessive abnimmt, je schlechter der Gesundheitszustand der Befragten ausfällt. Auch die Ergebnisse aus den Posthoc-Tests zeigen sich innerhalb der Grafik: Die Konfidenzintervalle von Personen mit einem sehr guten Gesundheitszustand und einem zufriedenstellenden und weniger guten überschneiden sich nicht. Insofern finden wir bei diesen Gruppen signifikante Mittelwertunterschiede.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der Varianzanalyse werden zumeist in Textform dargestellt. Dafür werden folgende Informationen benötigt:\n✅ die Mittelwerte und Standardabweichung der einzelnen Faktorstufen\n✅ der df-Wert\n✅ der F-Wert\n✅ der p-Wert\n✅ der pes-Wert\n✅ die Posthoc-Test Ergebnisse\nDas Format ist üblicherweise:\n\nBeispiel: Personen mit einem sehr guten Gesundheitszustand haben durchschnittlich ein höheres Vertrauen in das Gesundheitswesen (M = 5.13;SD=1.45) als Personen mit einem guten (M = 4.99;SD=1.32), zufriedenstellenden (M = 4.85;SD=1.37), weniger guten (M = 4.82;SD=1.48) oder schlechtem (M = 4.71;SD=1.55) Gesundheitszustand . Der Gesundheitszustand hat dabei einen signifikanten Einfluss auf das Vertrauen in das Gesundheitswesen (F3470)=5,46,p&lt;0,001. Die Effektstärke nach Cohen (1992) liegt bei alpha=0,006 und entspricht einem kleinen Effekt. Post-Hoc Paarvergleiche mit Tamhames ergaben, dass sich der Mittelwert für die Personen mit sehr guten Gesundheitszustand signifikant von den Personen mit zufriedenstellendem (p&lt;0.029) und weniger gutem (p&lt;0.0174) Zustand unterscheidet. Die anderen Gesundheitsgruppen unterscheiden sich hingegen nicht signifikant voneinander."
  },
  {
    "objectID": "Skript_6.4.html#interpretation-der-posthoc-tests-und-des-gesamtmodells",
    "href": "Skript_6.4.html#interpretation-der-posthoc-tests-und-des-gesamtmodells",
    "title": "Die Varianzanalyse",
    "section": "3.5 Interpretation der Posthoc-Tests und des Gesamtmodells",
    "text": "3.5 Interpretation der Posthoc-Tests und des Gesamtmodells\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht.\nIn unserem Beispiel sehen wir signifikante Unterschiede zwischen Personen die einen sehr guten Gesundheitszustand aufweisen und Personen, denen es gesundheitlich schlechter geht (Kategorien zufriedenstellender Gesundheitszustand, weniger guter Gesundheitszustand und je nach Test schlechter Gesundheitszustand). Dementsprechend ist der Gesundheitszustand ein signifikanter Faktor um das Vertrauen in das Gesundheitswesen vorherzusagen. Hier sind jedoch lediglich gravierende Unterschiede in der eigenen Gesundheitseinschätzung maßgeblich für Unterschiede in das Vertrauen."
  },
  {
    "objectID": "Skript_6.4.html#interpretation-des-outputs-1",
    "href": "Skript_6.4.html#interpretation-des-outputs-1",
    "title": "Die Varianzanalyse",
    "section": "4.1 Interpretation des Outputs",
    "text": "4.1 Interpretation des Outputs\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass der Gesundheitszustand erneut einen signifikanten Einfluss (p&lt;.001) auf das Vertrauen in das Gesundheitswesen hat (welcher im vorliegenden Modell etwas höher mit 0.9 Prozent erklärter Varianz ausfällt). Zusätzlich hat neben der Gesundheitszustand auch das Alter der Befragten einen signifikanten (p&lt;.001), wenngleich geringeren Einfluss auf das Vertrauen in das Gesundheitswesen. Zudem sehen wir einen signifikanten Einfluss der Interaktion von Alter und Gesundheitszustand (p = .020).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n4.1.1 Post-Hoc Tests\n\nemmeans::emmeans(fit2, specs = c(\"gesund\", \"agef\")) %&gt;% \n  pairs() \n\n contrast                                       estimate     SE   df t.ratio\n SEHR GUT jung - GUT jung                        0.26152 0.0818 3465   3.198\n SEHR GUT jung - ZUFRIEDENSTELLEND jung          0.56895 0.0971 3465   5.861\n SEHR GUT jung - WENIGER GUT jung                0.65126 0.1406 3465   4.632\n SEHR GUT jung - SCHLECHT jung                   0.73979 0.2727 3465   2.713\n SEHR GUT jung - SEHR GUT alt                    0.09591 0.1434 3465   0.669\n SEHR GUT jung - GUT alt                         0.00434 0.0871 3465   0.050\n SEHR GUT jung - ZUFRIEDENSTELLEND alt           0.13472 0.0858 3465   1.570\n SEHR GUT jung - WENIGER GUT alt                 0.17433 0.1083 3465   1.609\n SEHR GUT jung - SCHLECHT alt                    0.32668 0.1692 3465   1.931\n GUT jung - ZUFRIEDENSTELLEND jung               0.30743 0.0853 3465   3.602\n GUT jung - WENIGER GUT jung                     0.38974 0.1328 3465   2.935\n GUT jung - SCHLECHT jung                        0.47827 0.2688 3465   1.779\n GUT jung - SEHR GUT alt                        -0.16560 0.1357 3465  -1.220\n GUT jung - GUT alt                             -0.25718 0.0738 3465  -3.484\n GUT jung - ZUFRIEDENSTELLEND alt               -0.12680 0.0723 3465  -1.755\n GUT jung - WENIGER GUT alt                     -0.08719 0.0980 3465  -0.890\n GUT jung - SCHLECHT alt                         0.06517 0.1628 3465   0.400\n ZUFRIEDENSTELLEND jung - WENIGER GUT jung       0.08231 0.1427 3465   0.577\n ZUFRIEDENSTELLEND jung - SCHLECHT jung          0.17084 0.2738 3465   0.624\n ZUFRIEDENSTELLEND jung - SEHR GUT alt          -0.47303 0.1455 3465  -3.252\n ZUFRIEDENSTELLEND jung - GUT alt               -0.56461 0.0905 3465  -6.240\n ZUFRIEDENSTELLEND jung - ZUFRIEDENSTELLEND alt -0.43423 0.0892 3465  -4.868\n ZUFRIEDENSTELLEND jung - WENIGER GUT alt       -0.39462 0.1111 3465  -3.553\n ZUFRIEDENSTELLEND jung - SCHLECHT alt          -0.24226 0.1710 3465  -1.417\n WENIGER GUT jung - SCHLECHT jung                0.08853 0.2921 3465   0.303\n WENIGER GUT jung - SEHR GUT alt                -0.55535 0.1775 3465  -3.129\n WENIGER GUT jung - GUT alt                     -0.64692 0.1361 3465  -4.752\n WENIGER GUT jung - ZUFRIEDENSTELLEND alt       -0.51655 0.1353 3465  -3.818\n WENIGER GUT jung - WENIGER GUT alt             -0.47693 0.1506 3465  -3.167\n WENIGER GUT jung - SCHLECHT alt                -0.32458 0.1989 3465  -1.632\n SCHLECHT jung - SEHR GUT alt                   -0.64387 0.2935 3465  -2.194\n SCHLECHT jung - GUT alt                        -0.73545 0.2704 3465  -2.719\n SCHLECHT jung - ZUFRIEDENSTELLEND alt          -0.60507 0.2700 3465  -2.241\n SCHLECHT jung - WENIGER GUT alt                -0.56546 0.2780 3465  -2.034\n SCHLECHT jung - SCHLECHT alt                   -0.41311 0.3069 3465  -1.346\n SEHR GUT alt - GUT alt                         -0.09158 0.1390 3465  -0.659\n SEHR GUT alt - ZUFRIEDENSTELLEND alt            0.03880 0.1382 3465   0.281\n SEHR GUT alt - WENIGER GUT alt                  0.07841 0.1532 3465   0.512\n SEHR GUT alt - SCHLECHT alt                     0.23077 0.2009 3465   1.149\n GUT alt - ZUFRIEDENSTELLEND alt                 0.13038 0.0782 3465   1.666\n GUT alt - WENIGER GUT alt                       0.16999 0.1025 3465   1.659\n GUT alt - SCHLECHT alt                          0.32234 0.1655 3465   1.948\n ZUFRIEDENSTELLEND alt - WENIGER GUT alt         0.03961 0.1013 3465   0.391\n ZUFRIEDENSTELLEND alt - SCHLECHT alt            0.19197 0.1648 3465   1.165\n WENIGER GUT alt - SCHLECHT alt                  0.15236 0.1776 3465   0.858\n p.value\n  0.0453\n  &lt;.0001\n  0.0002\n  0.1688\n  0.9997\n  1.0000\n  0.8626\n  0.8440\n  0.6480\n  0.0119\n  0.0964\n  0.7484\n  0.9693\n  0.0179\n  0.7635\n  0.9968\n  1.0000\n  0.9999\n  0.9998\n  0.0384\n  &lt;.0001\n  0.0001\n  0.0141\n  0.9222\n  1.0000\n  0.0558\n  0.0001\n  0.0053\n  0.0498\n  0.8328\n  0.4609\n  0.1661\n  0.4288\n  0.5748\n  0.9428\n  0.9997\n  1.0000\n  1.0000\n  0.9795\n  0.8145\n  0.8183\n  0.6362\n  1.0000\n  0.9775\n  0.9976\n\nP value adjustment: tukey method for comparing a family of 10 estimates"
  },
  {
    "objectID": "Skript_6.4.html#interpretation-der-posthoc-tests-und-des-gesamtmodells-1",
    "href": "Skript_6.4.html#interpretation-der-posthoc-tests-und-des-gesamtmodells-1",
    "title": "Die Varianzanalyse",
    "section": "4.2 Interpretation der Posthoc-Tests und des Gesamtmodells",
    "text": "4.2 Interpretation der Posthoc-Tests und des Gesamtmodells\nDie Tabelle zu den Posthoc-Tests ist erwartungsgemäß sehr lang, da alle unsere einzelnen Gruppen miteinander verglichen werden müssen. Hier zeigt sich bereits, dass die mehrfaktorielle Varianzanalyse nur dann Sinn ergibt, wenn unsere Variablen nicht zu viele Ausprägungen aufweisen.\nErneut konzentrieren wir uns vorrangig auf die Signifikanzwerte. Wenn wir uns nur auf die junge Altersgruppe konzentrieren, erkennen wir die bereits bekannten Gruppenunterschieden aus der einfaktoriellen Varianzanalyse (zwischen Personen die einen sehr guten Gesundheitszustand aufweisen und Personen, mit einem zufriedenstellenden und weniger guten Gesundheitszustand) wieder. Des Weiteren zeigen sich signifikante Gruppenunterschiede zwischen jungen und alten Personen, die einen guten Gesundheitszustand aufweisen, sowie jungen Personen die einen zufriedenstellenen oder weniger guten Gesundheitszustand haben und alten Personen die einen sehr guten, guten, zufriedenstellenden oder weniger guten Gesundheitszustand haben.\nDementsprechend ist das Alter sowie der Gesundheitszustand ein signifikanter Vorhersagefaktor wobei sich eine Vielzahl an Gruppen signifikant unterscheiden. Insbesondere bei solch komplexen Gruppenunterschieden ist es sinnvoll, die Ergebnisse auch einmal zu visualisieren."
  },
  {
    "objectID": "Skript_6.4.html#literatur",
    "href": "Skript_6.4.html#literatur",
    "title": "Die Varianzanalyse",
    "section": "4.4 Literatur",
    "text": "4.4 Literatur\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link\n\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link"
  },
  {
    "objectID": "Skript_3.4.html#boxplots",
    "href": "Skript_3.4.html#boxplots",
    "title": "Tabellen und Grafiken in R",
    "section": "3.6 Boxplots",
    "text": "3.6 Boxplots\nEin Box-Plot, auch als Box-Whisker-Plot oder Kastengrafik bezeichnet, ist ein grafisches Darstellungsinstrument, das verwendet wird, um die Verteilung von mindestens ordinalskalierten Merkmalen übersichtlich darzustellen. Es fasst robuste Streuungs- und Lagemaße in einer einzigen Grafik zusammen. Diese Darstellung bietet auf einen Blick Informationen darüber, wo die Datenwerte liegen und wie sie über diesen Bereich verteilt sind. Dies wird erreicht, indem die sogenannte Fünf-Punkte-Zusammenfassung, bestehend aus dem Median, den beiden Quartilen und den beiden Extremwerten, in einer grafischen Form präsentiert wird.\nZunächst legen wir noch einen weiteren Data Frame an, der Infortationen zur Demokratiezufriedenheit der RespondentInnen enthält.\n\ndemokratiezefriedenheit &lt;- sample_gross %&gt;% \n  mutate(altersgruppe = cut(alter, \n                           breaks = c(0, 24, 34, 44, 54, 64, Inf),\n                           labels = c(\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\")),\n         zufriedenheit_demokratie_zusammengefasst = as_factor(case_when(\n           zufriedenheit_demokratie %in% c(\"SEHR ZUFRIEDEN\", \n                                           \"ZIEMLICH ZUFRIEDEN\", \n                                           \"ETWAS ZUFRIEDEN\") ~ \"eher zufrieden\",\n           zufriedenheit_demokratie %in% c(\"ETWAS UNZUFRIEDEN\",\n                                           \"ZIEML. UNZUFRIEDEN\",\n                                           \"SEHR UNZUFRIEDEN\") ~ \"eher unzufrieden\"))) %&gt;% \n  select(alter, altersgruppe, geschlecht, zufriedenheit_demokratie_zusammengefasst) %&gt;% \n  drop_na()\n\nDann erstellen wir ein einfaches Boxplot, welches das Alter der eher zufriedenen Personen mit dem der eher unzufriedenen vergleicht.\n\nggplot(demokratiezefriedenheit, aes(zufriedenheit_demokratie_zusammengefasst, alter)) + \n  geom_boxplot()\n\n\n\n\nDer Einsatz von Boxplots für den Gruppenverleich bietet sich oftmals an, wenn man verschiedene Lageparameter vergleichen möchte.\nNun erstellen wir einen anderen Data Frame, jetzt zum Vertrauen in gesellschaftliche Institutionen, um ein weiteres Beispiel für die Verwendung von Boxplots in den Blick zu nehmen.\n\nvertrauen_summiert &lt;- sample_gross %&gt;% \n  rowwise() %&gt;% \n  mutate(vertrauen_gesamt = sum(across(starts_with(\"vertrauen_\")))) %&gt;% \n  select(alter, geschlecht, entwicklung_kriminalitaet, vertrauen_gesamt) %&gt;% \n  ungroup() %&gt;% drop_na()\n\n\nggplot(vertrauen_summiert, aes(entwicklung_kriminalitaet, vertrauen_gesamt)) +\n  geom_boxplot() + geom_jitter(alpha = 0.3) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Kriminalitätseinschätzung und Vertrauen in gesellschaftliche Institutionen\") + \n  xlab(\"Kriminalitätseinschätzung\") + ylab(\"Vertrauen in gesellschaftliche Institutionen\")\n\n\n\n\nWir speichern unser Plot zudem als PDF.\n\nggsave(\"Kriminalität_und_Vertrauen.pdf\")"
  },
  {
    "objectID": "Skript_4.1.html#data-management",
    "href": "Skript_4.1.html#data-management",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "2.1 Data Management",
    "text": "2.1 Data Management\nIm Folgenden werden wir aus dem Allbus-2021-Datensatz ein paar Beispiele herausgreifen, um die Berechnung und Visualisierung von Häufigkeiten und Parametern zu demonstrieren. Dazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, dplyr) \n\nDann legen wir den Visualisierungshintergrund fest:\n\ntheme_set(theme_classic()) \n\nNun laden wir den Allbus-Datensatz:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nBei den Häufigkeiten beschränken wir uns auf einen nominal skalierten Datensatz: Das Geschlecht (“sex”).\nAnschließend konvertieren wir die Daten zu Zahlenwerten und entfernen fehlerhafte Daten:\n\n1allbus_messniveau_bsp = daten %&gt;%\n2  select(\"sex\") %&gt;%\n3  na.omit() %&gt;%\n4  mutate(sex = haven::as_factor(sex))\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz allbus_messniveau_bsp,\n\n2\n\nMit select wählen wir die Variable sexaus\n\n3\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n4\n\nWir kodieren die Variable sex als Faktor und übernehmen die Label."
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-der-normalverteilung",
    "href": "Skript_4.4.html#visualisierung-der-normalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "3.1 Visualisierung der Normalverteilung",
    "text": "3.1 Visualisierung der Normalverteilung\nDie folgenden vier Codebeispiele ergeben Visualisierungen der Normalverteilung bei unterschiedlichen Stichprobengrößen von \\(n = 10\\) bis \\(n = 10000\\). Dabei wird jeweils eine Zufallsstichprobe gezogen und die Verteilung der “Messwerte” als Histogramm ausgegeben. Je größer die Stichprobe, desto stärker sollte sich dabei die Verteilung der Zufallswerte der Dichtefunktion der Normalverteilung annähern, die hier rot dargestellt ist. Mittelwert und Standardabweichung sind so gewählt, dass die Standardnormalverteilung herauskommt. Wiederholen Sie die einzelnen Beispiele mehrmals, dann sollte jedesmal eine andere Stichproben-Verteilung herauskommen:"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-der-normalverteilung",
    "href": "Skript_4.4.html#interpretation-der-normalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "3.2 Interpretation der Normalverteilung",
    "text": "3.2 Interpretation der Normalverteilung\nAus der Visualisierung der unterschiedlichen Stichprobengrößen sollte ersichtlich geworden sein, warum ausreichend große Stichproben in der Statistik so wichtig sind. Erst ab einer ausreichend großen Anzahl Messwerten kann überhaupt sicher gesagt werden, ob die Daten einer bestimmten Verteilung folgen.\nWenn man weiß, dass ein bestimmtes Merkmal normalverteilt ist, kann man das nutzen, um mit relativ wenig Aufwand Berechnungen anzustellen. Angenommen, für unsere Einkommensdaten träfe das auch zu, dann könnte man z.B. mithilfe der Normalverteilung ausrechnen, wieviel Prozent der Population theoretisch über maximal 1500 Euro Netto-Einkommen im Monat verfügen. Dazu verwenden wir die Verteilungsfunktion der Normalverteilung, da wir an einem kumulierten Wert interessiert sind, nämlich allen möglichen Einkommenswerten bis maximal 1500 Euro. Wir übergeben der Funktion Mittelwert und Standardabweichung unserer Daten sowie die besagte Einkommensobergrenze:\n\nincome_mean = mean(allbus_df$Einkommen) \nincome_sd = sd(allbus_df$Einkommen)\nincome_median = median(allbus_df$Einkommen)\n# Die Verteilungsfunktion der Normalverteilung: \"p\" + \"norm\": \npnorm(1500, mean=income_mean, sd=income_sd) \n\n[1] 0.2790626\n\n\nWir bekommen als Ergebnis ca. 0.278. Das vergleichen wir mit dem 27,8%-Quantil unserer Messwerte:\n\nquantile(allbus_df$Einkommen, probs = c(0.278))\n\n27.8% \n 1500 \n\n\nZur Erinnerung: Das 27,8%-Quantil teilt die untersten 27,8% vom Rest der Messwerte. Interessanterweise liegt die Grenze genau bei 1500 Euro, was exakt der Vorhersage entspricht. Die Übereinstimmung wird aber deutlich schwächer, wenn wir das mit dem (100-27,8)%-Quantil vergleichen:\n\nq = quantile(allbus_df$Einkommen, probs = c(1 - 0.278))\nq\n\n  72.2% \n2967.56 \n\n\n\npnorm(2977.378, mean=income_mean, sd=income_sd)\n\n[1] 0.6301709\n\n\nUnsere Messwerte ergeben für das 72,2%-Quantil einen Wert von ca. 2977 Euro. Kalkulieren wir für das selbe Quantil bei der Normalverteilung das zu erwartende Einkommen, werden uns dagegen ca. 3393 Euro angegeben:\n\nqnorm(0.722, mean=income_mean, sd=income_sd)\n\n[1] 3390.184\n\n\nDie Datenlage weicht folglich um ca. 416 Euro von der theoretischen Annahme ab und fällt deutlich geringer aus. Ein Umstand, der sich auch grafisch widerspiegelt, wenn wir die Normalverteilung in unser Histogramm einzeichnen: Die blaue Kurve markiert den Median als Mittelwert, die rote das arithmetische Mittel, die gelbe das geometrische Mittel.\n\n\n\n\n\nMan sieht darin zweierlei: Zum einen gibt es zwar eine gewisse Übereinstimmung, aber doch auch deutliche Unterschiede zwischen den Messwerten und der Normalverteilung und zum zweiten wirken Median und geometrisches Mittel etwas genauer als das arithmetische Mittel. Hier zeigt sich die stärkere Robustheit des Medians gegenüber Ausreißern."
  },
  {
    "objectID": "Skript_4.4.html#überprüfung-der-normalverteilung-q-q-plot",
    "href": "Skript_4.4.html#überprüfung-der-normalverteilung-q-q-plot",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "3.3 Überprüfung der Normalverteilung Q-Q-Plot",
    "text": "3.3 Überprüfung der Normalverteilung Q-Q-Plot\nEine andere Möglichkeit, die Daten mit visuellen Methoden auf Normalverteilung zu prüfen, ist ein sogenannter Q-Q-Plot. Dabei werden die Quantile der theoretischen Verteilung auf einer Achse aufgetragen und die dazu korrespondierenden Quantile der empirischen Daten auf der anderen. Sind die Daten normalverteilt, sollten die Punkte im Plot alle sehr dicht an einer gemeinsamen Linie liegen. Glücklicherweise gibt es auch für diesen Plot eine Funktion:\n\nqqnorm(allbus_df$Einkommen) # Erstellen des Q-Q-Plots\nqqline(allbus_df$Einkommen) # Einfügen der Linie, auf der die Punkte liegen sollten\n\n\n\n\nWie leicht zu sehen ist, weichen die Daten sehr stark von der Linie ab. Der Bereich unterhalb von ca. 3000 Euro Einkommen scheint zumindest teilweise als normalverteilt interpretierbar zu sein, während für höhere Werte die Ergebnisse massiv abweichen. Allerdings muss auch dazugesagt werden, dass ab 5000 Euro aufwärts die Anzahl an Messwerten deutlich abnimmt."
  },
  {
    "objectID": "Skript_4.4.html#überprüfung-der-normalverteilung-mit-stat.-testverfahren",
    "href": "Skript_4.4.html#überprüfung-der-normalverteilung-mit-stat.-testverfahren",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "3.4 Überprüfung der Normalverteilung mit stat. Testverfahren",
    "text": "3.4 Überprüfung der Normalverteilung mit stat. Testverfahren\nEine weitere Möglichkeit, auf Normalverteilung zu testen, sind der Kolmogorov-Smirnov-Test und der Shapiro-Wilk-Test. Diese gehen von der Null-Hypothese aus, dass die Daten normalverteilt sind. Wenn der p-Wert also nahe bei 1 liegt, kann man davon ausgehen, dass die Hypothese bestätigt ist. Wenn der p-Wert gegen 0 geht, deutet das darauf hin, dass die Daten nicht normalverteilt sind. Allerdings sind beide Verfahren nicht unproblematisch, da sie mit zunehmender Stichprobengröße immer anfälliger für Ausreißer werden und die Null-Hypothese eher ablehnen. Der Shapiro-Wilk-Test wird vor allem bei kleinen Stichproben (n &lt; 50) eingesetzt.\nZur Demonstration der R-Funktionalitäten sind die beiden Tests hier aufgeführt, wobei die Größe der Stichprobe eine Ablehnung der Null-Hypothese erwarten lässt. Zunächst der Kolmogorov-Smirnov-Test. Es werden an die Funktion übergeben: Die Stichprobe, die Art der Verteilung, auf die getestet werden soll (hier: die Normalverteilung), der Mittelwert sowie die Standardabweichung der Stichprobe:\n\nks.test(allbus_df$Einkommen, \"pnorm\", mean=mean(allbus_df$Einkommen), sd=sd(allbus_df$Einkommen))\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  allbus_df$Einkommen\nD = 0.14579, p-value = 3.442e-15\nalternative hypothesis: two-sided\n\n\nDie Warnung macht uns darauf aufmerksam, dass manche Messwerte im Datensatz mehrfach vorkommen. Der p-Wert ist extrem klein, es wird also angenommen, dass die Daten nicht normalverteilt sind.\nAnalog dazu der Shapiro-Wilk-Test:\n\nshapiro.test(allbus_df$Einkommen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_df$Einkommen\nW = 0.83575, p-value &lt; 2.2e-16\n\n\nAuch hier ist der p-Wert extrem klein, weshalb auch hier die Annahme abgelehnt wird, dass die Daten normalverteilt sind.\nUm die Problematik dieser beiden Tests zu demonstrieren, wird im folgenden Code-Beispiel eine kleine zufällige Teil-Stichprobe aus dem Einkommens-Datensatz gezogen. Führen Sie es mehrmals aus und schauen Sie sich an, wie stark der p-Wert schwankt:\n\nsample_einkommen = sample(allbus_df$Einkommen, size=20)\nks.test(sample_einkommen, \"pnorm\", mean=mean(allbus_df$Einkommen), sd=sd(allbus_df$Einkommen))\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  sample_einkommen\nD = 0.23576, p-value = 0.2162\nalternative hypothesis: two-sided\n\nshapiro.test(sample_einkommen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sample_einkommen\nW = 0.88422, p-value = 0.02108"
  },
  {
    "objectID": "Skript_5.2.html#auswahl-der-variablen-für-die-faktorenanalyse",
    "href": "Skript_5.2.html#auswahl-der-variablen-für-die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "1.1 Auswahl der Variablen für die Faktorenanalyse",
    "text": "1.1 Auswahl der Variablen für die Faktorenanalyse\nDie Variablen werden aufgrund ihrer Nützlichkeit als Indikatoren für die zu untersuchende latente Variable ausgewählt. Entsprechend ist es wichtig, dass die Variablen inhaltliche, diskriminante und konvergente Validität aufweisen. Etwas vereinfacht ausgedrückt sollten die Indikatoren über eine inhaltliche Passung zur latenten Variable verfügen, möglichst gut von anderen latenten Variablen abgrenzbar und mit mehreren unterschiedlichen Arten der Messung nachweisbar sein.\nIn unserem Fall möchten wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nFür die statistische Identifizierung einer latenten Variablen bzw. eines Faktors werden mindestens drei gemessene Variablen benötigt, obwohl mehr Indikatoren vorzuziehen sind. Es werden beispielsweise auch vier bis sechs Indikatoren pro Faktor empfohlen. Im Allgemeinen funktioniert die EFA besser, wenn jeder Faktor überdeterminiert ist (d. h. es werden mehrere gemessene Variablen von der zu entdeckenden latenten Variable bzw. Faktor beeinflusst). Unabhängig von der Anzahl sollten Variablen, die voneinander abhängig sind, nicht in eine Faktorenanalyse einbezogen werden.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nNeben der Auswahl der Variablen bzw. Indikatoren müssen auch die Fälle (in unserem Fall die Anzahl der befragten Personen) festgelegt werden. Hier sollten wir uns zunächst fragen, ob die Stichprobe der Teilnehmer:innen in Bezug auf die gemessenen Indikatoren sinnvoll ist? Handelt es sich um eine repräsentative Stichprobe? Bei dem Allbus ist das der Fall und entsprechend können wir davon ausgehen, dass wir eine passende Stichprobe für die Durchführung eine Faktorenanalyse vorliegen haben."
  },
  {
    "objectID": "Skript_5.2.html#varianz",
    "href": "Skript_5.2.html#varianz",
    "title": "Die Faktorenanalyse",
    "section": "3.1 Varianz",
    "text": "3.1 Varianz\nWir sollten sichergehen, dass die Daten aus unserer Stichprobe ausreichend varrieren. Wir werfen hierfür ein Blick in die Daten.\n\n1colors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n1\n\nVisuelle Überprüfung mit einem Histogram für die erste Variable Ver_Gesundheitswesen. Die restlichen Variablen sollten auch überprüft werden."
  },
  {
    "objectID": "Skript_5.2.html#linearität",
    "href": "Skript_5.2.html#linearität",
    "title": "Die Faktorenanalyse",
    "section": "3.2 Linearität",
    "text": "3.2 Linearität\nDer Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Variablen. Wenn die tatsächliche Beziehung nicht linear ist, dann verringert sich der Wert von r. Wir können auf Linearität u.a. visuell durch das Betrachten der Daten mittel Streudiagramm prüfen.\n\n1ggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen, y = Ver_BVerfG)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"darkgreen\", size = 1)\n\n\n1\n\nVisuelle Überprüfung mit einem Streudiagramm für die erste Variable Ver_Gesundheitswesen & Ver_BVerfG. Die restlichen Variablen sollten auch überprüft werden."
  },
  {
    "objectID": "Skript_5.2.html#normalverteilung",
    "href": "Skript_5.2.html#normalverteilung",
    "title": "Die Faktorenanalyse",
    "section": "3.3 Normalverteilung",
    "text": "3.3 Normalverteilung\nDer Pearson-Korrelationskoeffizient setzt eine Normalverteilung voraus. Allerdings finden sich in der Realität fast nie perfekt normalverteilte Daten. Schiefe und Kurtosis sind besonders einflussreich die Ergebnisse der Faktorenanalyse und können im Extremfall artefaktische Ergenbnisse erzeugen.\n\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n2\n\nStatistische Überprüfung mittels Shapiro Wilk Test für die erste Variable Ver_Gesundheitswesen. Ein p-Wert unter 0.05 = keine Normalverteilung und ein p-Wert über 0.05 = Normalverteilung\n\n\n\n\n\n\n2shapiro.test(allbus_vertrauen$Ver_Gesundheitswesen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_vertrauen$Ver_Gesundheitswesen\nW = 0.92081, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "Skript_5.2.html#messniveau-der-variablen",
    "href": "Skript_5.2.html#messniveau-der-variablen",
    "title": "Die Faktorenanalyse",
    "section": "3.4 Messniveau der Variablen",
    "text": "3.4 Messniveau der Variablen\nBei Pearson-Korrelationen wird davon ausgegangen, dass normalverteilte Variablen auf Intervall- oder Verhältnisskalen gemessen werden, d. h. es handelt sich um kontinuierliche Daten mit gleichen Intervallen. Diese Eigenschaften treffen nicht auf ordinale (bspw. Kategorien) oder dochotome (bspw. Wahr-Falsch-Items) Variablen zu, was sich negativ auf Pearson-Korrelationskoeffizieten auswirkt und zu verzerrten Ergebnissen führen kann. Allerdings ist ein beträchtlicher Teil der Daten, mit denen wir zu tun haben, ordinal oder dichotom skaliert, um auch mit diesen Daten arbeiten zu können nutzen wir die polychorische Korrelation, welche robuster Nicht-Normalverteilung ist."
  },
  {
    "objectID": "Skript_5.2.html#fehlende-werte",
    "href": "Skript_5.2.html#fehlende-werte",
    "title": "Die Faktorenanalyse",
    "section": "3.5 Fehlende Werte",
    "text": "3.5 Fehlende Werte\nIn jeder Studie sollten wir die Anzahl und die Art der fehlenden Werte sowie die Gründe und die Methoden für den Umgang mit diesen Daten angegeben werden.\n\nallbus_vertrauen = allbus_vertrauen %&gt;%   \n1  mutate(across(Ver_Gesundheitswesen:Ver_EU_Par, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n2  na.omit()\n\n\n1\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter. Während %in% angibt, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n2\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz"
  },
  {
    "objectID": "Skript_5.2.html#ausreißer",
    "href": "Skript_5.2.html#ausreißer",
    "title": "Die Faktorenanalyse",
    "section": "3.6 Ausreißer",
    "text": "3.6 Ausreißer\nWir sollten Ausreißer identifizieren und im Zweifel von der Analyse ausschließen, da diese zu einer Verzerrung der Ergebnisse führen können. Zu den Methoden zur Erkennung von Ausreißern gehören Boxplots und Streudiagramme für einzelne Variablen sowie der Mahalanobis-Abstand für mehrere Variablen.\n\n1boxplot(allbus_vertrauen)\n\n\n1\n\nErstellen des Boxpltos"
  },
  {
    "objectID": "Skript_5.2.html#korrelation-der-variablen-untereinander",
    "href": "Skript_5.2.html#korrelation-der-variablen-untereinander",
    "title": "Die Faktorenanalyse",
    "section": "3.7 Korrelation der Variablen untereinander",
    "text": "3.7 Korrelation der Variablen untereinander\nWir sollten trotz unserer guten Datengrundlage nochmals prüfen, ob die gemessenen Variablen ausreichend miteinander korreliert sind, um eine Faktorenanalyse zu rechtfertigen. Eine Korrelation zwischen zwei Variablen gibt an, ob und wie stark ein Zusammenhang zwischen den beiden Variablen besteht. An dieser Stelle ist es wichtig, sich zu merken, dass eine Korrelation die Stärke eines Zusammenhangs angibt. Eine genauere Erklärung findet ihr im Kapitel 7. Zunächst können wir einen Blick in die Korrelationsmatrix werfen - eine beträchtliche Anzahl von Korrelationen sollte ±.30 überschreiten. Alternativ können wir ein objektiveren Test der Faktorfähigkeit der Korrelationsmatrix durchführen. Hierfür greifen wir auf den Sphärizitätstest nach Bartlett (1954) zurück.\n\n1htmlTable(round(cor(allbus_vertrauen), digits = 3))\n\n\n1\n\nDie htmlTable Funktion ermöglicht uns eine schönere Darstellung der Tabelle, round rundet die Werte auf die von uns mit digits = 3 festgelegten drei Nachkommastellen, während wir mit cor die Korrelationen für die Werte in unserem Datensatz allbus_vertrauen berechnen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\nVer_Gesundheitswesen\n1\n0.435\n0.446\n0.375\n0.184\n0.21\n0.384\n0.337\n0.334\n0.37\n0.43\n0.413\n0.349\n0.359\n0.357\n\n\nVer_BVerfG\n0.435\n1\n0.629\n0.397\n0.148\n0.223\n0.635\n0.385\n0.424\n0.474\n0.561\n0.45\n0.429\n0.456\n0.476\n\n\nVer_Bundestag\n0.446\n0.629\n1\n0.553\n0.267\n0.323\n0.571\n0.483\n0.487\n0.42\n0.816\n0.445\n0.677\n0.656\n0.661\n\n\nVer_Verwaltung\n0.375\n0.397\n0.553\n1\n0.274\n0.306\n0.471\n0.361\n0.377\n0.384\n0.485\n0.422\n0.472\n0.456\n0.455\n\n\nVer_kath_Kirche\n0.184\n0.148\n0.267\n0.274\n1\n0.699\n0.23\n0.2\n0.202\n0.138\n0.261\n0.219\n0.301\n0.288\n0.284\n\n\nVer_evan_Kirche\n0.21\n0.223\n0.323\n0.306\n0.699\n1\n0.271\n0.258\n0.279\n0.2\n0.327\n0.27\n0.35\n0.328\n0.325\n\n\nVer_Justiz\n0.384\n0.635\n0.571\n0.471\n0.23\n0.271\n1\n0.356\n0.418\n0.453\n0.56\n0.53\n0.48\n0.534\n0.541\n\n\nVer_TV\n0.337\n0.385\n0.483\n0.361\n0.2\n0.258\n0.356\n1\n0.714\n0.378\n0.49\n0.351\n0.492\n0.411\n0.415\n\n\nVer_Zeitung\n0.334\n0.424\n0.487\n0.377\n0.202\n0.279\n0.418\n0.714\n1\n0.488\n0.502\n0.365\n0.489\n0.456\n0.461\n\n\nVer_Uni\n0.37\n0.474\n0.42\n0.384\n0.138\n0.2\n0.453\n0.378\n0.488\n1\n0.467\n0.409\n0.368\n0.43\n0.435\n\n\nVer_Regierung\n0.43\n0.561\n0.816\n0.485\n0.261\n0.327\n0.56\n0.49\n0.502\n0.467\n1\n0.494\n0.719\n0.699\n0.694\n\n\nVer_Polizei\n0.413\n0.45\n0.445\n0.422\n0.219\n0.27\n0.53\n0.351\n0.365\n0.409\n0.494\n1\n0.414\n0.374\n0.37\n\n\nVer_Parteien\n0.349\n0.429\n0.677\n0.472\n0.301\n0.35\n0.48\n0.492\n0.489\n0.368\n0.719\n0.414\n1\n0.707\n0.696\n\n\nVer_Kom_EU\n0.359\n0.456\n0.656\n0.456\n0.288\n0.328\n0.534\n0.411\n0.456\n0.43\n0.699\n0.374\n0.707\n1\n0.957\n\n\nVer_EU_Par\n0.357\n0.476\n0.661\n0.455\n0.284\n0.325\n0.541\n0.415\n0.461\n0.435\n0.694\n0.37\n0.696\n0.957\n1\n\n\n\n\n\nNoch zu prüfen ist die Korrelation der Items miteinander, hierfür nehmen wir den Bartlett Test.\n\n1cortest.bartlett(allbus_vertrauen)\n\n\n1\n\nWir führen den Bartlett-Test durch\n\n\n\n\n$chisq\n[1] 33164.76\n\n$p.value\n[1] 0\n\n$df\n[1] 105\n\n\nBei großen Stichprobenumfängen, wie in unserem Fall mit dem Allbus, reagiert der Bartlett-Test selbst auf geringfügige Abweichungen vom Zufallsprinzip empfindlich, so dass seine Ergebnisse durch ein Maß für die Stichprobenadäquanz ergänzt werden sollten. Das Kaiser-Meyer-Olkin (KMO; Kaiser, 1974) Maß für die Stichprobenadäquanz ist das Verhältnis von Korrelationen und partiellen Korrelationen, das das Ausmaß widerspiegelt, in dem Korrelationen eine Folge der über alle Variablen geteilten Varianz sind und nicht der von bestimmten Variablenpaaren geteilten Varianz.\n\n1KMO(allbus_vertrauen)\n\n\n1\n\nWir lassen uns das KMO berechnen\n\n\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = allbus_vertrauen)\nOverall MSA =  0.89\nMSA for each item = \nVer_Gesundheitswesen           Ver_BVerfG        Ver_Bundestag \n                0.96                 0.91                 0.91 \n      Ver_Verwaltung      Ver_kath_Kirche      Ver_evan_Kirche \n                0.95                 0.73                 0.78 \n          Ver_Justiz               Ver_TV          Ver_Zeitung \n                0.94                 0.87                 0.88 \n             Ver_Uni        Ver_Regierung          Ver_Polizei \n                0.94                 0.92                 0.94 \n        Ver_Parteien           Ver_Kom_EU           Ver_EU_Par \n                0.97                 0.83                 0.83 \n\n\nKMO-Werte reichen von 0,00 bis 1,00 und können sowohl für die gesamte Korrelationsmatrix als auch für jede gemessene Variable berechnet werden. Insgesamt sind KMO-Werte ≥.70 erwünscht und Werte unter .50 werden im Allgemeinen als inakzeptabel angesehen. In diesem Fall ist die Korrelationsmatrix nicht faktoriell."
  },
  {
    "objectID": "Skript_5.2.html#methode-der-schätzung",
    "href": "Skript_5.2.html#methode-der-schätzung",
    "title": "Die Faktorenanalyse",
    "section": "4.1 Methode der Schätzung",
    "text": "4.1 Methode der Schätzung\nNachdem wir die Faktorenanalyse (EFA) als bevorzugtes Modell festgelegt haben, müssen wir noch die Methode zur Schätzung (Extraktion) des Faktorenmodells auswählen. Konkret suchen wir ein mathematischen Verfahren, dass die Beziehungen zwischen den gemessenen Variablen und den Faktoren (d. h. die Regression der gemessenen Variablen auf die gemeinsamen Faktoren) möglichst genau schätzt.\nWir möchten kurz anmerken, dass der mathematische Hintergrund an dieser Stelle des Kurses noch nicht so wichtig ist, da hier einige Grundlagen erst imd Kapitel 7 erklärt werden. Trotzdem macht es unserer Einschätzung nach Sinn die Begriffe bereits zu kennen und deren Vor- und Nachteile bennen zu können.\nEs existieren eine ganze Reihe von unterschiedlichen Schätzmethoden, von denen zwei Methoden am häufigsten angewendet werden. (1) Die ML-Schätzung (Maximum Liklelihood) beruht auf der Normalverteilung und ist entsprechend empfindlicher multivariater Normalität und erfordert meistens einen größere Stichprobe (mehr Fälle). (2) Die PA (wird auch als Hauptfaktoren, MINRES oder OLS bezeichnet) ist im Gegenzug dazu eine Methode der kleinsten Quadrate, welche keine Annahmen über Verteilungen trifft. PA nutzt hier eine wiederholte Zwischenschätzung, welche eine bessere Schätzung der Gemeinsamkeit ermöglicht und wiederholt diese bis eine zufriedenstellende Lösung erreicht ist.\nDie PA eignet sich als Methode der Schätzung insbesondere dann, wenn der Zusammenhang zwischen den gemessenen Variablen und den Faktoren relativ schwach sind (≤.40), der Stichprobenumfang relativ klein ist (≤300), die multivariate Normalität verletzt ist oder wenn die Anzahl der den gemessenen Variablen zugrunde liegenden Faktoren falsch spezifiziert ist. Im Gegensatz dazu ist eine ML-Schätzung besser geeignet, wenn die Beziehungen zwischen Faktoren und Variablen stark sind (&gt;.40), der Stichprobenumfang groß ist, multivariate Normalität erreicht wird und die Anzahl der Faktoren korrekt angegeben ist.\nWir können für unser Beispiel weiterhin die Maximum-likelihood Faktorenanalyse aus dem psych-Paket mit der Funktion principal verwenden, da wir in unserem Fall die entsprechenden Voraussetzungen in Bezug auf den Stichprobenumfang, die Stärke der Beziehung, sowie der Anzahl der Faktoren erfüllen. Für dieses Beispiel nutzen wir trotz der nicht perfekten Normalverteilung die ML Methode, alternativ könnten die PA Methode als robustere Variante nehmen."
  },
  {
    "objectID": "Skript_5.2.html#anzahl-der-faktoren",
    "href": "Skript_5.2.html#anzahl-der-faktoren",
    "title": "Die Faktorenanalyse",
    "section": "4.2 Anzahl der Faktoren",
    "text": "4.2 Anzahl der Faktoren\nWie bereits bei der ML-Schätzung angedeutet, müssen wir die Anzahl der Faktoren festlegen. Hierfür müssen wir die Anzahl der Faktoren für die weitere Analyse festlegen. Wir erreichen das indem wir mehrere Modelle schätzen und somit Rückschlüsse auf ein optimales Modell mit der für uns passenden Anzahl an Faktoren ziehen. Vereinfacht können wir uns das Auswringen eines nassen Handtuchs vorstellen, bei der der erste Faktor die meiste Varianz extrahiert - ergo die größte Mene an Wasser - und die nachfolgenden Faktoren sukzessive kleinere Anteile der Varianz extrahieren. Auf diese Art und Weise können wir eine Schätzung des optimalen Modells vornehmen.\nWir verwenden hierfür die nfactors-Funktion, welche uns mehere Schätzungen ausgibt.\n\n1nfactors(allbus_vertrauen, rotate = \"varimax\", fm=\"mle\")\n\n\n1\n\nMit der nfactors Funktion können wir verschiedene Schätzungen durchführen. Hierfür wählen wir ein Rotation mit rotate aus, sowie eine Methode der Schätzung mittels fm\n\n\n\n\n\n\n\n\nNumber of factors\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.88  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.9  with  3  factors\nThe Velicer MAP achieves a minimum of 0.04  with  1  factors \nEmpirical BIC achieves a minimum of  -125.8  with  6  factors\nSample Size adjusted BIC achieves a minimum of  -26  with  8  factors\n\nStatistics by number of factors \n   vss1 vss2   map dof   chisq     prob sqresid  fit RMSEA  BIC SABIC complex\n1  0.88 0.00 0.035  90 1.0e+04  0.0e+00     7.0 0.88 0.187 9520  9806     1.0\n2  0.57 0.90 0.040  76 5.0e+03  0.0e+00     5.8 0.90 0.141 4348  4590     1.5\n3  0.62 0.90 0.044  63 3.0e+03  0.0e+00     3.8 0.93 0.120 2514  2714     1.5\n4  0.51 0.85 0.048  51 1.6e+03 1.8e-308     3.0 0.95 0.098 1221  1383     1.7\n5  0.38 0.71 0.061  40 6.0e+02 1.2e-100     2.4 0.96 0.066  275   402     2.0\n6  0.38 0.68 0.076  30 3.0e+02  6.2e-46     2.2 0.96 0.053   56   151     2.1\n7  0.34 0.58 0.101  21 1.5e+02  1.1e-21     2.0 0.97 0.044  -19    48     2.4\n8  0.32 0.57 0.138  13 3.8e+01  3.1e-04     1.3 0.98 0.024  -67   -26     2.5\n9  0.32 0.55 0.164   6 9.6e+00  1.4e-01     1.4 0.98 0.014  -39   -20     2.6\n10 0.31 0.52 0.164   0 6.0e+00       NA     1.6 0.97    NA   NA    NA     2.7\n11 0.29 0.53 0.240  -5 7.4e-02       NA     1.4 0.98    NA   NA    NA     2.7\n12 0.29 0.48 0.412  -9 2.6e-06       NA     1.4 0.98    NA   NA    NA     2.6\n13 0.30 0.52 0.719 -12 1.4e-08       NA     1.5 0.98    NA   NA    NA     2.7\n14 0.29 0.52 1.000 -14 0.0e+00       NA     1.4 0.98    NA   NA    NA     2.7\n15 0.36 0.68    NA -15 9.2e+02       NA     2.5 0.96    NA   NA    NA     2.2\n    eChisq    SRMR  eCRMS eBIC\n1  5.5e+03 9.0e-02 0.0969 4749\n2  3.8e+03 7.4e-02 0.0875 3150\n3  1.5e+03 4.8e-02 0.0614 1029\n4  7.6e+02 3.3e-02 0.0480  347\n5  2.0e+02 1.7e-02 0.0279 -122\n6  1.2e+02 1.3e-02 0.0245 -126\n7  6.4e+01 9.7e-03 0.0217 -106\n8  1.0e+01 3.9e-03 0.0111  -95\n9  2.8e+00 2.0e-03 0.0085  -46\n10 2.5e+00 1.9e-03     NA   NA\n11 1.1e-02 1.2e-04     NA   NA\n12 1.3e-06 1.4e-06     NA   NA\n13 5.4e-09 8.9e-08     NA   NA\n14 1.4e-12 1.4e-09     NA   NA\n15 2.3e+02 1.9e-02     NA   NA\n\n\nIn der Ergebnissdarstellung sehen wir verschiedene Metriken, anhand derer wir uns für die Anzahl der Faktoren entscheiden können. Wir schauen uns in diesem Beispiel insbesondere Very Simple Structure (VSS) und den maximalen durchschnittlichen Teilwert (MAP) an.\nDurch VSS werden unter anderem drei Faktoren vorgeschlagen. Entsprechend schauen wir uns diese einmal kurz an, um uns ein Bild zu machen.\nIm Anschluss daran betrachten wir das Ergebnis der minimalen durchschnittlichen Teilwerte (MAP), welche als die genauesten empirischen Schätzungen für die Anzahl der beizubehaltenden Faktoren betrachtet wird. Der MAP Wert schlägt uns einen Faktor vor, entsprechend gehen wir im folgenden von einem Faktor bzw. einer lateten Variable aus."
  },
  {
    "objectID": "Skript_5.2.html#rotation-der-faktoren",
    "href": "Skript_5.2.html#rotation-der-faktoren",
    "title": "Die Faktorenanalyse",
    "section": "4.3 Rotation der Faktoren",
    "text": "4.3 Rotation der Faktoren\nBei der Durchführung der Faktorenanalyse werden sogenannte Faktorladungen ermittelt, die anzeigen, wie stark jede Variable mit den extrahierten Faktoren zusammenhängt. Während des Analyseprozesses kann es vorkommen, dass die Faktorladungen rotiert werden, um eine eindeutigere und interpretierbarere Struktur der Faktoren zu erzielen. Die Rotation der Faktorladungen ermöglicht es, die Ausprägung der Faktoren auf weniger, aber stärker ausgeprägte Variablen zu konzentrieren, was die Interpretation und Verständlichkeit der Analyseergebnisse erleichtert. Es gibt verschiedene Rotationsmethoden, wie beispielsweise die Varimax- oder Quartimax-Rotation, die je nach Ziel der Faktorenanalyse angewendet werden können. Es existieren Dutzende von analytischen Rotationsmethoden, wobei Varimax die beliebteste orthogonale Rotationsmethode ist, während Promax und Oblimin die beliebtesten schrägen Rotationsmethoden sind. Sowohl bei Promax als auch bei Oblimin können wir den Grad der Korrelation zwischen den Faktoren kontrollieren (über die Parameter Kappa bzw. Delta).\nWichtig ist, dass sich die Interpretation der Faktorladungen zwischen orthogonalen und schrägen Rotationen unterscheiden. Bei orthogonalen Lösungen können die Faktorladungen als Korrelationen zwischen gemeinsamen Faktoren und gemessenen Variablen interpretiert werden. Diese Korrelationen reichen von -1,00 bis +1,00, und der Anteil der Varianz in einer gemessenen Variablen, der durch einen gemeinsamen Faktor beigetragen wurde, kann durch Quadrieren der Faktorladung berechnet werden. Im Gegensatz dazu ergeben sich bei schrägen Lösungen zwei verschiedene Arten von Faktorladungen: Struktur- und Musterkoeffizienten. Strukturkoeffizienten können auch als Korrelationen zwischen gemeinsamen Faktoren und den gemessenen Variablen interpretiert werden. Im Gegensatz dazu sind die Musterkoeffizienten keine einfachen Faktor-Variablen-Korrelationen mehr, sondern sie ähneln standardisierten partiellen Regressionskoeffizienten. Das heißt, sie sind Korrelationen zwischen gemeinsamen Faktoren und gemessenen Variablen, nachdem der Einfluss aller anderen gemeinsamen Faktoren kontrolliert (herausgerechnet) wurde. Dementsprechend können Musterkoeffizienten den Wert von 1,00 überschreiten und können nicht quadriert werden, um den Anteil der Varianz zu ermitteln, der eindeutig auf einen gemeinsamen Faktor zurückzuführen ist.\nIn unserem Fall greifen wir auf die etablierte Rotationsmethode varimax zurück, welches wir entsprechend im R-Code spezifizieren. Zusätzlich geben wir unsere erwartete Anzahl an Faktoren an, welche wir zuvor bestimmt haben (in unserem Fall: 3 und 1).\nWir starten mit der durch VSS vorgeschlagenen Anzahl von drei Faktoren.\n\n1fit3 =  principal(allbus_vertrauen, nfactors = 3, method = \"ml\", rotate = \"varimax\")\nfit3\n\n\n1\n\nMit der principal Funktion führen wir eine Faktorenanalyse durch. Hierfür wählen wir die Anzahl an Faktoren mit nfactors aus, legen ein Rotation mit rotate sowie eine Methode der Schätzung mittels method fest\n\n\n\n\nPrincipal Components Analysis\nCall: principal(r = allbus_vertrauen, nfactors = 3, rotate = \"varimax\", \n    method = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                      RC1  RC3   RC2   h2   u2 com\nVer_Gesundheitswesen 0.65 0.14  0.11 0.45 0.55 1.2\nVer_BVerfG           0.69 0.34 -0.01 0.59 0.41 1.5\nVer_Bundestag        0.51 0.68  0.13 0.73 0.27 1.9\nVer_Verwaltung       0.51 0.36  0.24 0.45 0.55 2.3\nVer_kath_Kirche      0.09 0.15  0.90 0.85 0.15 1.1\nVer_evan_Kirche      0.18 0.18  0.88 0.83 0.17 1.2\nVer_Justiz           0.63 0.40  0.10 0.57 0.43 1.8\nVer_TV               0.60 0.28  0.13 0.46 0.54 1.5\nVer_Zeitung          0.64 0.30  0.13 0.52 0.48 1.5\nVer_Uni              0.68 0.23  0.01 0.51 0.49 1.2\nVer_Regierung        0.49 0.71  0.12 0.76 0.24 1.8\nVer_Polizei          0.69 0.16  0.17 0.53 0.47 1.2\nVer_Parteien         0.33 0.76  0.20 0.72 0.28 1.5\nVer_Kom_EU           0.25 0.89  0.14 0.88 0.12 1.2\nVer_EU_Par           0.26 0.89  0.14 0.88 0.12 1.2\n\n                       RC1  RC3  RC2\nSS loadings           4.04 3.85 1.84\nProportion Var        0.27 0.26 0.12\nCumulative Var        0.27 0.53 0.65\nProportion Explained  0.41 0.40 0.19\nCumulative Proportion 0.41 0.81 1.00\n\nMean item complexity =  1.5\nTest of the hypothesis that 3 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.06 \n with the empirical chi square  2705.49  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.98\n\n\n\n1fit1 =  principal(allbus_vertrauen, nfactors = 1, method = \"ml\", rotate = \"varimax\")\nfit1\n\n\n1\n\nMit der principal Funktion führen wir eine Faktorenanalyse durch. Hierfür wählen wir die Anzahl an Faktoren mit nfactors aus, legen ein Rotation mit rotate sowie eine Methode der Schätzung mittels method fest\n\n\n\n\nPrincipal Components Analysis\nCall: principal(r = allbus_vertrauen, nfactors = 1, rotate = \"varimax\", \n    method = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                      PC1   h2   u2 com\nVer_Gesundheitswesen 0.57 0.32 0.68   1\nVer_BVerfG           0.70 0.49 0.51   1\nVer_Bundestag        0.84 0.71 0.29   1\nVer_Verwaltung       0.66 0.43 0.57   1\nVer_kath_Kirche      0.41 0.17 0.83   1\nVer_evan_Kirche      0.48 0.23 0.77   1\nVer_Justiz           0.73 0.53 0.47   1\nVer_TV               0.64 0.41 0.59   1\nVer_Zeitung          0.68 0.46 0.54   1\nVer_Uni              0.62 0.39 0.61   1\nVer_Regierung        0.85 0.72 0.28   1\nVer_Polizei          0.63 0.39 0.61   1\nVer_Parteien         0.79 0.63 0.37   1\nVer_Kom_EU           0.81 0.66 0.34   1\nVer_EU_Par           0.81 0.66 0.34   1\n\n                PC1\nSS loadings    7.21\nProportion Var 0.48\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n with the empirical chi square  5969.7  with prob &lt;  0 \n\nFit based upon off diagonal values = 0.96"
  },
  {
    "objectID": "Skript_5.2.html#interpretation-der-ergebnisse",
    "href": "Skript_5.2.html#interpretation-der-ergebnisse",
    "title": "Die Faktorenanalyse",
    "section": "4.4 Interpretation der Ergebnisse",
    "text": "4.4 Interpretation der Ergebnisse\nBei der Betrachtung des Outputs der Faktorenanalyse beginnen wir mit den Faktorladungen. Faktorladungen geben an, wie stark jede Variable mit den extrahierten Faktoren korreliert. Hohe positive Ladungen zeigen eine starke Beziehung zwischen der Variable und dem Faktor an, während hohe negative Ladungen darauf hindeuten, dass die Variable invers mit dem Faktor zusammenhängt. Variablen mit Ladungen nahe null haben wenig oder keine Beziehung zum jeweiligen Faktor. Durch die Betrachtung dieser Ladungen können wir die die Faktoren interpretieren und auch benennen. Auf diese Art und Weise können wir Rückschlüsse auf die zugrunde liegende latente Variable ziehen.\nWenn wir die Ergebnisse unserer ersten Faktorenanalyse - mit drei Faktoren - betrachten, sehen wir, dass die Faktorenladungen in den Spalten RC1 bis RC3 unterschiedlich stark ausgeprägt sind. Wenn wir die Indikatoren für die Spalte RC2 näher betrachten fällt uns auf, dass hier vor allem die beiden Indikatoren zu dem Vertrauen in die evangelische und katholische Kirche hohe Werte aufweisen, während alle anderen Faktorladungen niedriger ausfallen. Vor diesem Hintergrund können wir davon ausgehen, dass es sich bei dem Faktor RC2, um das Vertrauen in Kirchen handelt. Wir können dieses Vorgehen für die beiden anderen Faktoren wiederholen und auf diese Weise die Faktoren interpretieren.\nFür den Fall der zweiten Faktorenanalyse - mit einem Faktor - übersteigen für alle Indikatoren die Faktorladungen über den Wert 0.3, was auf einen relativ starken und damit für uns guten Zusammenhang mit dem Faktor bzw. der latenten Variable spricht.\nWelche der beiden Lösungen besser ist hängt von unserer theoretischen Grundlage und unserer Forschungsfrage ab. Entsprechend treffen wir an dieser Stelle keine pauschale Aussage, ob die eine oder andere Lösung grundsätzlich besser ist, sondern plädieren für eine reflektierte Auseinandersetzung und Abwägung zwischen den empirischen Ergebnissen und den theoretischen Annahmen.\nZusätzlich zur Interpretation der Faktorladungen ist es auch wichtig, die Uniquenes der Faktoren zu berücksichtigen. Diese zeigen an, wie viel Varianz in den Daten von jedem extrahierten Faktor erklärt wird. In anderen Worten sagt sie also aus, wie gut die Information der Variablen in den Faktoren insgesamt erhalten geblieben ist.\n\nprint(fit1$uniquenesses, digits = 2)\n\nVer_Gesundheitswesen           Ver_BVerfG        Ver_Bundestag \n                0.68                 0.51                 0.29 \n      Ver_Verwaltung      Ver_kath_Kirche      Ver_evan_Kirche \n                0.57                 0.83                 0.77 \n          Ver_Justiz               Ver_TV          Ver_Zeitung \n                0.47                 0.59                 0.54 \n             Ver_Uni        Ver_Regierung          Ver_Polizei \n                0.61                 0.28                 0.61 \n        Ver_Parteien           Ver_Kom_EU           Ver_EU_Par \n                0.37                 0.34                 0.34 \n\n\nWenn wir auf die Uniqueness der einzelnen Indikatoren blicken, wird relativ schnell klar, dass wir eine Reihe von Indikatoren mit hohen Uniqueness-Werten haben, deren Varianz zu großen Teilen von der latenten Variable erklären wird und somit stärker als andere Indikatoren zu der latenten Variable “beitragen”.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ die Werte des Kaiser-Meyer-Olkin Kritierums (KMO)\n✅ das Ergebnis des Barlett-Tests\n✅ die Faktorladungen\n✅ die Uniqueness\n✅ die gewählte Rotations-Methode\nDas Format ist normalerweise:\n\nBeispiel: Zunächst wurde Faktorenanalyse der 15 gemessenen Indikatoren durchgeführt. Das Kaiser-Meyer-Olkin (KMO)-Maß für die Stichprobenadäquanz betrug 0,89. Dies deutet darauf hin, dass die Korrelationsmuster relativ kompakt sind und die Faktorenanalyse eindeutige und zuverlässige Faktoren ergeben sollte. Der Bartlett-Test auf Sphärizität war ebenfalls signifikant (χ2(105) = 33164.76, p &lt; .001). Dies bedeutet, dass es einige Beziehungen zwischen den untersuchten Variablen gibt. Sowohl der KMO-Test als auch der Bartlett-Test bestätigen, dass die Faktorenanalyse angemessen ist.\n\n\nDie Faktoren werden rotiert, um eine einfache Struktur zu erhalten. In diesem Fall wurde die varimax Rotationsmethode verwendet. Nach sorgfältiger Betrachtung der zusammenhängenden Variablen in der Analyse wurden dann die Faktorbezeichnung vorgeschlagen und in Tabelle 1 dargestellt. Dabei handelt es sich um Vertrauen in gesellschaftliche Institutionen. Für den Faktor wurden Faktorladungen erstellt (siehe Tabelle 1)."
  },
  {
    "objectID": "Skript_5.3.html#auswahl-der-variablen-für-die-indexbildung",
    "href": "Skript_5.3.html#auswahl-der-variablen-für-die-indexbildung",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Auswahl der Variablen für die Indexbildung",
    "text": "1.1 Auswahl der Variablen für die Indexbildung\nWir greifen natürlich auf die gleiche Datengrundlage zurück, welche wir auch für die Faktorenanalyse verwendet haben. Was in unserem Fall bedeutet, dass wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nWir bereiten die Daten entsprechend vor, indem wir die fehlenden Werte entfernen und die Variablen in numerische umwandeln.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nWir haben nun alle Daten geladen und die Variablen entsprechend vorbereitet. Wir können eigentlich mit der Indexbidlung beginnen, müssen uns allerdings davor noch entscheiden, welche Art von Index wir bilden möchten."
  },
  {
    "objectID": "Skript_7.1.html#visualisieren-des-zusammenhangs",
    "href": "Skript_7.1.html#visualisieren-des-zusammenhangs",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.1 Visualisieren des Zusammenhangs",
    "text": "3.1 Visualisieren des Zusammenhangs\nEin gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) “aufeinandergestapelt”, wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den as.factor-Befehl nutzen:\n\ndaten &lt;- daten %&gt;% \n  mutate(Konfession = haven::as_factor(Konfession),\n         sex = haven::as_factor(sex))\n\nggplot(daten, aes(x = Konfession, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Verteilung der Konfessionszugehörigkeit nach Geschlecht\",\n       x = \"Konfession\", y = \"Anteil\") +\n  scale_fill_manual(values = c(\"darkseagreen\",\"grey\", \"lightblue\")) +\n  rotate_x_text(45)\n\n\n\n\nDiese Visualisierung zeigt sehr schön, was wir auch schon in der Kreuztabelle ablesen konnten: Es gibt keinen deutlich sichtbaren Zusammenhang zwischen der Konfessionszugehörigkeit und dem Geschlecht - allenfalls lässt sich vermuten, dass mehr Frauen als Männer überhaupt einer Konfession angehören. Ob sich der Zusammenhang als statistisch bedeutsam erweist, können wir nun mit dem Chi-Quadrat Test prüfen."
  },
  {
    "objectID": "Skript_7.1.html#überprüfen-des-zusammenhangs",
    "href": "Skript_7.1.html#überprüfen-des-zusammenhangs",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "3.2 Überprüfen des Zusammenhangs",
    "text": "3.2 Überprüfen des Zusammenhangs\nDer Chi-Quadrat Test ist ein statistisches Verfahren, das verwendet wird, um festzustellen, ob es einen signifikanten Unterschied zwischen den beobachteten und erwarteten Häufigkeiten in einer Kreuztabelle gibt. Da wir schon eine Kreuztabelle erstellt haben, können wir die Funktion chisq.test auf unsere Kreuztabelle anwenden - das geht ganz einfach:\n\nchisq.test(kreuztabelle)\n\n\n    Pearson's Chi-squared test\n\ndata:  kreuztabelle\nX-squared = 22.946, df = 10, p-value = 0.01095"
  },
  {
    "objectID": "Skript_7.1.html#data-management-1",
    "href": "Skript_7.1.html#data-management-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.1 Data Management",
    "text": "4.1 Data Management\n\ndaten &lt;- daten %&gt;%\n  rename(Einkommensgruppe = di02a)%&gt;%\n  rename(Bildung = educ)%&gt;%\n  filter(between(Einkommensgruppe, 1, 25))%&gt;%\n  filter(between(Bildung, 1, 5))\n\nNun nutzen wir die Funktion cor.test() und spezifizieren bei “method” den Korrelationskoeefizienten, zuerst “spearman”, dann zum Vergleich auch “kendall”:"
  },
  {
    "objectID": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-spearmans-rho",
    "href": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-spearmans-rho",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.2 Berechnung der Rangkorrelation nach Spearman’s Rho",
    "text": "4.2 Berechnung der Rangkorrelation nach Spearman’s Rho\n\nresult_spearman &lt;- cor.test(daten$Einkommensgruppe, daten$Bildung, method = \"spearman\")\nprint(result_spearman)\n\n\n    Spearman's rank correlation rho\n\ndata:  daten$Einkommensgruppe and daten$Bildung\nS = 3725893693, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2799733 \n\n\nSchauen wir uns nun den Output an: Der Wert rho gibt den berechneten Korrelationskoeffizienten wieder. Der Wert von 0,28 deutet darauf hin, dass es einen positiven Zusammenhang zwischen den beiden Variablen Bildungsabschluss und Einkommensgruppe gibt.\nDer sehr kleine p-Wert (kleiner als 2.2e-16, was nahezu null ist), zeigt an, dass der beobachtete Zusammenhang zwischen den Variablen statistisch höchst signifikant ist.\nDer S-Wert (3725893693) repräsentiert die Summe der quadrierten Unterschiede zwischen den Rängen der beiden Variablen. Er ist Teil der Berechnung des Spearman’s Rho und wird für die Interpretation eigentlich nicht unbedingt benötigt.\nSchauen wir uns nun zum Vergleich das Ergebnis mit Kendall’s Tau an:"
  },
  {
    "objectID": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-kendalls-tau",
    "href": "Skript_7.1.html#berechnung-der-rangkorrelation-nach-kendalls-tau",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "4.3 Berechnung der Rangkorrelation nach Kendall’s Tau",
    "text": "4.3 Berechnung der Rangkorrelation nach Kendall’s Tau\n\nresult_kendall &lt;- cor.test(daten$Einkommensgruppe, daten$Bildung, method = \"kendall\")\nprint(result_kendall)\n\n\n    Kendall's rank correlation tau\n\ndata:  daten$Einkommensgruppe and daten$Bildung\nz = 16.019, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.2202662 \n\n\nDer z-Wert gibt an, wie viele Standardabweichungen die beobachtete Korrelation von der erwarteten Korrelation entfernt ist. Ein höherer z-Wert deutet darauf hin, dass die beobachtete Korrelation signifikant von Null abweicht. In unserem Fall ist der z-Wert sehr hoch (16.343), was darauf hindeutet, dass die beobachtete Korrelation sehr weit von Null entfernt ist.\nDer p-Wert ist die Wahrscheinlichkeit mit der die beobachtete Korrelation zufällig ist. Ein kleiner p-Wert deutet darauf hin, dass die beobachtete Korrelation sehr unwahrscheinlich ist, wenn kein Zusammenhang bestehen würde. Da unser p-Wert extrem klein ist (&lt; 2.2e-16), können wir von einem Zusammenhang ausgehen. Beide Rangkorrelationskoeffizienten liefern uns also ein ähnliches Ergebnis und zeigen einen (positiven) Zusammenhang zwischen dem Bildungsabschluss und der Einkommensgruppe."
  },
  {
    "objectID": "Skript_7.1.html#data-management-2",
    "href": "Skript_7.1.html#data-management-2",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "5.1 Data Management",
    "text": "5.1 Data Management\n\ndaten_neu &lt;- daten %&gt;%\n  filter(between(Einkommensgruppe, 1, 25))%&gt;%\n  mutate(sex = as.numeric(sex)) %&gt;%\n  filter(between(sex, 1, 3))\n\nNun erstellen wir wieder eine Kreuztabelle, um die Häufigkeiten der verschiedenen Kombinationen von Kategorien beider Variablen anzuzeigen. Damit erhalten wir einen ersten Überblick über die Verteilung der Daten entlang der Vergleichskategorie:"
  },
  {
    "objectID": "Skript_7.1.html#erstellen-einer-kreuztabelle",
    "href": "Skript_7.1.html#erstellen-einer-kreuztabelle",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "5.2 Erstellen einer Kreuztabelle",
    "text": "5.2 Erstellen einer Kreuztabelle\n\nkreuztabelle_2 &lt;- table(daten_neu$sex, daten_neu$Einkommensgruppe)\nprint(kreuztabelle_2)\n\n   \n      1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n  2   6   2  10  27  15  22  26  42  64  48  70  68 119 163 154 113  79  71 101\n  3  13  23  36  64  42  82  68 134 127  88 112 111 158 167 159 102  51  47  63\n   \n     20  21  22  23  24  25\n  2  77  48  35  32  17  16\n  3  27  16  10   7   5   6\n\n\nIm Datensatz entsprechen höhere Werte bei der variable Einkommensgruppe einem höheren Einkommen (1=bis unter 200 Euro; 2= 200 bis unter 300 Euro; 25=7.500 bis unter 10.000 Euro). Wenn wir uns nun die Kreuztabelle anschauen, können wir schon einen ersten Trend erkennen: Offenbar sind mehr Männer in höheren Einkommensgruppen als Frauen; dafür sind weniger Männer in niedrigen Einkommensgruppen. Hier deutet sich also an, dass Geschlecht und Einkommensgruppe einen Zusammenhang aufweisen.\nMit einem gestapelten Balkendiagramm können wir diese Tendenz auch visualisieren:"
  },
  {
    "objectID": "Skript_7.1.html#visualisieren-des-zusammenhangs-1",
    "href": "Skript_7.1.html#visualisieren-des-zusammenhangs-1",
    "title": "Pearson Chi-Quadrat-Test & Rangkorrelation",
    "section": "5.3 Visualisieren des Zusammenhangs",
    "text": "5.3 Visualisieren des Zusammenhangs\nEin gestapeltes Balkendiagramm ist ein Diagramm, dass die Verteilung von verschiedenen Kategorien innerhalb einer Gesamtheit farblich (nach Kategorien) differenziert darstellen kann. Dazu werden mehrere Balken für jede Kategorie (nach Farbe sortiert) “aufeinandergestapelt”, wobei die Höhe des gesamten Balkens die Gesamtsumme (je Farbkategorie) repräsentiert. Um das auszuführen, müssen wir die unsere Daten wie Faktoren behandeln, weshalb wir den as.factor-Befehl nutzen:\n\ndaten_neu &lt;- daten_neu %&gt;% \n  mutate(Einkommensgruppe = haven::as_factor(Einkommensgruppe),\n         sex = haven::as_factor(sex))\n\nggplot(daten_neu, aes(x = Einkommensgruppe, fill = sex)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Verteilung der Einkommensgruppe nach Geschlecht\",\n       x = \"Einkommensgruppe\", y = \"Anteil\") +\n  scale_fill_manual(values = c(\"darkseagreen\",\"grey\", \"lightblue\")) +\n  rotate_x_text(45)\n\n\n\n\nDiese Visualisierung zeigt sehr schön, was wir auch schon anhand der Kreuztabelle ablesen konnten: Es sind mehr Frauen als Männer in den unteren Einkommensgruppen zu finden; es sind mehr Männer als Frauen in höheren Einkommensgruppen zu finden.\nNun prüfen wir mit einem Chi-Quadrat Test, ob der beobachtete Zusammenhang auch statistisch signifikant ist. Dazu nutzen wir wieder die chisq.test-Funktion, die wir auf unsere Kreuztabelle anwenden:"
  },
  {
    "objectID": "Autoren.html#cornelius-puschmann",
    "href": "Autoren.html#cornelius-puschmann",
    "title": "Das Team stellt sich vor",
    "section": "Cornelius Puschmann",
    "text": "Cornelius Puschmann\nVita\n\n\n\n\n\nCornelius Puschmann\n\n\nCornelius Puschmann ist Professor für Kommunikations- und Medienwissenschaft mit dem Schwerpunkt Digitale Kommunikation am ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung. Nach Stationen an der Humboldt Universität zu Berlin, der Zeppelin Universität Friedrichshafen und dem Leibniz-Institut für Medienforschug | Hans Bredow Institut wurde er 2019 nach Bremen berufen. Gastwissenschaftler war Cornelius Puschmann u.a. am Oxford Internet Institute der University of Oxford und am Berkman Klein Center for Internet and Society der Harvard University, sowie am Department of Media Studies der Universität Amsterdam.\nForschungsschwerpunkte\n\nDigitale Mediennutzungsforschung\nHate Speech / Desinformation in sozialen Medien\nMethoden der automatisierten Inhaltsanalyse\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite"
  },
  {
    "objectID": "Autoren.html#stephanie-geise",
    "href": "Autoren.html#stephanie-geise",
    "title": "Das Team stellt sich vor",
    "section": "Stephanie Geise",
    "text": "Stephanie Geise\nVita\n\n\n\n\n\nStephanie Geise, © B. Köhler\n\n\nProf. Dr. habil. Stephanie Geise ist Professorin für Kommunikations- und Medienwissenschaft am ZeMKI der Universität Bremen und vertritt den Schwerpunkt Politische Kommunikation & Innovative Methoden. Ihre Forschungsschwerpunkte liegen im Bereich der politischen Kommunikation und Medienwirkung, mit einem Fokus auf visuelle und multimodale Kommunikation. Im Fokus steht die Frage, wie sich multimodale Medienbotschaften – z.B. klassische Nachrichten oder Social Media Posts – auf das politische Denken und Handeln auswirken. Zur Analyse nutzt sie häufig Kombinationen aus „klassischen” standardisierten Methoden (z.B. Befragungen, Experimentaldesigns) und innovativen computerbasierten Beobachtungsverfahren (z.B. Eyetracking, Automated Emotion Detection). Stephanie Geise leitet das von der Deutschen Forschungsgemeinschaft (DFG) geförderte Forschungsprojekt “Remixing Multimodal News Reception” und koordiniert das DFG-gefördertes Netzwerk „Potenziale und Herausforderungen der Computational Communication Science am Beispiel von Online-Protest”, dass computationale Methoden zur Analyse von Online-Protest reflektiert und weiterentwickelt. Sie leitet das SKILL-Innovation Lab „Digital Data Literacy & Analysis” am ZeMKI der Universität Bremen und hat die Kapitel zum „Überprüfen von Zusammenhängen” zum Lehrbuch beigesteuert.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nRezeptions- und Wirkungsforschung\nVisuelle und multimodale Kommunikation\nInnovative Methoden & Methodenkombinationen\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_7.3.html#data-management-1",
    "href": "Skript_7.3.html#data-management-1",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "1.1 Data Management",
    "text": "1.1 Data Management\nFür den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(Alter = age)%&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(TV_Konsum, 0, 1500))"
  },
  {
    "objectID": "Skript_7.3.html#metrisches-skalenniveau-linearität-des-zusammenhangs",
    "href": "Skript_7.3.html#metrisches-skalenniveau-linearität-des-zusammenhangs",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.1 Metrisches Skalenniveau & Linearität des Zusammenhangs",
    "text": "2.1 Metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in dem letzten Skript bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt. Hier könnt ihr noch einmal nachschauen, wie das geht."
  },
  {
    "objectID": "Skript_7.3.html#homoskedastizität-der-residuen",
    "href": "Skript_7.3.html#homoskedastizität-der-residuen",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.2 Homoskedastizität der Residuen",
    "text": "2.2 Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß - unabhängig davon, wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-Package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde. Wir müssen diese Funktion lediglich auf unser geschätzes Regressionsmodell anwenden:\n\ncheck_heteroscedasticity(model)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p &lt; .001).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift - wie im vorliegenden Fall - ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). Dann liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen. Es ist dann auch unbedingt geboten, diese Limitation anzugeben.\nWie das ganze aussieht, können wir uns auch grafisch über die plot()-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei mittleren Werten erkennbar ist, weil wir einen leicht zur Mitte geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\nIm Beispiel sehen wir aber auch, dass die Abweichung bei uns zwar vorhanden, aber nicht allzu “dramatisch” ist. Die Voraussetzung ist zwar verletzt, und das müssen wir auch berücksichtgen, aber die Regressionsanalyse ist sehr robust gegen die Verletzung ihrer Voraussetzungen. Wir können den Fehler zudem korrigieren! Und das werden wir nun auch tun :)\n\n2.2.1 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen wir nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robust gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai (2007) empfehlen. HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value              Pr(&gt;|t|)    \n(Intercept) 64.64384    5.29202  12.215 &lt; 0.00000000000000022 ***\nAlter        2.19279    0.10197  21.504 &lt; 0.00000000000000022 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# diese Variante wählen, wenn Residuen nicht normalverteilt sind \n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn wir diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen wir, dass sich die eigentlichen Koeffizienten (Estimates) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!"
  },
  {
    "objectID": "Skript_7.3.html#unabhängigkeit-der-residuen",
    "href": "Skript_7.3.html#unabhängigkeit-der-residuen",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.3 Unabhängigkeit der Residuen",
    "text": "2.3 Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation()-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.570).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0.588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!"
  },
  {
    "objectID": "Skript_7.3.html#normalverteilung-der-residuen",
    "href": "Skript_7.3.html#normalverteilung-der-residuen",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.4 Normalverteilung der Residuen",
    "text": "2.4 Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn ihr hier selbstständig weitermachen wollt - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models() auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist."
  },
  {
    "objectID": "Skript_7.3.html#ausreißer-im-modell",
    "href": "Skript_7.3.html#ausreißer-im-modell",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.5 Ausreißer im Modell",
    "text": "2.5 Ausreißer im Modell\nAusreißer in den Daten sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, können wir wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n\n\n\n\nExkurs Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\n\n\n\n\n\nEs gibt im performance-Package auch eine sehr gehaltvolle Funktion, die uns eine visuelle Inspektion der Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion können wir uns dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\nHier der Link zur Dokumentation des Performance-Packages mit weiteren Informationen: Link\n\n#check_model(model)\n#model_performance(model)\n\n\n\n\n\n\n\n\n\n\nLiteratur\n\n\n\n\n📖 Hayes, A. F.,& & Cai, L. (2007). Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722"
  },
  {
    "objectID": "Skript_7.3.html#exkurs-visuelle-inspektion-der-modellgüte-bzw.-der-modellannahmen",
    "href": "Skript_7.3.html#exkurs-visuelle-inspektion-der-modellgüte-bzw.-der-modellannahmen",
    "title": "Prüfung der Voraussetzungen bei der linearen Regression",
    "section": "2.6 Exkurs Visuelle Inspektion der Modellgüte bzw. der Modellannahmen",
    "text": "2.6 Exkurs Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr gehaltvolle Funktion, die uns eine visuelle Inspektion der Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion können wir uns dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\nHier der Link zur Dokumentation des Performance-Packages mit weiteren Informationen: Link\n\n#check_model(model)\n#model_performance(model)\n\n\n\n\n\n\n\nLiteratur\n\n\n\n\n📖 Hayes, A. F.,& & Cai, L. (2007). Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722"
  },
  {
    "objectID": "Skript_7.2.html#berechnen-der-korrelation",
    "href": "Skript_7.2.html#berechnen-der-korrelation",
    "title": "Korrelation & Regression",
    "section": "1.2 Berechnen der Korrelation",
    "text": "1.2 Berechnen der Korrelation\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\nkorrelation &lt;- daten_neu %&gt;% \n  summarize(correlation = cor(TV_Konsum, TV_Vertrauen, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1       0.114\n\n\nSchauen wir uns den (sehr übersichtlichen) Output an: Wie erhalten einen Korrelationskoeffizient von 0,113903. Das bedeutet, dass zwischen den beiden Variablen TV_Konsum und TV_Vertrauen ein schwacher positiver linearer Zusammenhang besteht. Höhere Werte in einer Variable gehen also tendenziell mit höheren Werten in der anderen Variable einher. Jedoch ist die Korrelation relativ niedrig, was darauf hindeutet, dass der Zusammenhang zwischen den beiden Variablen nicht besonders stark ist."
  },
  {
    "objectID": "Skript_7.2.html#visualisierung-der-korrelation",
    "href": "Skript_7.2.html#visualisierung-der-korrelation",
    "title": "Korrelation & Regression",
    "section": "1.3 Visualisierung der Korrelation",
    "text": "1.3 Visualisierung der Korrelation\nUm die Korrelation zwischen den Variablen “TV_Konsum” und “TV_Vertrauen” zu visualisieren, können wir mit dem ggplot2-Paket ein Scatterplot erstellen:\n\nscatterplot &lt;- ggplot(daten_neu, aes(x = TV_Konsum, y = TV_Vertrauen)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"TV Konsum\", y = \"TV Vertrauen\", title = \"Korrelation zwischen TV Konsum und Vertrauen\")\nprint(scatterplot)\n\n\n\n\nHier können wir den oben bereits ermittelten Befund noch einmal grafisch inspizieren. Der leichte positive Zusammenhang zwischen dem Vertrauen in die Insitution Fernsehen und der täglichen Fernsehnutzung zeigt sich ganz schön.\n\n\n\n\n\n\nExkurs: Übungsaufgabe Korrelationsanalyse\n\n\n\n\n\nIhr könnt zur Übung noch eine Korrelationsanalyse durchführen, um den Zusammenhang zwischen der täglichen Fernsehnutzung in Minuten (umbenannt in “TV_Konsum”) und der quasi-metrisch gemessenen Einschätzung, ob die Entwicklung der Kriminalität in Deutschland zu- oder abgenommen hat (cf03; 1=hat stark zugenommen; 5=hat stark abgenommen). Wenn wir der Kultivierungsthese folgen, vermuten wir, dass ein erhöhter Fernsehkonsum zu einer stärkeren Wahrnehmung der Kriminalität führt. Entsprechend hätten wir ins unserem Fall (wegen der gegenläufigen Kodierung der Variablen) einen negativen linearen Zusammenhang. Dazu müsst ihr zuerst wieder einen Teildatensatz mit den benötigten Variablen erzeugen, diese ggf. umbenennen und filtern. Dann ermittelt ihr die Korrelation beider Variablen.\n\n1.4 Data Management\n\ndaten_neu2 &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(Kriminalitätsprognose= cf03)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(Kriminalitätsprognose, 1, 5))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:\n\n\n1.5 Berechnung und Ausgabe des Korrelationskoeffizienten\n\nkorrelation &lt;- daten_neu2 %&gt;% \n  summarize(correlation = cor(TV_Konsum, Kriminalitätsprognose, use = \"complete.obs\"))\nprint(korrelation)\n\n# A tibble: 1 × 1\n  correlation\n        &lt;dbl&gt;\n1      -0.196\n\n\nBetrachten wir den Output: Der Korrelationskoeffizient von -0,196 deutet auf einen schwachen negativen linearen Zusammenhang zwischen der “täglichen Fernsehnutzung in Minuten” und der Einschätzung zur Entwicklung der Kriminalität in Deutschland hin.\nACHTUNG! Zur Interpretation müssen Sie nun aber auch berücksichtigen, wie die Werte der Variable gelabelt sind: Ein höherer Wert bedeutet, dass man eine Abnahme der Kriminalitätsentwicklung vermutet (1=hat stark zugenommen; 5=hat stark abgenommen). Der negative Korrelationskoeffizient bedeutet dann, dass höhere Werte in der “täglichen Fernsehnutzung in Minuten” tendenziell mit niedrigeren Werten in der Einschätzung zur Kriminalitätsentwicklung korrelieren. Oder anders gesagt: Personen, die mehr Fernsehen schauen, tendieren eher zur Annahme, dass die Kriminalität in Deutschland weniger zugenommen oder sogar abgenommen hat.\nSo oder so ist zu beachten, dass der Korrelationskoeffizient nur den linearen Zusammenhang erfasst und keine Aussagen zur Kausalitäten des Zusammenhangs macht. Korrelation ist nicht Kausalität!\nZur grafischen Illustration erstellen wir nun wieder ein Scatterplot:\n\n\n1.6 Visualisierung der Korrelation mit Hilfe eines Scatterplots\n\nscatterplot &lt;- ggplot(daten_neu2, aes(x = TV_Konsum, y = Kriminalitätsprognose)) +\n  geom_point(color = \"darkgreen\") +\n  labs(x = \"TV Konsum\", y = \"Kriminalitätsprognose\", title = \"Korrelation zwischen TV Konsum und Kriminalitätsprognose\")\nprint(scatterplot)"
  },
  {
    "objectID": "Skript_7.2.html#data-management-1",
    "href": "Skript_7.2.html#data-management-1",
    "title": "Korrelation & Regression",
    "section": "1.4 Data Management",
    "text": "1.4 Data Management\n\ndaten_neu2 &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(Kriminalitätsprognose= cf03)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(Kriminalitätsprognose, 1, 5))\n\nNun berechnen wir, ob eine Korrelation zwischen beiden Variablen vorliegt. Dazu nutzen wir die Funktion cor(). Mit dem print-Befehl geben wir uns den Wert aus:"
  },
  {
    "objectID": "Skript_7.2.html#data-management-2",
    "href": "Skript_7.2.html#data-management-2",
    "title": "Korrelation & Regression",
    "section": "2.1 Data Management",
    "text": "2.1 Data Management\nWie immer besteht der erste Schritt nun darin, die benötigten Pakete sowie den Datensatz zu laden. Für die lineare Regression kommen zwei neue Pakete hinzu: Wir laden das Paket broom, um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können sowie das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt. Das Paket see kommt außerdem dazu, weil es uns eine toolbox für die Visualisierung der Zusammenhänge bereitstellt.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, dplyr, broom, lm.beta, performance, see) \n\n1theme_set(theme_minimal())\n2options(scipen = 999)\n\n\n1\n\nVisualisierungshintergrund der Grafiken in ggplot festlegen\n\n2\n\nAnzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\n\n\n\n\nAnschließend laden wir unseren Datensatz.\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nFür den Teildatensatz bennenen wir die Variablen lm02 und age um und filtern die missings heraus (z.B. -9=Keine Angabe):\n\ndaten &lt;- daten %&gt;%\n  rename(Alter = age)%&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  filter(between(Alter, 18, 100))%&gt;%\n  filter(between(TV_Konsum, 0, 1500))"
  },
  {
    "objectID": "Skript_7.2.html#durchführung-der-einfachen-linearen-regression",
    "href": "Skript_7.2.html#durchführung-der-einfachen-linearen-regression",
    "title": "Korrelation & Regression",
    "section": "2.3 Durchführung der einfachen linearen Regression",
    "text": "2.3 Durchführung der einfachen linearen Regression\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regression prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: TV_Konsum), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (TV_Konsum) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll. Zum Schluss lassen wir uns das Modell ausgeben.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\nmodel &lt;- lm(TV_Konsum ~ Alter, data = daten) \nprint(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nCoefficients:\n(Intercept)        Alter  \n     64.644        2.193  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn nun mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter, data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-242.61  -67.44  -22.09   35.02 1292.03 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  64.6438     5.6890   11.36 &lt;0.0000000000000002 ***\nAlter         2.1928     0.1007   21.77 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 122.5 on 4927 degrees of freedom\nMultiple R-squared:  0.08778,   Adjusted R-squared:  0.0876 \nF-statistic: 474.1 on 1 and 4927 DF,  p-value: &lt; 0.00000000000000022"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-der-ergebnisse",
    "href": "Skript_7.2.html#interpretation-der-ergebnisse",
    "title": "Korrelation & Regression",
    "section": "2.4 Interpretation der Ergebnisse",
    "text": "2.4 Interpretation der Ergebnisse\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten” die abhängige Variable “TV_Konsum” durch die unabhängige Variable “Alter” zu erklären.\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nDas Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\nMit St.error wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (8,8) Prozent der Varianz der TV-Nutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Intensität der Fernsehnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (das kommt in der Realität aber fast nicht vor).\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)"
  },
  {
    "objectID": "Skript_7.2.html#interpretation-der-ergebnisse-1",
    "href": "Skript_7.2.html#interpretation-der-ergebnisse-1",
    "title": "Korrelation & Regression",
    "section": "2.7 Interpretation der Ergebnisse",
    "text": "2.7 Interpretation der Ergebnisse\nIn der Ausgabe kann ich nun die laut Modell prognostizierte Fernsehnutzung für meine Befragten entsprechend ihres Alters sehen - Befragte(r) 10 hat so z.B. eine prognostizierte Fernsehnutzung von 189 Minuten.\nNun haben wir im Rahmen unserer Befragung die TV-Nutzung der Befragten aber ja schon erhoben. Wozu dient diese Prognose dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nhead(residuals(model), 100)\n\n&lt;labelled&lt;double&gt;[100]&gt;: FERNSEHGESAMTDAUER PRO TAG IN MINUTEN\n           1            2            3            4            5            6 \n  26.9456805  -90.8615329 -124.8018509 -177.8739847  -20.5966124  -70.0779345 \n           7            8            9           10           11           12 \n-102.6202274  -33.7533320    3.5240404  -69.6326793  198.9692956 -111.6562944 \n          13           14           15           16           17           18 \n -27.9697337   56.9456805   46.6807600  -82.0903864  -31.6562944 -126.6451312 \n          19           20           21           22           23           24 \n -79.8975998  -19.8975998   75.2827349 1059.4033876    0.5364922   56.9456805 \n          25           26           27           28           29           30 \n  -3.0543195  -26.4759596  -76.2110391    7.9096136  -39.6326793  108.1745341 \n          31           32           33           34           35           36 \n -71.8254659  -90.1625203 -124.4523446    6.5115885 -117.8739847  426.1148521 \n          37           38           39           40           41           42 \n -83.4884115  146.9456805  125.7168270  -29.4635078  -17.1749722   82.5601073 \n          43           44           45           46           47           48 \n -12.5244785  -28.6687462 -138.7533320  -65.2471061  -12.6202274  -27.8739847 \n          49           50           51           52           53           54 \n   8.4394546  150.6322412   94.9220655  -66.6451312  -34.5480935  -39.6326793 \n          55           56           57           58           59           60 \n -52.8851479 -132.9697337  -65.9461186  -20.5966124  -49.8975998  -24.9821856 \n          61           62           63           64           65           66 \n -31.6562944  -14.7172651    7.9096136    2.0302663  -92.3553069    5.7168270 \n          67           68           69           70           71           72 \n -69.6326793  -36.7408801   -5.2471061  -72.6202274   84.7528939  -93.7533320 \n          73           74           75           76           77           78 \n-174.1874241  -82.7893990  126.5601073    6.5115885   52.5601073  -71.8254659 \n          79           80           81           82           83           84 \n  13.7889609  -44.0182525  -26.4759596  -20.5966124   75.9817475  -87.1749722 \n          85           86           87           88           89           90 \n   5.7168270    6.4158395  -89.4635078   18.8735466  -45.5120266    3.8735466 \n          91           92           93           94           95           96 \n   5.7168270  149.8374797  -67.0903864  -80.6923613   68.7889609  246.5115885 \n          97           98           99          100 \n -80.6923613  -16.2110391  -35.2471061  -58.6687462 \n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n   -10       TNZ: FILTER\n    -9      KEINE ANGABE\n\n\nFür unseren Fall Nummer 65 beträgt die Abweichung der Prognose von der Beobachtung -92 Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 8 Prozent nicht besonders groß ist."
  },
  {
    "objectID": "Skript_7.2.html#data-management",
    "href": "Skript_7.2.html#data-management",
    "title": "Korrelation & Regression",
    "section": "1.1 Data Management",
    "text": "1.1 Data Management\nUm das zu prüfen, müssen wir zunächst wieder unsere Pakete und Allbus-Daten laden und aufbereiten. Los geht’s!\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, haven, dplyr) \ntheme_set(theme_classic())\n\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nDann erzeugen wir einen neuen Teildatensatz, benennen die benötigten Variablen in “TV_Konsum” und “TV_Vertrauen” um und selektieren die fehlende Werte (z.B. -9=Keine Angabe) mittels Filter-Funktion.\n\ndaten_neu &lt;- daten %&gt;%\n  rename(TV_Konsum = lm02)%&gt;%\n  rename(TV_Vertrauen= pt09)%&gt;%\n  filter(between(TV_Konsum, 0, 1500))%&gt;%\n  filter(between(TV_Vertrauen, 1, 7))"
  },
  {
    "objectID": "Skript_7.4.html#vorhersage-des-multivariaten-modells",
    "href": "Skript_7.4.html#vorhersage-des-multivariaten-modells",
    "title": "Die multiple Regression",
    "section": "2.3 Vorhersage des multivariaten Modells",
    "text": "2.3 Vorhersage des multivariaten Modells\nAuch für die Kombination verschiedener Merkmale bzw. unabhängiger Variablen können wir uns über die predict.lm-Funktion Prognosen erstellen lassen. So können wir beispielsweise vergleichen, wie sich der Fernsehkonsum bestimmter “Idealtypen” von Befragten unterscheiden würde:\n\npredict.lm(model2, data.frame(Alter = c(25, 25), Bildung = c(0,5), TV_Vertrauen = c(7, 1)))\n\n        1         2 \n249.00453  86.07011 \n\n\nEine Person, die 25 Jahre alt ist und keinen Schulabschluss, aber dafür sehr großes Vertrauen in das Fernsehen hat, hat einen prognostizierten täglichen TV-Konsum von 249 Minuten. Eine Person, die ebenfalls 25 Jahre alt ist und Abitur hat, dem Fernsehen aber “überhaupt kein Vertrauen” entgegenbringt, hat einen täglichen prognostizierten Fernsehkonsum von 86 Minuten."
  },
  {
    "objectID": "Skript_7.4.html#regressionsmodell-zum-zusammenhang-von-alter-bildung-tv-vertrauen-geschlecht-und-tv-konsum",
    "href": "Skript_7.4.html#regressionsmodell-zum-zusammenhang-von-alter-bildung-tv-vertrauen-geschlecht-und-tv-konsum",
    "title": "Die multiple Regression",
    "section": "2.5 Regressionsmodell zum Zusammenhang von Alter, Bildung, TV-Vertrauen, Geschlecht und TV-Konsum",
    "text": "2.5 Regressionsmodell zum Zusammenhang von Alter, Bildung, TV-Vertrauen, Geschlecht und TV-Konsum\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable sex_d ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel3 &lt;- lm(TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, data = daten) \nsummary(lm.beta(model3))\n\n\nCall:\nlm(formula = TV_Konsum ~ Alter + Bildung + TV_Vertrauen + sex_d, \n    data = daten)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-255.89  -64.84  -18.97   33.01 1245.99 \n\nCoefficients:\n              Estimate Standardized Std. Error t value            Pr(&gt;|t|)    \n(Intercept)  170.54603           NA   12.96665  13.153 &lt;0.0000000000000002 ***\nAlter          1.56629      0.20625    0.13830  11.325 &lt;0.0000000000000002 ***\nBildung      -26.32891     -0.23397    1.99103 -13.224 &lt;0.0000000000000002 ***\nTV_Vertrauen   5.34102      0.05330    1.71809   3.109              0.0019 ** \nsex_d          4.60020      0.01766    4.34277   1.059              0.2896    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 121 on 3110 degrees of freedom\nMultiple R-squared:  0.1386,    Adjusted R-squared:  0.1375 \nF-statistic: 125.1 on 4 and 3110 DF,  p-value: &lt; 0.00000000000000022\n\n\n\n2.5.1 Inhaltliche Interpretation des Outputs: Was bedeutet das Ergebnis?\nGender hat keinen signifikanten Einfluss auf den TV-Konsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy-Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (4,6) Minuten höheren TV-Konsum als Männer (wobei dieser Befund statistisch ja nicht signifikant ist)."
  },
  {
    "objectID": "Skript_6.4.html#überprüfung-der-normalverteilung",
    "href": "Skript_6.4.html#überprüfung-der-normalverteilung",
    "title": "Die Varianzanalyse",
    "section": "2.1 Überprüfung der Normalverteilung",
    "text": "2.1 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\n\n#Histogramm ausgeben\n1ggplot(daten, aes(trustges)) +\n2  geom_histogram(aes(y = after_stat(count)),\n                 color = \"black\", fill = \"grey\", \n3                 binwidth = 1) +\n4  labs(x = \"Vertrauen in das Gesundheitswesen\",\n         y = \"\")\n\n\n1\n\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier daten) und unter aes unsere Variable (hier trustges).\n\n2\n\nDie Spezifizierungen innerhalb der Klammern unseres Histogramms geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = after_stat(count)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik.\n\n3\n\nMit binwidth = 1 verweisen wir hier auf die Breite der Balken unseres Histogramms.\n\n4\n\nFür ein verschönertes Aussehen unseres Graphen nutzen wir den Befehl labs um zusätzlich die Achsen zu beschriften.\n\n\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Vertrauen in das Gesundheitswesen rechtssteil ist, also die Teilnehmer der Befragung eher ein höheres Vertrauen angegeben haben.\nZusätzliche Gewissheit bezüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (welcher für kleinere Stichproben zwischen 3 und 5000 Fällen konzipiert ist). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\nLillieTest(daten$trustges)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$trustges\nD = 0.18718, p-value &lt; 0.00000000000000022\n\nshapiro.test(daten$trustges)\n\n\n    Shapiro-Wilk normality test\n\ndata:  daten$trustges\nW = 0.92102, p-value &lt; 0.00000000000000022\n\n\nIm vorliegenden Beispiel sind beide Tests signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests wie den Kruskal-Wallis-Test ausgewichen werden."
  },
  {
    "objectID": "Skript_6.4.html#deskriptive-statistiken",
    "href": "Skript_6.4.html#deskriptive-statistiken",
    "title": "Die Varianzanalyse",
    "section": "3.1 Deskriptive Statistiken",
    "text": "3.1 Deskriptive Statistiken\nZunächst interessieren uns die deskriptiven Statistiken. Dafür gruppieren wir die Ergebnisse unserer abhängigen Variablen anhand der Ausprägungen unserer unabhängigen Variablen und lassen uns jeweils den Mittelwert und die Standardabweichung ausgeben.\n\ndaten %&gt;% \n1  group_by(gesund) %&gt;%\n  summarise(Mittelwert = mean(trustges, na.rm = T),\n            Standardabweichung = sd(trustges, na.rm = T)) %&gt;% \n2  kable(digits = 2, col.names = c(\"Gesundheitszustand\", \"M\", \"SD\"), caption = \"Descriptives Vertrauen\")\n\n\n1\n\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion.\n\n2\n\nIm nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\n\n\n\nDescriptives Vertrauen\n\n\nGesundheitszustand\nM\nSD\n\n\n\n\nSEHR GUT\n5.13\n1.45\n\n\nGUT\n4.99\n1.32\n\n\nZUFRIEDENSTELLEND\n4.85\n1.37\n\n\nWENIGER GUT\n4.82\n1.48\n\n\nSCHLECHT\n4.71\n1.55\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass das Vertrauen in das Gesundheitswesen am höchsten ausgeprägt ist, bei Personen die einen guten Gesundheitszustand aufweisen. Ob dieser augenscheinliche Unterschied auch statistisch signifikant ist, möchten wir in einem nächsten Schritt mit der einfaktoriellen ANOVA berechnen."
  },
  {
    "objectID": "Skript_6.4.html#durchführen-der-varianzanalyse",
    "href": "Skript_6.4.html#durchführen-der-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "3.2 Durchführen der Varianzanalyse",
    "text": "3.2 Durchführen der Varianzanalyse\nZur Berechnung der Varianzanalye nutzen wir die Funktion aov_car aus dem afex-Paket:\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \n1aov_car(trustges ~ gesund + Error(respid), data = ., anova_table = \"pes\")\n\nprint(fit)\n\n2eta_squared(fit, partial = F)\n\n\n1\n\nInnerhalb der Funktion aov_car müssen wir zunächst die abhängige Variable (trustges) angeben und nach einer Tilde die unabhängige Variabel (gesund). Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier respid) angeben. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n2\n\nMit eta_squared können wir zusätzlich die Effektstärke angeben. Diese findet sich für die einfaktorielle Anova auch in der Spalte pes, aber hier erhalten wir zusätzlich ein Konfidenzintervall für den Effekt.\n\n\n\n\nAnova Table (Type 3 tests)\n\nResponse: trustges\n  Effect      df  MSE        F  pes p.value\n1 gesund 4, 3470 1.91 5.46 *** .006   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n# Effect Size for ANOVA (Type III)\n\nParameter |       η² |       95% CI\n-----------------------------------\ngesund    | 6.25e-03 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00]."
  },
  {
    "objectID": "Skript_2.1.html#zusammenfassung",
    "href": "Skript_2.1.html#zusammenfassung",
    "title": "Der Aufbau von Datensätzen",
    "section": "3.1 Zusammenfassung",
    "text": "3.1 Zusammenfassung\nIm nächsten Kapitel werden wir uns eingehender mit der Behandlung von tabellenstrukturierten Daten in unterschiedlichen Formaten beschäftigen."
  },
  {
    "objectID": "Skript_2.2.html#csv-dateien",
    "href": "Skript_2.2.html#csv-dateien",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "CSV-Dateien sind eine sehr einfache Art von Daten, die in R eingelesen werden können. Sie sind als Textdateien gespeichert, wobei die Tabellenspalten durch Kommas (oder manchmal Semikolons oder Tab-Zeichen) getrennt sind. Um eine CSV-Datei in R einzulesen, können Sie die Funktion read_csv (bzw. hier read_csv2) aus dem Paket readr verwenden.\nDieser Code importiert die Datei geschlechterverteilung.csv in das R-Environment. Die Datei geschlechterverteilung.csv muss im gleichen Verzeichnis wie der R-Code gespeichert sein.\n\ngeschlechterverteilung_csv &lt;- read_csv2(\"geschlechterverteilung.csv\")\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nRows: 2 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (1): geschlecht\ndbl (3): anzahl, anteil, prozent\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nDie Funktion read_csv2 liefert hier eine Menge zusätzlicher Informationen, etwa dazu, welche Datentypen für die eingelesenen Variablen gewählt wurden. Es wurde hier deshalb nicht read_csv sondern read_csv2 gewählt, weil letztere das Semikolon (;) als Trennzeichen und das Komma (,) als Dezimalzeichen verwendet. Hingegen geht read_csv vom Komma als Trennzeichen und dem Punkt als Dezimalzeichen aus, wie im angelsächsischem Gebrauch üblich. Liest man also mit der falschen Funktion ein, sind die Daten unter Umständen fehlerhaft."
  },
  {
    "objectID": "Skript_2.2.html#excel-dateien",
    "href": "Skript_2.2.html#excel-dateien",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "Ebenfalls nützlich ist die Möglichkeit, Daten aus Microsoft Excel in R zu imoportiern. Excel-Dateien können mit der Funktion readxl::read_excel() in R eingelesen werden. Diese Funktion unterstützt alle gängigen Excel-Formate, einschließlich XLSX, XLS und CSV.\n\ngeschlechterverteilung_excel &lt;- read_excel(\"geschlechterverteilung.xlsx\")"
  },
  {
    "objectID": "Skript_2.2.html#spss-dateien",
    "href": "Skript_2.2.html#spss-dateien",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "SPSS ist eine statistische Softwareanwendung, die von er Firma IBM (früher SPSS Inc) für Datenmanagement, inferenzstatistische Analysen und (kommerziell) Business Intelligence entwickelt wurde.SPSS-Dateien können mit der Funktion haven::read.spss() in R eingelesen werden. Diese Funktion unterstützt alle gängigen SPSS-Formate, einschließlich SAV und DSP.\nDa der ALLBUS im konkreten Fall im Stata-Format verwendet wird, verwenden wir für SPSS ein anderes Beispiel, nämlich die dritte Welle (in 2016 erhoben) des Global Report on Adult Learning and Education (GALE-3) der UNESCO.\n\ngale3_spss &lt;- read_spss(\"https://uil.unesco.org/i/doc/adult-education/grale-3/survey-data/grale-3-suquant.sav\")\n\nDas Beispiel zeigts dass wir neben lokal abgespeicherten Datein auch problemlos Web-Adressen als Pfadangabe beim Import von Daten verwenden können, wenn diese direkt auf die relevanten Daten zeigen."
  },
  {
    "objectID": "Skript_2.2.html#stata-dateien",
    "href": "Skript_2.2.html#stata-dateien",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "Bei STATA handelt es sich um eine weitere kommerzielle Statistik-Software. Mittels Stata analysiert man einfache und komplexe Datenmodelle. Die Software gehört neben SPSS zu den bekanntesten Programmen für die professionelle Datenauswertung. STATA-Dateien können mit der Funktion haven::read_dta() in R eingelesen werden. Diese Funktion unterstützt alle gängigen STATA-Formate, einschließlich DTA und DTB.\nWir verwnden als Beispiel für den Import von STATA-Daten hier den ALLBUS, da dieser im STATA-Format vorliegt.\n\nallbus_stata &lt;- read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_2.2.html#andere-textbasierte-formate",
    "href": "Skript_2.2.html#andere-textbasierte-formate",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "Für andere Datenformate gibt es in der Regel spezielle Funktionen, die in den entsprechenden Paketen enthalten sind. Zum Beispiel kann die Funktion readr::read_tsv() für das Einlesen von TSV-Dateien (die mit einem Tabulator- oder TAB-Zeichen getrennt sind) verwendet werden. Die meisten Funktionen zum Einlesen von (Text)Daten haben eine Reihe von Optionen, mit denen Sie den Importprozess anpassen können. Zu den häufigsten Optionen gehören:\n\n\n\n\n\n\n\nFunktionsargument\nBeschreibung\n\n\n\n\nheader\nGibt an, ob die Datendatei eine Kopfzeile enthält\n\n\nsep\nGibt das Trennzeichen zwischen den Werten an\n\n\nna\nGibt an, wie fehlende Werte codiert werden sollen\n\n\ndec\nGibt das Dezimaltrennzeichen an"
  },
  {
    "objectID": "Skript_2.2.html#zusammenfassung",
    "href": "Skript_2.2.html#zusammenfassung",
    "title": "Einlesen von Datensätzen in unterschiedlichen Formaten",
    "section": "",
    "text": "Es gibt verschiedene Möglichkeiten, Daten in R einzulesen. Die gebräuchlichste Methode ist die Verwendung von Funktionen. Für jedes gängige Datenformat gibt es eine entsprechende Funktion."
  },
  {
    "objectID": "Skript_6.1.html#überblick-über-die-verschiedenen-textverfahren",
    "href": "Skript_6.1.html#überblick-über-die-verschiedenen-textverfahren",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "flowchart LR\n  A&gt;Mittelwertvergleich] --&gt; B[unabhängige Stichprobe]\n  B --&gt; D(2 Faktorstufen)\n  B --&gt; E(mehr als 2 Faktorstufen)\n  D --&gt; F(t-test für unabhängige Stichprobe)\n  D --&gt; G(nicht parametrisch: Mann-Whitney Test)\n  E --&gt; H(Varianzanalyse: ein- und mehrfaktoriell)\n  E --&gt; I(nicht parametrisch: Kruskal-Wallis Test)\n  A --&gt; C[verbundene Stichprobe]\n  \n\nclick H \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html\""
  },
  {
    "objectID": "Skript_6.1.html#überblick-über-die-verschiedenen-testverfahren",
    "href": "Skript_6.1.html#überblick-über-die-verschiedenen-testverfahren",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "2.1 Überblick über die verschiedenen Testverfahren",
    "text": "2.1 Überblick über die verschiedenen Testverfahren\nWie wir sehen gibt es eine ganze Reihe an Testverfahren für Mittelwertvertgleiche. Die folgende Grafik kann als Orientierung dienen, welches Verfahren für eure spezielle Datenlage Sinn ergibt. Durch einen Klick auf die Verfahren kommt ihr jeweils zum entsprechenden Kapitel.\n\n\n\n\nflowchart LR\n  A&gt;Mittelwertvergleich] --&gt; B(unabhängige Stichprobe)\n  B --&gt; D(2 Faktorstufen)\n  B --&gt; E(mehr als 2 Faktorstufen)\n  D --&gt; F(t-test für unabhängige Stichprobe)\n  D --&gt; G(nicht parametrisch: Mann-Whitney Test)\n  E --&gt; H(Varianzanalyse: ein- und mehrfaktoriell)\n  E --&gt; I(nicht parametrisch: Kruskal-Wallis Test)\n  A --&gt; C(verbundene Stichprobe)\n  C --&gt; J(2 Faktorstufen)\n  C --&gt; K(mehr als 2 Faktorstufen)\n  J --&gt; L(t-test für verbundene Stichproben)\n  J --&gt; M(nicht parametrisch: Wilcoxon-Test)\n  K --&gt; N(einfaktorielle Varianzanalyse mit Messwiederholung)\n  \nclick I \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html#exkurs-kruskal-wallis-test\"\nclick H \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html\"\nclick N \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html\""
  },
  {
    "objectID": "Skript_6.1.html#wann-verwende-ich-welches-testverfahren",
    "href": "Skript_6.1.html#wann-verwende-ich-welches-testverfahren",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "1.1 Wann verwende ich welches Testverfahren?",
    "text": "1.1 Wann verwende ich welches Testverfahren?\nDie folgende Grafik kann als Orientierung dienen, welches Verfahren für eure spezielle Datenlage Sinn ergibt.\n\n\n\n\n\nflowchart LR\n  A&gt;Mittelwertvergleich] --&gt; B(unabhängige Stichprobe)\n  B --&gt; D(2 Faktorstufen)\n  B --&gt; E(mehr als 2 Faktorstufen)\n  D --&gt; F(t-test für unabhängige Stichprobe)\n  D --&gt; G(nicht parametrisch: Mann-Whitney Test)\n  E --&gt; H(Varianzanalyse: ein- und mehrfaktoriell)\n  E --&gt; I(nicht parametrisch: Kruskal-Wallis Test)\n  A --&gt; C(verbundene Stichprobe)\n  C --&gt; J(2 Faktorstufen)\n  C --&gt; K(mehr als 2 Faktorstufen)\n  J --&gt; L(t-test für verbundene Stichproben)\n  J --&gt; M(nicht parametrisch: Wilcoxon-Test)\n  K --&gt; N(einfaktorielle Varianzanalyse mit Messwiederholung)\n  \nclick I \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html#exkurs-nicht-parametrische-testverfahren\"\nclick H \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html\"\nclick N \"https://patrickzerrer.github.io/R_Hands_on_Book/Skript_6.4.html#umgang-mit-messwiederholungen\""
  },
  {
    "objectID": "Skript_6.3.html#deskriptives-der-abhängigen-variablen",
    "href": "Skript_6.3.html#deskriptives-der-abhängigen-variablen",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.2 Deskriptives der abhängigen Variablen",
    "text": "1.2 Deskriptives der abhängigen Variablen\nAls ersten Schritt schauen wir uns die deskriptiven Statistiken unserer abhängigen Variablen Vertrauen in die Regierung für die beiden Faktorstufen unserer unabhängigen Variablen an.\n\ndes_stat &lt;- (favstats(trustreg ~ eastwest, data = daten))\nrownames(des_stat) &lt;- c(\"Alte Bundesländer\", \"Neue Bundesländer\")\nkable(des_stat[,2:10], digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\nAlte Bundesländer\n1\n3\n4\n5\n7\n4.15\n1.58\n2335\n0\n\n\nNeue Bundesländer\n1\n3\n4\n5\n7\n3.86\n1.71\n1183\n0\n\n\n\n\n\nAnhand dieser deskriptiven Daten sehen wir, dass das Vertrauen in die Bundesregierung in den alten Bundesländern im Mittelwert mit 4,15 stärker ausgeprägt ist als in den neuen Bundesländern (Mittelwert von 3,86. Um zu prüfen, ob dieser Unterschied des Mittelwertes statistisch signifikant ist, berechnen wir in einem zweiten Schritt die Teststatistik des t-tests. Zunächst müssen wir jedoch prüfen, ob unsere Daten für die Berechnung geeignet sind. Dazu führen wir eine Voraussetzungsprüfung durch."
  },
  {
    "objectID": "Skript_6.3.html#voraussetzungsprüfung",
    "href": "Skript_6.3.html#voraussetzungsprüfung",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.3 Voraussetzungsprüfung",
    "text": "1.3 Voraussetzungsprüfung\nBei dem t-test handelt es sich um ein statistisches Verfahren. Um die Güte der Ergebnisse sicherzustellen müssen dabei einige Voraussetzungen beachtet werden.Dies sind im Überblick:\n\ndie korrekte Skalierung der UV (=Faktor) und AV (=numerisch)\ndie Normalverteilung der abhängigen Variablen\ndie Varianzhomogenität\n\n\n1.3.1 Skalierung der UV und AV\nDie abhängige Variable muss als numerischer Vektor vorliegen, die unabhängige Variable wiederum als Faktor (mit zwei Ausprägungen). Die korrekte Skalierung der Variablen haben wir bereits innerhalb des Data Managements sichergestellt, können uns aber zusätzlich mit dem Befehl class noch einmal einen Überblick über das Skalenniveau unserer Variablen verschaffen.\n\ndaten %&gt;%  \n  select(eastwest, trustreg) %&gt;%  \n  lapply(class)\n\n$eastwest\n[1] \"factor\"\n\n$trustreg\n[1] \"numeric\"\n\n\n\n\n1.3.2 Normalverteilung der AV\n\ndaten  %&gt;%  \n  group_by(eastwest) %&gt;%  \n  do(tidy(shapiro.test(.$trustreg)))\n\n# A tibble: 2 × 4\n# Groups:   eastwest [2]\n  eastwest           statistic  p.value method                     \n  &lt;fct&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1 ALTE BUNDESLAENDER     0.937 2.69e-30 Shapiro-Wilk normality test\n2 NEUE BUNDESLAENDER     0.930 5.06e-23 Shapiro-Wilk normality test\n\n\n\n\n1.3.3 Varianzhomogenität\n\ndaten %&gt;%  \n  drop_na(eastwest, trustreg) %&gt;% \n  mutate(eastwest = as.factor(eastwest)) %&gt;% \n  leveneTest(trustreg~eastwest, data = .)\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value   Pr(&gt;F)   \ngroup    1  9.5219 0.002046 **\n      3516                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests",
    "href": "Skript_6.3.html#durchführung-des-t-tests",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.4 Durchführung des t-Tests",
    "text": "1.4 Durchführung des t-Tests\n\ndaten %&gt;% \n  drop_na(eastwest, trustreg) %&gt;%\n  filter(.,trustreg &gt;0) %&gt;% \n  t.test(trustreg ~ eastwest,\n         alternative = \"two.sided\", \n         var.equal = T,\n         data = .)\n\n\n    Two Sample t-test\n\ndata:  trustreg by eastwest\nt = 4.9349, df = 3516, p-value = 8.391e-07\nalternative hypothesis: true difference in means between group ALTE BUNDESLAENDER and group NEUE BUNDESLAENDER is not equal to 0\n95 percent confidence interval:\n 0.1726028 0.4001616\nsample estimates:\nmean in group ALTE BUNDESLAENDER mean in group NEUE BUNDESLAENDER \n                        4.147752                         3.861369 \n\n\nEffektstärke cohens d berechnen\n\ndaten %&gt;% \n  drop_na(eastwest, trustreg) %&gt;% \n  cohens_d(trustreg~eastwest, data = .)\n\nCohen's d |       95% CI\n------------------------\n0.18      | [0.11, 0.25]\n\n- Estimated using pooled SD."
  },
  {
    "objectID": "Skript_6.3.html#interpretation-des-outputs",
    "href": "Skript_6.3.html#interpretation-des-outputs",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.5 Interpretation des Outputs",
    "text": "1.5 Interpretation des Outputs\nDer Output des t-tests gibt uns die folgenden Informationen aus:\n\ndata\nt\ndf\np-value\n95% confidence intervall\nmeans in group 1 & 2\n\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse des t-tests werden üblicherweise im Text angegeben. Für diese Angabe werden die folgenden Informationen benötigt:\n✅ die Mittelwerte und Standardabweichung der einzelnen Vergleichsgruppen\n✅ der df-Wert\n✅ der F-Wert\n✅ der p-Wert\n✅ die Effektgröße\nDas Format ist üblicherweise:\n\nBeispiel: Personen mit einem sehr guten Gesundheitszustand haben durchschnittlich ein höheres Vertrauen in das Gesundheitswesen (M = 5.13;SD=1.45) als Personen mit einem guten (M = 4.99;SD=1.32), zufriedenstellenden (M = 4.85;SD=1.37), weniger guten (M = 4.82;SD=1.48) oder schlechtem (M = 4.71;SD=1.55) Gesundheitszustand . Der Gesundheitszustand hat dabei einen signifikanten Einfluss auf das Vertrauen in das Gesundheitswesen (F3470)=5,46,p&lt;0,001. Die Effektstärke nach Cohen (1992) liegt bei alpha=0,006 und entspricht einem kleinen Effekt. Post-Hoc Paarvergleiche mit Tamhames ergaben, dass sich der Mittelwert für die Personen mit sehr guten Gesundheitszustand signifikant von den Personen mit zufriedenstellendem (p&lt;0.029) und weniger gutem (p&lt;0.0174) Zustand unterscheidet. Die anderen Gesundheitsgruppen unterscheiden sich hingegen nicht signifikant voneinander."
  },
  {
    "objectID": "Skript_6.3.html#was-bedeutet-unser-output-inhaltlich",
    "href": "Skript_6.3.html#was-bedeutet-unser-output-inhaltlich",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.4 Was bedeutet unser Output inhaltlich?",
    "text": "2.4 Was bedeutet unser Output inhaltlich?\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse des t-tests werden üblicherweise im Text angegeben. Für diese Angabe werden die folgenden Informationen benötigt:\n✅ die Werte des Kaiser-Meyer-Olkin Kritierums (KMO)\nDas Format ist üblicherweise:\n\nBeispiel: Zunächst wurde Faktorenanalyse der 15 gemessenen Indikatoren durchgeführt. Das Kaiser-Meyer-Olkin (KMO)-Maß für die Stichprobenadäquanz betrug 0,89. Dies deutet darauf hin, dass die Korrelationsmuster relativ kompakt sind und die Faktorenanalyse eindeutige und zuverlässige Faktoren ergeben sollte. Der Bartlett-Test auf Sphärizität war ebenfalls signifikant (χ2(105) = 33164.76, p &lt; .001). Dies bedeutet, dass es einige Beziehungen zwischen den untersuchten Variablen gibt. Sowohl der KMO-Test als auch der Bartlett-Test bestätigen, dass die Faktorenanalyse angemessen ist."
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Test",
    "text": "1.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Test\nHistorie der Testverfahren"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben",
    "href": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.1 Durchführung des t-tests für verbundene Stichproben",
    "text": "2.1 Durchführung des t-tests für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-wilcoxon-vorzeichen-rangtest",
    "href": "Skript_6.3.html#nicht-parametrischer-test-wilcoxon-vorzeichen-rangtest",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.2 Nicht-parametrischer Test: Wilcoxon-Vorzeichen-Rangtest",
    "text": "2.2 Nicht-parametrischer Test: Wilcoxon-Vorzeichen-Rangtest\n\npairwise.wilcox.test(daten$trustges, daten$gesund,\n                 p.adjust.method = \"BH\")\n\n\n    Pairwise comparisons using \n\ndata:  daten$trustges and daten$gesund \n\n&lt;0 x 0 matrix&gt;\n\nP value adjustment method: BH"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests-für-einstichproben",
    "href": "Skript_6.3.html#durchführung-des-t-tests-für-einstichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.3 Durchführung des t-tests für Einstichproben",
    "text": "3.3 Durchführung des t-tests für Einstichproben\n\nt.test(daten_one$einkommen , mu = 3813)\n\n\n    One Sample t-test\n\ndata:  daten_one$einkommen\nt = -23.889, df = 795, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 3813\n95 percent confidence interval:\n 2343.44 2566.61\nsample estimates:\nmean of x \n 2455.025"
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.4 Nicht-parametrischer Test:",
    "text": "3.4 Nicht-parametrischer Test:"
  },
  {
    "objectID": "Skript_6.3.html#datenmanagement",
    "href": "Skript_6.3.html#datenmanagement",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.1 Datenmanagement",
    "text": "1.1 Datenmanagement\nWir beginnen mit dem Laden der notwendigen Pakete.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n1p_load(tidyverse, ggplot2, haven, mosaic, knitr,effectsize, car, broom)\n\n2theme_set(theme_classic())\n\n\n1\n\nMittels p_load laden wir alle benötigten Pakete gleichzeitig.\n\n2\n\nWir legen allgemein den Hintergrund theme_classic fest.\n\n\n\n\nAnschließend laden wir die Daten aus dem Allbus:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nUm mit dem Datensatz zu arbeiten benötigen wir einige grundlegende Schritte des Datenmanagements (für ausführliche Erklärungen siehe hier). Für unseren t-test für unabhängige Stichproben möchten wir uns anschauen, wie sich der Wohnort der Befragten auf ihr Vertrauen in die Bundesregierung auswirkt. Wir nutzen dazu die folgenden Variablen:\n\n\n\n\n\n\n\n\nVariable\nBeschreibung\nAusprägungen\n\n\n\n\neastwest\nErhebungsgebiet West-Ost\n1= Alte Bundesländer (Westen)\n2 = Neue Bundesländer (Osten)\n\n\npt12\nVertrauen in die Bundesregierung\n-42 = Datenfehler\n-11 = TNZ Split\n-9 = Keine Angabe\n1 = Gar kein Vertrauen\n…\n7 = Großes Vertrauen\n\n\n\nInnerhalb unseres Datenmanageements schließen wir fehlerhafte und fehlende Werte der Variablen pt12 aus und benennen diese anschließend um:\n\ndaten &lt;- daten %&gt;%\n1  filter(., pt12 &gt; 0) %&gt;%\n2  rename(., trustreg = pt12) %&gt;%\n3  mutate(trustreg = as.numeric(trustreg),\n         eastwest = haven::as_factor(eastwest))\n\n\n1\n\nWir schließend fehlende Werte (siehe Tabelle oben aus). Da diese alle negativ sind, können wir einfach alle Werte kleiner als 0 ausschließen.\n\n2\n\nHier benennen wir die Variablen anders um unsere weitere Arbeit zu vereinfachen.\n\n3\n\nWir bringen unsere Daten in das korrekte Format. Für den t-Test muss die abhängige Variable als Vektor vorliegen und die unabhängige Variable als Faktor."
  },
  {
    "objectID": "Skript_2.3.html#erste-schritte-mit-dem-allbus",
    "href": "Skript_2.3.html#erste-schritte-mit-dem-allbus",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "Wir beginnen damit, die notwendigen Pakete zu laden, die wir für die ersten Schritte mit den Daten benötigen. Das sind hier die Pakete haven (für das Einlesen der ALLBUS-Daten im Stata-Format) und das Pakete readr (für das Einlesen einiger Vorab vorbereiteter Samples aus dem Gesamtdatensatz), sowie das Paket dplyr, mit dem wir am Schluss einen Beispielhaften Teildatensatz bilden.\n\nlibrary(haven)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttache Paket: 'dplyr'\n\n\nDie folgenden Objekte sind maskiert von 'package:stats':\n\n    filter, lag\n\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nNun lesen wir den ALLBUS-Datensatz mittels der Stata-Importfunktion read_dta aus dem Paket haven ein.\n\ndaten &lt;- read_dta(\"Datensatz/Allbus_2021.dta\")\n\nAls nächstes laden wir zudem noch drei zuvor erstellte Zufallssamples im Umfang von 20, 200 und 500 Zeilen aus dem Gesamtdatensatz. Diese enthalten weiterhin eine deutlich kleinere Anzahl relevanter Variablen und sind daher etwas übersichtlicher als der sehr große Hauptdatensatz.\n\nsample_klein &lt;- read_rds(\"Datensatz/ALLBUS_sample_klein.rds\")\nsample_mittel &lt;- read_rds(\"Datensatz/ALLBUS_sample_mittel.rds\")\nsample_gross &lt;- read_rds(\"Datensatz/ALLBUS_sample_gross.rds\")\n\nZunächst schauen wir uns die Daten an. Dies geschieht entweder daturch, dass man den Objektnamen verwendet (also im folgende Beispiel einfach sample_klein) oder indem man mit einem Klick in RStudio unter dem Reiter Environment oder mit dem Befehl View() aufruft. Bei diesem Vorgehen zeigt RStudio die Daten an, was i.d.R. den praktischsten Zugang darstellt.\n\nsample_klein\n\n# A tibble: 20 × 4\n   alter geschlecht bildung            fernsehkonsum\n   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;                      &lt;dbl&gt;\n 1    37 MANN       FACHHOCHSCHULREIFE             3\n 2    38 MANN       MITTLERE REIFE                 7\n 3    47 FRAU       VOLKS-,HAUPTSCHULE             6\n 4    66 MANN       HOCHSCHULREIFE                 2\n 5    47 FRAU       HOCHSCHULREIFE                 7\n 6    75 MANN       VOLKS-,HAUPTSCHULE             7\n 7    41 FRAU       MITTLERE REIFE                 4\n 8    18 MANN       NOCH SCHUELER                  7\n 9    91 MANN       &lt;NA&gt;                          NA\n10    56 MANN       HOCHSCHULREIFE                 4\n11    58 MANN       HOCHSCHULREIFE                 7\n12    32 MANN       MITTLERE REIFE                 2\n13    47 MANN       FACHHOCHSCHULREIFE             1\n14    49 MANN       MITTLERE REIFE                 1\n15    23 MANN       MITTLERE REIFE                 0\n16    48 FRAU       MITTLERE REIFE                 0\n17    49 MANN       HOCHSCHULREIFE                 0\n18    36 MANN       FACHHOCHSCHULREIFE             7\n19    70 MANN       HOCHSCHULREIFE                 7\n20    43 FRAU       MITTLERE REIFE                 7\n\n\nDer kleine Beispieldatensatz illustriert den grundlegeenden Aufbau des ALLBUS. Dieser folgt (beim ALLBUS, aber auch den meisten anderen Befragungen) den folgenden Prinzipien\n\njede Befragungswelle ist ein einzeles Data Frame-Objekt (= eine große Tabelle)\ndie Zeilen der Tabelle sind die Beobachtungen (= RespondentInnen)\ndie Spalten der Tabelle sind die Variablen (= Antworten auf Survey-Fragen, oder bei der Möglichkeit zur Mehrfachnennung, die einzelnen Antwortoptionen)\ndie Zelleninhalte sind i.d.R. (Dummy)Zahlenwerte (= etwa “1” für wenig Zustimmung und “5” für hohe Zustimmung)"
  },
  {
    "objectID": "Skript_2.3.html#einen-überblick-über-den-allbus-gewinnen",
    "href": "Skript_2.3.html#einen-überblick-über-den-allbus-gewinnen",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "Im Beispieldatensatz sind die Werte der Variablen alter, geschlecht und bildung recht leicht nachvollziebar, wobei sie etwas unterschiedliche Datentypen aufweisen, wie man mit Hilfe der Funktion str ermitteln kann.\n\nstr(sample_klein)\n\ntibble [20 × 4] (S3: tbl_df/tbl/data.frame)\n $ alter        : num [1:20] 37 38 47 66 47 75 41 18 91 56 ...\n $ geschlecht   : Factor w/ 4 levels \"KEINE ANGABE\",..: 2 2 3 2 3 2 3 2 2 2 ...\n $ bildung      : Factor w/ 9 levels \"NICHT BESTIMMBAR\",..: 6 5 4 7 7 4 5 9 NA 7 ...\n $ fernsehkonsum: num [1:20] 3 7 6 2 7 7 4 7 NA 4 ...\n\n\nBei der Variable alter handelt es sich schlicht um eine Zahl (num), während geschlecht und bildung sogenannte Faktoren sind. Faktoren nutzt man in R, um wiederholende nicht numerische (typischerweise nominal oder ordinalskalierte) Werte zu speichern. Praktisch jeder Faktor könnte genausogut eine Zeichenkette (chr) sein, aber oftmals sind Faktoren praktischer, weil sie eine festen Reihenfolge haben können (“ranked factors”), die sich bei Bedarf auch in Zahlen umwandel lassen. Im konkreten Beispiel ist das Geschlecht ein ungerankter Faktor, der Bildungsgrad hat hingegen einen Rang. Der Fernsehkonsum ist schließlich eine Likert-skalierte Variable, die wir hier und auch an anderer Stelle als metrische Variable behandeln (und dafür den R-Datentypen numeric verwenden), auch wenn das strikt genommen nicht immer zulässig ist – zumindest dann nicht, wenn man nur auf Grundlage eines einzelnen Items misst und eine 5- oder 7-Punkt Skala verwendet (vgl etwa https://doi.org/10.1080/01488376.2017.1329775).\nWie sehen die anderen Samples aus? Wir sehen uns im nächsten Schritt das große Zufallssample (500 Fälle) an.\n\nsample_gross\n\n# A tibble: 500 × 29\n   alter geschlecht fernsehkonsum politisches_interesse links_rechts_einordnung\n   &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt; &lt;fct&gt;                                   &lt;dbl&gt;\n 1    50 FRAU                   7 MITTEL                                      5\n 2    77 MANN                   7 STARK                                       3\n 3    64 FRAU                   7 &lt;NA&gt;                                        5\n 4    53 MANN                  NA UEBERHAUPT NICHT                            8\n 5    29 MANN                   0 SEHR STARK                                  2\n 6    44 FRAU                   3 MITTEL                                      7\n 7    79 MANN                   5 MITTEL                                      4\n 8    32 MANN                   7 MITTEL                                      5\n 9    36 FRAU                   0 STARK                                       6\n10    66 MANN                   7 STARK                                       3\n# ℹ 490 more rows\n# ℹ 24 more variables: wahlabsicht_partei &lt;fct&gt;,\n#   zufriedenheit_demokratie &lt;fct&gt;, entwicklung_kriminalitaet &lt;fct&gt;,\n#   social_media_nachrichtenquelle &lt;dbl&gt;, glaubwuerdigkeit_oer_tv &lt;fct&gt;,\n#   glaubwuerdigkeit_privat_tv &lt;fct&gt;, glaubwuerdigkeit_zeitungen &lt;fct&gt;,\n#   glaubwuerdigkeit_social_media &lt;fct&gt;, vertrauen_mitmenschen &lt;dbl&gt;,\n#   vertrauen_gesundheitswesen &lt;dbl&gt;, …\n\n\nDa wir es jetzt mit einer größeren Zahl an Beobachtungen and Variablen zu tun haben kann es nützlich sein, sich einen Überblick zu verschaffen.\nZunächst lassen wir uns die Variablennamen (also die Spalten) ausgeben. Dies geschieht mit der Funktion colnames.\n\ncolnames(sample_gross)\n\n [1] \"alter\"                              \"geschlecht\"                        \n [3] \"fernsehkonsum\"                      \"politisches_interesse\"             \n [5] \"links_rechts_einordnung\"            \"wahlabsicht_partei\"                \n [7] \"zufriedenheit_demokratie\"           \"entwicklung_kriminalitaet\"         \n [9] \"social_media_nachrichtenquelle\"     \"glaubwuerdigkeit_oer_tv\"           \n[11] \"glaubwuerdigkeit_privat_tv\"         \"glaubwuerdigkeit_zeitungen\"        \n[13] \"glaubwuerdigkeit_social_media\"      \"vertrauen_mitmenschen\"             \n[15] \"vertrauen_gesundheitswesen\"         \"vertrauen_bundesverfassungsgericht\"\n[17] \"vertrauen_bundestag\"                \"vertrauen_stadt_gemeindeverwaltung\"\n[19] \"vertrauen_katholische_kirche\"       \"vertrauen_evangelische_kirche\"     \n[21] \"vertrauen_justiz\"                   \"vertrauen_fernsehen\"               \n[23] \"vertrauen_zeitungswesen\"            \"vertrauen_hochschulen\"             \n[25] \"vertrauen_bundesregierung\"          \"vertrauen_polizei\"                 \n[27] \"vertrauen_parteien\"                 \"vertrauen_eu_kommission\"           \n[29] \"vertrauen_eu_parlament\"            \n\n\nEine etwas detailliertere Beschreibung erhalten wir durch die Funktion str. Diese liefert uns auch die Dimensionen (Anzahl der Zeilen und Spalten) des Data Frames, sowie die Variablen, deren Datentyp und die ersten zehn Ausprägungen.\n\nstr(sample_gross)\n\ntibble [500 × 29] (S3: tbl_df/tbl/data.frame)\n $ alter                             : num [1:500] 50 77 64 53 29 44 79 32 36 66 ...\n $ geschlecht                        : Factor w/ 4 levels \"KEINE ANGABE\",..: 3 2 3 2 2 3 2 2 3 2 ...\n $ fernsehkonsum                     : num [1:500] 7 7 7 NA 0 3 5 7 0 7 ...\n $ politisches_interesse             : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 4 NA 7 3 5 5 5 4 4 ...\n $ links_rechts_einordnung           : num [1:500] 5 3 5 8 2 7 4 5 6 3 ...\n $ wahlabsicht_partei                : Factor w/ 13 levels \"NICHT WAHLBERECHTIGT\",..: NA 9 NA 13 10 NA 11 6 NA 9 ...\n $ zufriedenheit_demokratie          : Factor w/ 10 levels \"DATENFEHLER: MFN\",..: 5 6 7 8 6 5 8 NA NA 6 ...\n $ entwicklung_kriminalitaet         : Factor w/ 8 levels \"DATENFEHLER: MFN\",..: 6 5 4 4 6 5 5 5 NA 6 ...\n $ social_media_nachrichtenquelle    : num [1:500] 0 7 0 7 7 6 0 7 NA 0 ...\n $ glaubwuerdigkeit_oer_tv           : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 4 4 5 5 4 NA 5 5 5 5 ...\n $ glaubwuerdigkeit_privat_tv        : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 NA 5 5 5 NA 6 5 6 NA ...\n $ glaubwuerdigkeit_zeitungen        : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 5 4 5 5 5 NA 7 5 4 4 ...\n $ glaubwuerdigkeit_social_media     : Factor w/ 7 levels \"DATENFEHLER: MFN\",..: 6 5 NA 5 6 4 NA 6 6 NA ...\n $ vertrauen_mitmenschen             : num [1:500] 3 3 3 2 3 3 2 NA 3 3 ...\n $ vertrauen_gesundheitswesen        : num [1:500] 6 6 5 7 6 5 5 NA 5 5 ...\n $ vertrauen_bundesverfassungsgericht: num [1:500] 6 6 4 1 6 6 2 NA 5 5 ...\n $ vertrauen_bundestag               : num [1:500] 5 5 5 1 2 7 1 NA 5 4 ...\n $ vertrauen_stadt_gemeindeverwaltung: num [1:500] 5 5 5 1 5 6 4 NA 5 3 ...\n $ vertrauen_katholische_kirche      : num [1:500] 2 1 1 1 2 5 1 NA 2 1 ...\n $ vertrauen_evangelische_kirche     : num [1:500] 3 3 1 1 4 2 1 NA 2 2 ...\n $ vertrauen_justiz                  : num [1:500] 5 3 3 4 4 1 3 NA 4 4 ...\n $ vertrauen_fernsehen               : num [1:500] 4 6 3 3 3 6 1 NA 4 4 ...\n $ vertrauen_zeitungswesen           : num [1:500] 4 6 3 3 3 6 1 NA 4 4 ...\n $ vertrauen_hochschulen             : num [1:500] 5 6 5 1 6 7 3 NA 5 5 ...\n $ vertrauen_bundesregierung         : num [1:500] 5 5 5 1 3 7 1 NA 5 4 ...\n $ vertrauen_polizei                 : num [1:500] 5 5 4 5 4 7 3 NA 5 4 ...\n $ vertrauen_parteien                : num [1:500] 4 4 3 1 3 4 1 NA 2 3 ...\n $ vertrauen_eu_kommission           : num [1:500] 5 5 3 1 3 5 3 NA 4 4 ...\n $ vertrauen_eu_parlament            : num [1:500] 5 5 3 1 3 6 3 NA 4 4 ...\n\n\nEine alternative (aber etwas ordentlichere) Ansicht erhält man mit dem Befehl glimpse aus dem Paket tibble (im tidyverse enthalten).\n\ntibble::glimpse(sample_gross)\n\nRows: 500\nColumns: 29\n$ alter                              &lt;dbl&gt; 50, 77, 64, 53, 29, 44, 79, 32, 36,…\n$ geschlecht                         &lt;fct&gt; FRAU, MANN, FRAU, MANN, MANN, FRAU,…\n$ fernsehkonsum                      &lt;dbl&gt; 7, 7, 7, NA, 0, 3, 5, 7, 0, 7, 7, 7…\n$ politisches_interesse              &lt;fct&gt; MITTEL, STARK, NA, UEBERHAUPT NICHT…\n$ links_rechts_einordnung            &lt;dbl&gt; 5, 3, 5, 8, 2, 7, 4, 5, 6, 3, 4, 7,…\n$ wahlabsicht_partei                 &lt;fct&gt; NA, DIE GRUENEN, NA, WUERDE NICHT W…\n$ zufriedenheit_demokratie           &lt;fct&gt; SEHR ZUFRIEDEN, ZIEMLICH ZUFRIEDEN,…\n$ entwicklung_kriminalitaet          &lt;fct&gt; IST GLEICH GEBLIEBEN, HAT ETWAS ZUG…\n$ social_media_nachrichtenquelle     &lt;dbl&gt; 0, 7, 0, 7, 7, 6, 0, 7, NA, 0, 0, N…\n$ glaubwuerdigkeit_oer_tv            &lt;fct&gt; SEHR GLAUBWUERDIG, SEHR GLAUBWUERDI…\n$ glaubwuerdigkeit_privat_tv         &lt;fct&gt; EHER GLAUBWUERDIG, NA, EHER GLAUBWU…\n$ glaubwuerdigkeit_zeitungen         &lt;fct&gt; EHER GLAUBWUERDIG, SEHR GLAUBWUERDI…\n$ glaubwuerdigkeit_social_media      &lt;fct&gt; EHER N. GLAUBWUERDIG, EHER GLAUBWUE…\n$ vertrauen_mitmenschen              &lt;dbl&gt; 3, 3, 3, 2, 3, 3, 2, NA, 3, 3, 1, 3…\n$ vertrauen_gesundheitswesen         &lt;dbl&gt; 6, 6, 5, 7, 6, 5, 5, NA, 5, 5, 3, N…\n$ vertrauen_bundesverfassungsgericht &lt;dbl&gt; 6, 6, 4, 1, 6, 6, 2, NA, 5, 5, 7, N…\n$ vertrauen_bundestag                &lt;dbl&gt; 5, 5, 5, 1, 2, 7, 1, NA, 5, 4, 4, N…\n$ vertrauen_stadt_gemeindeverwaltung &lt;dbl&gt; 5, 5, 5, 1, 5, 6, 4, NA, 5, 3, 5, N…\n$ vertrauen_katholische_kirche       &lt;dbl&gt; 2, 1, 1, 1, 2, 5, 1, NA, 2, 1, 1, N…\n$ vertrauen_evangelische_kirche      &lt;dbl&gt; 3, 3, 1, 1, 4, 2, 1, NA, 2, 2, 4, N…\n$ vertrauen_justiz                   &lt;dbl&gt; 5, 3, 3, 4, 4, 1, 3, NA, 4, 4, 5, N…\n$ vertrauen_fernsehen                &lt;dbl&gt; 4, 6, 3, 3, 3, 6, 1, NA, 4, 4, 5, N…\n$ vertrauen_zeitungswesen            &lt;dbl&gt; 4, 6, 3, 3, 3, 6, 1, NA, 4, 4, 6, N…\n$ vertrauen_hochschulen              &lt;dbl&gt; 5, 6, 5, 1, 6, 7, 3, NA, 5, 5, 6, N…\n$ vertrauen_bundesregierung          &lt;dbl&gt; 5, 5, 5, 1, 3, 7, 1, NA, 5, 4, 2, N…\n$ vertrauen_polizei                  &lt;dbl&gt; 5, 5, 4, 5, 4, 7, 3, NA, 5, 4, 6, N…\n$ vertrauen_parteien                 &lt;dbl&gt; 4, 4, 3, 1, 3, 4, 1, NA, 2, 3, 2, N…\n$ vertrauen_eu_kommission            &lt;dbl&gt; 5, 5, 3, 1, 3, 5, 3, NA, 4, 4, 2, N…\n$ vertrauen_eu_parlament             &lt;dbl&gt; 5, 5, 3, 1, 3, 6, 3, NA, 4, 4, 2, N…\n\n\nDie hier verwendete Syntax PAKETNAME::FUNKTION ist vielleicht zunächst etwas irritierend. Mit ihr rufen wir ein Paket auf, welches wir nicht vorher geladen haben. Das ist mitunter nützlich und kommt hier zur Anwendung, weil wir das Paket tibble hier ansonsten nicht benutzen.\nSchließlich lassen sich mit dem Befehle summary auch noch Eckwerte wie die Ausprägung von Faktorenstufen (bei Faktoren) oder Lageparameter (bei metrischen Variable) ermitteln.\n\nsummary(sample_gross)\n\n     alter              geschlecht  fernsehkonsum        politisches_interesse\n Min.   :18.00   KEINE ANGABE:  0   Min.   :0.000   MITTEL          :231      \n 1st Qu.:38.00   MANN        :245   1st Qu.:4.000   STARK           :138      \n Median :56.00   FRAU        :254   Median :7.000   WENIG           : 58      \n Mean   :53.78   DIVERS      :  1   Mean   :5.305   SEHR STARK      : 53      \n 3rd Qu.:68.00                      3rd Qu.:7.000   UEBERHAUPT NICHT: 17      \n Max.   :93.00                      Max.   :7.000   (Other)         :  0      \n NA's   :3                          NA's   :8       NA's            :  3      \n links_rechts_einordnung   wahlabsicht_partei       zufriedenheit_demokratie\n Min.   : 1.000          CDU-CSU    : 97      ZIEMLICH ZUFRIEDEN:151        \n 1st Qu.: 4.000          DIE GRUENEN: 89      ETWAS ZUFRIEDEN   : 63        \n Median : 5.000          SPD        : 51      ETWAS UNZUFRIEDEN : 46        \n Mean   : 4.935          FDP        : 50      SEHR ZUFRIEDEN    : 32        \n 3rd Qu.: 6.000          AFD        : 33      ZIEML. UNZUFRIEDEN: 25        \n Max.   :10.000          (Other)    : 56      (Other)           :  3        \n NA's   :23              NA's       :124      NA's              :180        \n        entwicklung_kriminalitaet social_media_nachrichtenquelle\n HAT ETWAS ZUGENOMMEN:168         Min.   :0.000                 \n HAT STARK ZUGENOMMEN:163         1st Qu.:0.000                 \n IST GLEICH GEBLIEBEN:118         Median :1.000                 \n HAT ETWAS ABGENOMMEN: 34         Mean   :2.921                 \n HAT STARK ABGENOMMEN:  3         3rd Qu.:7.000                 \n (Other)             :  0         Max.   :7.000                 \n NA's                : 14         NA's   :45                    \n         glaubwuerdigkeit_oer_tv        glaubwuerdigkeit_privat_tv\n EHER GLAUBWUERDIG   :233        EHER GLAUBWUERDIG   :232         \n SEHR GLAUBWUERDIG   :177        EHER N. GLAUBWUERDIG:154         \n EHER N. GLAUBWUERDIG: 62        SEHR GLAUBWUERDIG   : 40         \n GAR NICHT GLAUBWUERD: 13        GAR NICHT GLAUBWUERD: 23         \n DATENFEHLER: MFN    :  0        DATENFEHLER: MFN    :  0         \n (Other)             :  0        (Other)             :  0         \n NA's                : 15        NA's                : 51         \n        glaubwuerdigkeit_zeitungen      glaubwuerdigkeit_social_media\n EHER GLAUBWUERDIG   :273          EHER N. GLAUBWUERDIG:230          \n SEHR GLAUBWUERDIG   :124          GAR NICHT GLAUBWUERD:109          \n EHER N. GLAUBWUERDIG: 53          EHER GLAUBWUERDIG   : 74          \n GAR NICHT GLAUBWUERD: 11          SEHR GLAUBWUERDIG   : 11          \n DATENFEHLER: MFN    :  0          DATENFEHLER: MFN    :  0          \n (Other)             :  0          (Other)             :  0          \n NA's                : 39          NA's                : 76          \n vertrauen_mitmenschen vertrauen_gesundheitswesen\n Min.   :1.000         Min.   :1.000             \n 1st Qu.:2.000         1st Qu.:4.000             \n Median :2.000         Median :5.000             \n Mean   :2.225         Mean   :4.975             \n 3rd Qu.:3.000         3rd Qu.:6.000             \n Max.   :3.000         Max.   :7.000             \n NA's   :15            NA's   :175               \n vertrauen_bundesverfassungsgericht vertrauen_bundestag\n Min.   :1.000                      Min.   :1.000      \n 1st Qu.:4.000                      1st Qu.:3.000      \n Median :6.000                      Median :4.000      \n Mean   :5.212                      Mean   :4.076      \n 3rd Qu.:7.000                      3rd Qu.:5.000      \n Max.   :7.000                      Max.   :7.000      \n NA's   :179                        NA's   :183        \n vertrauen_stadt_gemeindeverwaltung vertrauen_katholische_kirche\n Min.   :1.000                      Min.   :1.000               \n 1st Qu.:4.000                      1st Qu.:1.000               \n Median :5.000                      Median :2.000               \n Mean   :4.495                      Mean   :2.259               \n 3rd Qu.:5.000                      3rd Qu.:3.000               \n Max.   :7.000                      Max.   :7.000               \n NA's   :177                        NA's   :179                 \n vertrauen_evangelische_kirche vertrauen_justiz vertrauen_fernsehen\n Min.   :1.000                 Min.   :1.000    Min.   :1.000      \n 1st Qu.:1.000                 1st Qu.:4.000    1st Qu.:3.000      \n Median :3.000                 Median :5.000    Median :4.000      \n Mean   :3.016                 Mean   :4.567    Mean   :3.642      \n 3rd Qu.:4.000                 3rd Qu.:6.000    3rd Qu.:5.000      \n Max.   :7.000                 Max.   :7.000    Max.   :7.000      \n NA's   :182                   NA's   :179      NA's   :176        \n vertrauen_zeitungswesen vertrauen_hochschulen vertrauen_bundesregierung\n Min.   :1.000           Min.   :1.000         Min.   :1.00             \n 1st Qu.:3.000           1st Qu.:5.000         1st Qu.:3.00             \n Median :4.000           Median :5.000         Median :4.00             \n Mean   :4.006           Mean   :5.176         Mean   :4.08             \n 3rd Qu.:5.000           3rd Qu.:6.000         3rd Qu.:5.00             \n Max.   :7.000           Max.   :7.000         Max.   :7.00             \n NA's   :177             NA's   :177           NA's   :177              \n vertrauen_polizei vertrauen_parteien vertrauen_eu_kommission\n Min.   :1.000     Min.   :1.000      Min.   :1.000          \n 1st Qu.:4.000     1st Qu.:2.000      1st Qu.:2.000          \n Median :5.000     Median :3.000      Median :4.000          \n Mean   :4.994     Mean   :3.205      Mean   :3.516          \n 3rd Qu.:6.000     3rd Qu.:4.000      3rd Qu.:5.000          \n Max.   :7.000     Max.   :6.000      Max.   :7.000          \n NA's   :177       NA's   :178        NA's   :178            \n vertrauen_eu_parlament\n Min.   :1.000         \n 1st Qu.:2.000         \n Median :4.000         \n Mean   :3.575         \n 3rd Qu.:5.000         \n Max.   :7.000         \n NA's   :178           \n\n\nNun schauen wir uns den ALLBUS selbst – also den Gesamtdatensatz – genauer an.\n\ndaten\n\n# A tibble: 5,342 × 544\n   za_nr           doi   version respid substudy mode    splt21  eastwest german\n   &lt;dbl+lbl&gt;       &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl+lb&gt; &lt;dbl+l&gt; &lt;dbl+l&gt; &lt;dbl+lb&gt; &lt;dbl+&gt;\n 1 5280 [ALLBUS 2… http… 2.0.0 …      1 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 2 5280 [ALLBUS 2… http… 2.0.0 …      2 1 [SIMU… 4 [MAI… 1 [SPL… 1 [ALTE… 1 [JA]\n 3 5280 [ALLBUS 2… http… 2.0.0 …      3 1 [SIMU… 4 [MAI… 1 [SPL… 1 [ALTE… 1 [JA]\n 4 5280 [ALLBUS 2… http… 2.0.0 …      4 1 [SIMU… 4 [MAI… 2 [SPL… 2 [NEUE… 1 [JA]\n 5 5280 [ALLBUS 2… http… 2.0.0 …      5 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 6 5280 [ALLBUS 2… http… 2.0.0 …      6 1 [SIMU… 3 [CAW… 2 [SPL… 1 [ALTE… 1 [JA]\n 7 5280 [ALLBUS 2… http… 2.0.0 …      7 2 [SEQU… 3 [CAW… 3 [SPL… 1 [ALTE… 1 [JA]\n 8 5280 [ALLBUS 2… http… 2.0.0 …      8 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n 9 5280 [ALLBUS 2… http… 2.0.0 …      9 2 [SEQU… 3 [CAW… 3 [SPL… 2 [NEUE… 1 [JA]\n10 5280 [ALLBUS 2… http… 2.0.0 …     10 1 [SIMU… 4 [MAI… 2 [SPL… 1 [ALTE… 1 [JA]\n# ℹ 5,332 more rows\n# ℹ 535 more variables: ep01 &lt;dbl+lbl&gt;, ep03 &lt;dbl+lbl&gt;, ep04 &lt;dbl+lbl&gt;,\n#   ep06 &lt;dbl+lbl&gt;, lm01 &lt;dbl+lbl&gt;, lm02 &lt;dbl+lbl&gt;, lm19 &lt;dbl+lbl&gt;,\n#   lm20 &lt;dbl+lbl&gt;, lm21 &lt;dbl+lbl&gt;, lm22 &lt;dbl+lbl&gt;, lm14 &lt;dbl+lbl&gt;,\n#   xr19 &lt;dbl+lbl&gt;, xr20 &lt;dbl+lbl&gt;, lm27 &lt;dbl+lbl&gt;, lm28 &lt;dbl+lbl&gt;,\n#   lm29 &lt;dbl+lbl&gt;, lm30 &lt;dbl+lbl&gt;, lm31 &lt;dbl+lbl&gt;, lm32 &lt;dbl+lbl&gt;,\n#   lm33 &lt;dbl+lbl&gt;, lm34 &lt;dbl+lbl&gt;, lm35 &lt;dbl+lbl&gt;, lm36 &lt;dbl+lbl&gt;, …\n\n\nEs wird schnell klar, das weniger die Anzahl der Beobachtungen als vielmehr die Anzahle der Variablen (544) eine Herausforderung darstellt, zumal diese eher kryptische Namen wie “hp06” haben. Wie sich also zurechfinden?"
  },
  {
    "objectID": "Skript_2.3.html#die-variablen-des-allbus",
    "href": "Skript_2.3.html#die-variablen-des-allbus",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "Zum Glück lassen sich die sog. Labels, also die sprechenden Beschriftungen die sowohl Fragen and auch Antwortoptionen im ALLBUS-Stata-Datensatz haben, mittels R extrahieren (dies geschieht mit dem Paket labelled). Wir haben dies bereits vorbereitend für den ALLBUS gemacht und laden die entsprechenden Tabellen nun nur noch.\n\nvariablen &lt;- read_csv2(\"Datensatz/ALLBUS_2021_variablen.csv\", show_col_types = FALSE)\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\nvariablen_optionen &lt;- read_csv2(\"Datensatz/ALLBUS_2021_variablen_optionen.csv\", show_col_types = FALSE)\n\nℹ Using \"','\" as decimal and \"'.'\" as grouping mark. Use `read_delim()` for more control.\n\n\nEs lohnt sich, beide Objekte mittels View() oder durch eine Klick ob die beiden Objekte variablen und variablen_optionen in RStudio anzuschauen. Interessant sind die Felder variable (der Name der Variable im ALLBUS) und label (eine sprechende Beschreibung).\nSuchen wir etwa nach “hp06” finden wie die Label-Beschreibung “EPIDEMIE: STAAT DARF KRANKE ISOLIEREN”, die schon deutlich besser interpretierbar ist als “hp06”. Eine genaue Dokumentation und (vor allem wichtig) der genaue Fragetext findet sich in den Dokumenten ZA5280_fb_CAWI.pdf (Fragebogen) und ZA5280_cdb.pdf (Variablenreport). Beide sind wie der ALLBUS selbst abgelegt im Ordner Datensatz, werden aber ausserhalb von RStudio geöffnet.\nDer Fragebogen reicht normalerweise aus, um sich einen Überblick zu verschaffen, aber der Variablenreport ist dann nützlich, wenn man den Zusammenhang zwischen einem Dummywert (bspw. “4”) und dessen Bedeutung in Verbindung mit einer bestimmten Variable herausfinden möchte. Die Fragen lauter bei hp06\nUnd was denken Sie über folgende Maßnahmen: Sollte in Deutschland in Zeiten schwerer Epidemien der Staat das Recht haben, Folgendes zu tun?\nNachweislich infizierte Personen isolieren\nUnd der Dummy-Wert “4” steht bei dieser Frage für die Antwort Auf keinen Fall.\nWenn man die drei kleinen Zufallssamples mit dem Hauptdatensatz vergleicht, fällt schnell auf, dass die Samples ausschließlich (echte) Zahlenfelder für (vor allem) ordinale Likert-skalierte Variablen enthalten. Bei diesen bedeutet ein höherer Wert i.d.R. mehr Zustimmung oder eine ausgeprägtere Verhaltensausprägung gegenüber geringeren Werten. Es gibt aber auch Fälle in denen diese sog. Polarität der Variablenwerte umgedreht ist und geringe Werte “stärker” sind als hohe, order solche, in denen wir es mit nominalen Skalen zu tun haben, die Zahlen also in keinerlei logischem Zusammenhang stehen.\nWas heißt das konkret? Zur Sicherheit sollte man im Rahmen einer eigenen Analyse in den Hauptdatensatz und in die Dokumentatuin schauen um absolut sicher zu sein, dass man keine unzulässigen Umformungen oder Berechnungen vornimmt (und etwa den Mittelwert einer nominalskalierten Variable bestimmt), oder die Ergebnisse misinterpretiert (etwa wenn die Polarität einer Variablen umgedreht codiert wurde). Es gilt immer: know your data.\nZunächst ist es aber vollkommen legetim, um die Variablenliste oder das Befragungsdokument nach interessanten Variablen zu durckforsten. Wir können in der View()-Ansicht des Ojkets variable nach Begriffen suchen, die in den Labels vorkommen. Beispielsweise finden wir mit einer Suche nach dem Begriff ‘medien’ die Variablen lm35 (Nutzung von sozialen Medien als Nachrichtenquelle) und lm39 (Glaubwürdigkeit sozialer Medien mit Blick auf Kriminalität), die uns vielleicht interessieren."
  },
  {
    "objectID": "Skript_2.3.html#bildung-eines-teilsamples",
    "href": "Skript_2.3.html#bildung-eines-teilsamples",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "Ein Schritt, der praktisch für alle Studienprojekte im Verlauf des Semesters relevant sein wird, ist die Bildung eines Teildatensatzes, welcher die Variablen (und ggf. Fälle) enthält, die für Ihre Analyse relevant sind.\nTechnisch gesehen ist das gar nicht unbedingt notwendig – wir können jeder Zeit Berechnungen am Gesamtdatensatz anstellen. Aber oft ist ein Teildatensatz übersichtlicher und ermöglicht ein besseres Verständnis der Daten.\nWie bildet man ein solches Teilsample? Entscheidend ist hier die Funktion select.\n\nfernsehkonsum &lt;- daten %&gt;% \n  select(age, sex, lm02)\n\nWir extrahieren hier mittels select drei Variablen, nämlich Alter (age), Geschlecht (sex) und den Fernsehkonsum in Minuten (lm02).\nDas Ergegnis ist ein Datensatz, der weiterhin alle 5.342 Fälle, aber nur drei (statt 544) Variablen enthält.\n\nfernsehkonsum\n\n# A tibble: 5,342 × 3\n   age       sex       lm02              \n   &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;         \n 1 54        2 [FRAU]  210               \n 2 53        1 [MANN]   90               \n 3 89        2 [FRAU]  135               \n 4 79        1 [MANN]   60               \n 5 62        2 [FRAU]  180               \n 6 23        1 [MANN]   45               \n 7 31        2 [FRAU]   30               \n 8 57        2 [FRAU]   -9 [KEINE ANGABE]\n 9 68        1 [MANN]  180               \n10 51        2 [FRAU]  180               \n# ℹ 5,332 more rows\n\n\nEine gewisse Komplikation ist allerdings die Tatsache, dass in diesem Ausschnitt die Variablen sex noch eine Dummy-Zahl ist (1 = männlich, 2 = weiblich, 3 = divers) und zudem die Variable lm02 negative Werte enthät. Diese zeigen keinen negativen Fernsehkonsum an, sondern werden für Spezialwerte verwendet (“keine Angabe”, “durch Filterbedingung weggefallen”). Als Faustregel gilt: Negative Werte im ALLBUS sollten praktisch immer durch NAs ersetzt werden. Das ist unbedingt von “0” als Wert zu unterscheiden. Mit einer “echten” Null kann ebenso wie mit “echten” Negtivwerten gerechnet werden, dies führt aber zu substantiellen Verzerrungen, wenn es sich um Dummy-Werte handelt.\nDer folgenden Codeblock bereinigt die Daten zum Fernsehkomsum. Dazu benennte er die drei Variablen znnächst in transparentere Were um. Anschlißend werden negative Werte in NAs umgewandelt (hier mit der Funktion replace_with_na_all aus dem Paket naniar). Dann wird das Geschlecht faktorisiert, was die Zahlen durch das Label (also MANN / FRAU / DIVERS) ersetzt. Und schließlich werden die Labels und Attribute entfernt, die nun nicht mehr benötigt werden.\n\nfernsehkonsum_bereinigt &lt;- fernsehkonsum %&gt;% \n  rename(alter = age,\n         geschlecht = sex,\n         fernsehkonsum_minuten = lm02) %&gt;% \n  naniar::replace_with_na_all(condition = ~.x &lt; 0) %&gt;% \n  mutate(geschlecht = as_factor(geschlecht)) %&gt;% \n  labelled::remove_labels() %&gt;%\n  labelled::remove_attributes(\"format.stata\")\n\n\nfernsehkonsum_bereinigt\n\n# A tibble: 5,342 × 3\n   alter geschlecht fernsehkonsum_minuten\n   &lt;dbl&gt; &lt;fct&gt;                      &lt;dbl&gt;\n 1    54 FRAU                         210\n 2    53 MANN                          90\n 3    89 FRAU                         135\n 4    79 MANN                          60\n 5    62 FRAU                         180\n 6    23 MANN                          45\n 7    31 FRAU                          30\n 8    57 FRAU                          NA\n 9    68 MANN                         180\n10    51 FRAU                         180\n# ℹ 5,332 more rows"
  },
  {
    "objectID": "Skript_2.3.html#zusammenfassung",
    "href": "Skript_2.3.html#zusammenfassung",
    "title": "Der ALLBUS Datensatz",
    "section": "5.1 Zusammenfassung",
    "text": "5.1 Zusammenfassung\nWir sind jetzt in einer guten Position, um mit der praktischen Arbeit am ALLBUS zu beginnnen, also der Analyse und Interpretation konkreter Daten."
  },
  {
    "objectID": "Skript_6.4.html#exkurs-nicht-parametrische-testverfahren",
    "href": "Skript_6.4.html#exkurs-nicht-parametrische-testverfahren",
    "title": "Die Varianzanalyse",
    "section": "3.7 Exkurs: nicht parametrische Testverfahren",
    "text": "3.7 Exkurs: nicht parametrische Testverfahren\nWenn wir die Voraussetzungen der Varianzanalyse nicht erfüllen, können wir statt der Varianzanalyse den Kruskal-Wallis Test rechnen. Hierfür müssen unsere Daten nicht die Voraussetzungen der Normalverteilung sowie der Homogenität der Fehlervarianzen erfüllen.\nDen Kruskal-Wallis-Test rufen wir mit der Funktion kruskal.test() auf:\n\nkruskal.test(trustges ~ gesund, data = daten)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  trustges by gesund\nKruskal-Wallis chi-squared = 25.191, df = 4, p-value = 0.00004606\n\n\nHier interessiert uns insbesondere der p-Wert. Wir sehen, wie auch in der Varianzanalyse, dass sich unsere Gruppen signifikant voneinander unterscheiden. Im Anschluss müssen wir dann erneut die Posthoc Tests durchführen.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse des Kruskal-Wallis Tests werden simultan zur Varianzanalyse in Textform angegeben. Dafür werden folgende Informationen benötigt:\n✅ die Mittelwerte und Standardabweichung der einzelnen Faktorstufen\n✅ der df-Wert\n✅ der chi-squared-Wert\n✅ der p-Wert\n✅ die Posthoc-Test Ergebnisse\nDas Format ist üblicherweise:\n\nBeispiel: Personen mit einem sehr guten Gesundheitszustand haben durchschnittlich ein höheres Vertrauen in das Gesundheitswesen (M = 5.13;SD=1.45) als Personen mit einem guten (M = 4.99;SD=1.32), zufriedenstellenden (M = 4.85;SD=1.37), weniger guten (M = 4.82;SD=1.48) oder schlechtem (M = 4.71;SD=1.55) Gesundheitszustand . Der Gesundheitszustand hat dabei einen signifikanten Einfluss auf das Vertrauen in das Gesundheitswesen H(4) = 25.191, p &lt; 0.001. Post-Hoc Paarvergleiche mit Tamhames ergaben, dass sich der Mittelwert für die Personen mit sehr guten Gesundheitszustand signifikant von den Personen mit zufriedenstellendem (p&lt;0.029) und weniger gutem (p&lt;0.0174) Zustand unterscheidet. Die anderen Gesundheitsgruppen unterscheiden sich hingegen nicht signifikant voneinander.\n\n\n\n\n\n\n\n\n\nExkurs: Einfaktorielle Varianzanalyse mit Messwiederholung\n\n\n\n\n\n3.8 Umgang mit Messwiederholungen\nHaben wir mehr als zwei Testzeitpunkte vorliegen, so können wir die ANOVA mit Messwiederholung rechnen. Die ANOVA mit Messwiederholung weitet dabei den Gedanken der t-Test für abhängige Stichproben weiter aus. Solltet ihr euch für dieses Verfahren interessieren, findet ihr hier weitere Infos dazu."
  },
  {
    "objectID": "Skript_6.4.html#exkurs-einfaktorielle-varianzanalyse-mit-messwiederholung-1",
    "href": "Skript_6.4.html#exkurs-einfaktorielle-varianzanalyse-mit-messwiederholung-1",
    "title": "Die Varianzanalyse",
    "section": "3.8 Exkurs: Einfaktorielle Varianzanalyse mit Messwiederholung",
    "text": "3.8 Exkurs: Einfaktorielle Varianzanalyse mit Messwiederholung\nHaben wir mehr als zwei Testzeitpunkte vorliegen, so können wir die ANOVA mit Messwiederholung rechnen. Die ANOVA mit Messwiederholung weitet dabei den Gedanken der t-Test für abhängige Stichproben weiter aus. Solltet ihr euch für dieses Verfahren interessieren, findet ihr hier weitere Infos dazu."
  },
  {
    "objectID": "Skript_6.4.html#umgang-mit-messwiederholungen",
    "href": "Skript_6.4.html#umgang-mit-messwiederholungen",
    "title": "Die Varianzanalyse",
    "section": "3.8 Umgang mit Messwiederholungen",
    "text": "3.8 Umgang mit Messwiederholungen\nHaben wir mehr als zwei Testzeitpunkte vorliegen, so können wir die ANOVA mit Messwiederholung rechnen. Die ANOVA mit Messwiederholung weitet dabei den Gedanken der t-Test für abhängige Stichproben weiter aus. Solltet ihr euch für dieses Verfahren interessieren, findet ihr hier weitere Infos dazu."
  },
  {
    "objectID": "Skript_6.4.html#visualisierung-der-interaktionen",
    "href": "Skript_6.4.html#visualisierung-der-interaktionen",
    "title": "Die Varianzanalyse",
    "section": "4.3 Visualisierung der Interaktionen",
    "text": "4.3 Visualisierung der Interaktionen\nUm einen möglichen Interaktionseffekt auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\n1emmip(fit2, agef ~ gesund) +\n2  labs(title = \"Geschätzes Randmittel von Vertrauen in das Gesundheitswesen\",\n       y = \"Geschätzte Randmittel\",\n       x = \"Gesundheitszustand\") +\n3  scale_color_manual(values = c(\"darkgreen\", \"tan4\"), name = \"Altersgruppe\") +\n4  rotate_x_text(45)\n\n\n1\n\nInnerhalb von emmip geben wir zunächst unser Modell der Varianzanalyse an und spezifizieren anschließend durch eine Tilde getrennt unsere Variablen. Die Reihenfolge der Variablen kann hier gerne variiert werden um anschließend zu schauen, wierum man die Grafik besser interpretieren kann.\n\n2\n\nMit labs geben die Bezeichnung für den Titel sowie die Achsen der Grafik an.\n\n3\n\nMit scale_color_manual legen wir eigene Farben für unseren Plot fest. Mittels name = \"Altersgruppe\" ändern wir den Titel der Plotlegende.\n\n\n4\n\nWir drehen unsere Grafikbeschriftung der x-Achse um 45 Grad, damit auch lange Labels lesbar sind.\n\n\n\n\n\n\n\nAnhand der Grafik werden die Interaktionseffekte sowie die Gruppenunterschiede der Posthoc-Tests noch einmal verdeutlicht: Während bei jungen Personen mit einem sehr guten Gesundheitszustand das Vertrauen in das Gesundheitswesen am höchsten ausgeprägt ist, nimmt dieses anschließend steil ab. Bei älteren Menschen wiederum ist das Vertrauen in das Gesundheitswesen bei Personen mit einem sehr guten Gesundheitszustand zwar geringer als bei jüngeren Personen mit einem sehr guten Gesundheitszustand, nimmt anschließend aber zu und im weiteren Verlauf weit weniger drastisch ab. Daher wird auch deutlich, weswegen wir eine solch hohe Anzahl an signifikanten Gruppenunterschieden zwischen den verschiedenen Gesundheitszuständen bei alten und jungen Leuten finden konnten.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der mehrfaktoriellen Varianzanalyse werden zumeist in Textform dargestellt. Dafür werden folgende Informationen benötigt:\n✅ die Mittelwerte und Standardabweichung der einzelnen Faktorstufen\n✅ der df-Wert\n✅ der F-Wert\n✅ der p-Wert\n✅ die jeweiligen pes-Wert\n✅ die Posthoc-Test Ergebnisse\nDas Format ist üblicherweise:\n\nBeispiel: Personen mit einem sehr guten Gesundheitszustand haben durchschnittlich ein höheres Vertrauen in das Gesundheitswesen (M = 5.13;SD=1.45) als Personen mit einem guten (M = 4.99;SD=1.32), zufriedenstellenden (M = 4.85;SD=1.37), weniger guten (M = 4.82;SD=1.48) oder schlechtem (M = 4.71;SD=1.55) Gesundheitszustand. Des Weiteren haben ältere Personen ein höheres Vertrauen in das Gesundheitswesen (M = 5.05;SD=1.36) als jüngere Personen (M = 4.85;SD=1.40). Der Gesundheitszustand (F3465)=7,66,p&lt;0,001 ηp2 = und das Alter (F3465)=14,63,p&lt;0,001 haben jeweils einen signifikanten Einfluss auf das Vertrauen in das Gesundheitswesen. Post-Hoc Paarvergleiche mit Tukey ergaben, dass sich der Mittelwert für junge Personen mit sehr guten Gesundheitszustand signifikant von jungen Personen mit gutem (p=.045), zufriedenstellendem (p&lt;0.001) und weniger gutem (p&lt;0.001) Zustand unterscheidet. Des Weiteren unterscheiden sich junge Personen mit guten Gesundheitszustand von jungen Personen mit zufriedenstellenden Gesundheitszustand (p=.01) sowie von alten Personen mit guten Gesundheitszustand (p=.018). Darüber hinaus unterscheiden sich junge Personen mit einem zufriedenstellenden Gesundheitszustand von alten Personen mit einem sehr guten (p=.03), guten (p&lt;0.001), zufriedenstellenden (p&lt;0.001) und weniger guten (p=.01) Gesundheitszustand. Zuletzt unterscheiden sich junge Personen mit einem weniger guten Gesundheitszustand von alten Personen mit einem guten (p&lt;0.001), zufriedenstellenden (p.005) und weniger guten Gesundheitszustand (p=.0489). Das Ergebnise zeigt zudem eine signifikante Interaktion von Gesundheitszustand und Alter ((F3465)=2,93, p=0,020, ηp2 = .003) auf das Vertrauen in das Gesundheitswesen. Dies weist darauf hin, dass sich der Gesundheitszustand je nach Altersgruppe unterschiedlich auswirkt. Bei einem sehr guten Gesundheitszustand ist das Vertrauen in das Gesundheitswesen bei jungen Menschen (M = 5.15, SD = 1.42) stärker ausgeprägt als bei alten Menschen (M = 5.05, SD = 1.57). Bei allen anderen Gesundheitszuständen ist es genau umgekehrt. Hier haben jeweils ältere Menschen ein höheres Vertrauen in das Gesundheitswesen als jüngere Menschen."
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-rangsummen-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-rangsummen-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "1.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Rangsummen-Test",
    "text": "1.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Rangsummen-Test\nWilcoxon-Rangsummentest Historie der Testverfahren"
  },
  {
    "objectID": "Skript_6.3.html#data-management",
    "href": "Skript_6.3.html#data-management",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.1 Data Management",
    "text": "3.1 Data Management\nDazu benennen wir zunächst die entsprechende Variable aus den Allbus-Daten in einkommen um und filtern fehlende Werte hinaus.\n\ndaten_one &lt;- daten %&gt;% \n  filter(., di01a &gt; 0) %&gt;% \n  rename(., einkommen = di01a)"
  },
  {
    "objectID": "Skript_6.3.html#voraussetzungsprüfung-2",
    "href": "Skript_6.3.html#voraussetzungsprüfung-2",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Voraussetzungsprüfung",
    "text": "3.2 Voraussetzungsprüfung\nDer Einstichprobentest setzt ähnlich wie der two-sided und paired test eine Normalverteilung der abhängigen Variablen voraus. Diese können wir erneut mit dem Shapiro-Wilk-Test überprüfen:\n\ndaten_one  %&gt;%  \n  do(tidy(shapiro.test(.$einkommen)))\n\n# A tibble: 1 × 3\n  statistic  p.value method                     \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;                      \n1     0.831 3.73e-28 Shapiro-Wilk normality test\n\n\nDa unser Ergebnis signifikant ausfällt, müssen wir hier leider die Normalverteilung unserer AV ablehnen (wir testen ja auf signifikante Abweichung von der Normalverteilung, daher möchten wir bei diesem Test nicht ein signifikantes Ergebnis haben). Um weitere Gewissheit zu erlangen, prüfen wir die Normalverteilung zusätzlich optisch mit einem Histogramm:\n\n1ggplot(daten_one, aes(einkommen)) +\n2  geom_histogram(aes(y = after_stat(count)),\n                 color = \"black\", fill = \"grey\", \n3                 binwidth = 500) +\n4  labs(x = \"Netto-Monatseinkommen\",\n         y = \"\")\n\n\n1\n\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier daten_one) und unter aes unsere Variable (hier einkommen).\n\n2\n\nDie Spezifizierungen innerhalb der Klammern unseres Histogramms geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = after_stat(count)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik.\n\n3\n\nMit binwidth = 500 verweisen wir hier auf die Breite der Balken unseres Histogramms.\n\n4\n\nFür ein verschönertes Aussehen unseres Graphen nutzen wir den Befehl labs um zusätzlich die Achsen zu beschriften.\n\n\n\n\n\n\n\nDie Grafik verstärkt die Einschätzung des Shapiro-Wilk-Tests, dass keine Normalverteilung vorliegt. In einem solchen Fall würden wir auf den nicht-parametrische one-sample Wilcoxon-Test ausweichen."
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-one-sample-wilcoxon-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test-one-sample-wilcoxon-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.4 Nicht-parametrischer Test: one sample Wilcoxon Test",
    "text": "3.4 Nicht-parametrischer Test: one sample Wilcoxon Test\n\nwilcox.test(daten_one$einkommen, mu = 3813)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  daten_one$einkommen\nV = 30680, p-value &lt; 2.2e-16\nalternative hypothesis: true location is not equal to 3813"
  }
]