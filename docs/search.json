[
  {
    "objectID": "Skript_7.3.html",
    "href": "Skript_7.3.html",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei mehr als zwei Variablen"
  },
  {
    "objectID": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Notebook gehen wir (wie angekündigt) zunächst näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Dann lernen wir die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren.\n\n1.1.1 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ESS8_vier_laender-Datensatzes, den wir entsprechend einlesen:\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\")\n#install.packages(\"lmtest\")\n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n#install.packages(\"sandwich\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\nlibrary(lmtest)\n\nWarning: Paket 'lmtest' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: zoo\n\n\nWarning: Paket 'zoo' wurde unter R Version 4.3.1 erstellt\n\n\n\nAttache Paket: 'zoo'\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\nWarning: Paket 'sandwich' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n1.1.2 Data Management\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die Internetnutzung (netustm); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Rezeptionszeit von politischen Nachrichten (nwspol) sowie das Geschlecht der Befragten (gndr) an. Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl um.\nDann setzen wir den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifizieren wir wieder, auf welche Variablen sich der Befehl beziehen soll). Das modifizierte Datenset weisen wir einem neuen Datenobjekt zu: daten_mod2\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(internetnutzung = netustm,\n         alter = agea,\n         politische_Nachrichtenrezeption = nwspol,\n         gender = gndr) %&gt;% \n  drop_na(c(internetnutzung, alter, politische_Nachrichtenrezeption, gender)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 158 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\n\n1.1.3 Erinnerung: Einfache lineare Regression mit Alter als UV und Internetnutzung als AV mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nsummary(model) # klassischer Output mit relevanten Kennzahlen\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-211.39  -97.38  -44.70   53.71  487.73 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 279.2690    45.2122   6.177 0.0000000149 ***\nalter        -1.9585     0.9218  -2.125       0.0361 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 150 on 98 degrees of freedom\nMultiple R-squared:  0.04404,   Adjusted R-squared:  0.03428 \nF-statistic: 4.514 on 1 and 98 DF,  p-value: 0.03613\n\n#summary(lm.beta(model)) # klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Internetnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um “den Estimate-Wert” in Messeinheiten (hier: -4.296 Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit dem Teil A dieses Skripts, nämlich der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern Sie sich, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Sie haben Recht: Eigentlich würden wir erst die Voraussetzungen prüfen, dann das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen.\n\n\n1.1.4 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)\n\n\n1.1.5 TEIL A: Prüfung der Voraussetzungen einer Regressionsanalyse\n\n1.1.5.1 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt.\n\n\n1.1.5.2 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß – unabhängig wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde.\n\ncheck_heteroscedasticity(model)\n\nOK: Error variance appears to be homoscedastic (p = 0.329).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). In diesem Fall liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\n\n\n1.1.5.3 Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei höheren Werten erkennbar ist, weil wir einen leicht nach rechts geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\n\n\n1.1.5.4 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 279.2690    44.7423  6.2417 0.00000001109 ***\nalter        -1.9585     0.8609 -2.2749       0.02509 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!\n\n\n1.1.5.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.294).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0,588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!\n\n\n1.1.5.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.\n\n\n1.1.5.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, kann ich wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n1.1.5.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr coole Funktion, die mir eine visuelle Inspektion meiner Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion kann ich mir dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\n\ncheck_model(model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n\n1.1.6 TEIL B: Die multiple lineare Regression\n\n1.1.6.1 Anwendungsbereich der multiplen linearen Regression\nDie multiple lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen metrischen Variablen besteht. Die multiple lineare Regressionsanalyse hat das Ziel, eine abhängige Variable (y) mittels mehrerer unabhängigen Variablen (x1, x2, …) zu erklären. (Zur Erinnerung: Für nur eine x-Variable nutzen wir die einfache lineare Regression)\nMit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen den unabhängigen und der einen abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variablen vorhergesagt werden?\nDie multiple Regression entspricht in ihrer Analyslogik also der einfachen linearen Regression - nur dass sie mehr als eine unabhängige Variable berücksichtigt.\n\n\n1.1.6.2 Ziel der Analyse\nMit Hilfe der multiplen Regression wollen wir die Annahme prüfen, dass die Variablen Alter (agea) sowie die Rezeptionszeit von politischen Nachrichten (nwspol) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) haben bzw. diese erklären und vorhersagen können. Alle Variablen sind metrisch und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n1.1.6.3 Modell zum Zusammenhang von Alter, politischer Nachrichtenrezeption und Internetnutzung spezifizieren und anzeigen lassen\nDie Berechnung der multiplen Regression unterscheidet sich nicht stark von der Berechnung der einfachen linearen Regression. In die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) einfach die zusätzliche unabhängige Variable (politische_Nachrichtenrezeption) ein, indem wir sie mit einem + Zeichen anhängen:\n\nmodel_m &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption, data = daten_mod)\nsummary(lm.beta(model_m))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption, \n    data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-203.71  -92.77  -47.73   38.81  457.27 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     261.8715           NA    44.9628   5.824\nalter                            -2.0528      -0.2200     0.9041  -2.271\npolitische_Nachrichtenrezeption   0.2785       0.2186     0.1234   2.256\n                                    Pr(&gt;|t|)    \n(Intercept)                     0.0000000745 ***\nalter                                 0.0254 *  \npolitische_Nachrichtenrezeption       0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147 on 97 degrees of freedom\nMultiple R-squared:  0.09171,   Adjusted R-squared:  0.07299 \nF-statistic: 4.897 on 2 and 97 DF,  p-value: 0.009415\n\n\n\n\n1.1.6.4 Interpretation des Outputs: Was sehen wir in der Regressionstabelle?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängigen Variablen “alter” und “politische_Nachrichtenrezeption” zu erklären.\n\n\n1.1.6.5 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n1.1.6.6 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n1.1.6.7 Standardized\nDiese Estimates sind die standardisierte b-Werte. Weil wir diese über die lm.beta-Funktion standardisiert haben, lassen sich die Koeefizienten auch bei unterschiedlicher Skalierung vergleichen.\n\n\n1.1.6.8 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n1.1.6.9 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n1.1.6.10 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n1.1.6.11 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter und politische Nachrichtenrezeption auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter und die politische Nachrichtenrezeption etwa 20 Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n1.1.6.12 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n1.1.6.13 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n1.1.6.14 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (-4.9169) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &lt; .05 statistisch signifikant. Diese Ergebnisse überraschen uns nicht: Den Einfluss des Alters haben wir ja letzte Woche schon überprüft. Mit der Erweiterung zur multiplen Regression können wir nun zusätzlich sagen, dass die politische Nachrichtenrezeption auch einen Einfluss auf die Internet-Nutzung hat, denn der Wert ist ebenfalls signifikant (p &lt; .05)! Die F-Statistik sagt uns zusätzlich, dass auch unser Gesamtmodell signifikant ist (p-value: 0.00001239, also &lt; .05).\n\n\n1.1.6.15 Erinnerung: Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model_m))\n\n# A tibble: 3 × 6\n  term                         estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;                           &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)                   262.          NA        45.0        5.82 7.45e-8\n2 alter                          -2.05        -0.220     0.904     -2.27 2.54e-2\n3 politische_Nachrichtenrezep…    0.278        0.219     0.123      2.26 2.63e-2\n\n\n\nglance(model_m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0917        0.0730  147.      4.90 0.00941     2  -639. 1287. 1297.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "href": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden",
    "text": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.3 Prüfung der Voraussetzungen",
    "text": "1.3 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "href": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.4 Berechnung und Interpretation einer multiplen Regression",
    "text": "1.4 Berechnung und Interpretation einer multiplen Regression"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Skript_1.1.html",
    "href": "Skript_1.1.html",
    "title": "Einführung in R und RStudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "Skript_1.2.html",
    "href": "Skript_1.2.html",
    "title": "R-Studio",
    "section": "",
    "text": "1 Wie funktioniert R-Studio?"
  },
  {
    "objectID": "Skript_1.3.html",
    "href": "Skript_1.3.html",
    "title": "Die Logik von Markdown und Quarto",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "Skript_2.1.html",
    "href": "Skript_2.1.html",
    "title": "Der Aufbau von Datensätzen",
    "section": "",
    "text": "1 Was ist ein Datensatz und wie ist dieser aufgebaut?\n\n\n2 Klassische Formen von Datensätzen in der Kommunikationswissenschaft"
  },
  {
    "objectID": "Skript_2.2.html",
    "href": "Skript_2.2.html",
    "title": "klassische Formen von Datensätzen",
    "section": "",
    "text": "1 Einlesen von Datensätzen und Praktische Tipps"
  },
  {
    "objectID": "Skript_2.3.html",
    "href": "Skript_2.3.html",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "1 Der ALLBUS"
  },
  {
    "objectID": "Skript_3.2.html",
    "href": "Skript_3.2.html",
    "title": "Datentypen und -strukturen",
    "section": "",
    "text": "Bild von Jae Rue auf Pixabay"
  },
  {
    "objectID": "Skript_3.3.html",
    "href": "Skript_3.3.html",
    "title": "Selektion, Manipulation und Transformation",
    "section": "",
    "text": "Bild von Gerd Altmann auf Pixabay"
  },
  {
    "objectID": "Skript_3.4.html",
    "href": "Skript_3.4.html",
    "title": "Tabellen und Grafiken in R",
    "section": "",
    "text": "Dieses Notebook illustriert verschiedene Möglichkeiten dafür, wie auf Grundlage der ALLBUS-Daten informative Tabellen und Plots erstellt werden können. Tabellen und Datenvisualisierungen (“Plots”) stellen zwei sehr effektive Möglichkeiten dar, sich schnell einen Überblick über komplexe Daten zu verschaffen. Dabei lassen sich oftmals Zusammenhänge zwischen zwei oder mehr Variablen erahnen, die dann später inferenzstatistisch auf die Solidität ihres Zusammenhangs hin überprpüft werden können.\nZunächst laden wir die die Pakete des tidyverse. Die Einstellung theme_set hat einen Einfluss auf die Grafiken, die wir später erstellen werden.\n\nlibrary(tidyverse)\n\nWarning: Paket 'tidyverse' wurde unter R Version 4.3.1 erstellt\n\n\nWarning: Paket 'tidyr' wurde unter R Version 4.3.1 erstellt\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(naniar)\n\nWarning: Paket 'naniar' wurde unter R Version 4.3.1 erstellt\n\nlibrary(haven)\ntheme_set(theme_minimal())\n\nJetzt laden wir den ALLBUS-Datensatz mittels der Stata-Importfunktion read_dta.\n\ndaten &lt;- read_dta(\"Datensatz/Allbus_2021.dta\")\n\nNun laden wir zudem noch drei kleinere Zufallssamples aus dem Gesamtdatensatz. Diese bestehen aus einer kleineren Anzahl relevanter Variablen und sind daher etwas übersichtlicher als der Hauptdatensatz.\n\nsample_klein &lt;- read_rds(\"Datensatz/ALLBUS_sample_klein.rds\")\nsample_mittel &lt;- read_rds(\"Datensatz/ALLBUS_sample_mittel.rds\")\nsample_gross &lt;- read_rds(\"Datensatz/ALLBUS_sample_gross.rds\")\n\nWie sehen die Daten aus?\n\nsample_klein\n\n# A tibble: 20 × 4\n   alter geschlecht bildung            fernsehkonsum\n   &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;                      &lt;dbl&gt;\n 1    37 MANN       FACHHOCHSCHULREIFE             3\n 2    38 MANN       MITTLERE REIFE                 7\n 3    47 FRAU       VOLKS-,HAUPTSCHULE             6\n 4    66 MANN       HOCHSCHULREIFE                 2\n 5    47 FRAU       HOCHSCHULREIFE                 7\n 6    75 MANN       VOLKS-,HAUPTSCHULE             7\n 7    41 FRAU       MITTLERE REIFE                 4\n 8    18 MANN       NOCH SCHUELER                  7\n 9    91 MANN       &lt;NA&gt;                          NA\n10    56 MANN       HOCHSCHULREIFE                 4\n11    58 MANN       HOCHSCHULREIFE                 7\n12    32 MANN       MITTLERE REIFE                 2\n13    47 MANN       FACHHOCHSCHULREIFE             1\n14    49 MANN       MITTLERE REIFE                 1\n15    23 MANN       MITTLERE REIFE                 0\n16    48 FRAU       MITTLERE REIFE                 0\n17    49 MANN       HOCHSCHULREIFE                 0\n18    36 MANN       FACHHOCHSCHULREIFE             7\n19    70 MANN       HOCHSCHULREIFE                 7\n20    43 FRAU       MITTLERE REIFE                 7"
  },
  {
    "objectID": "Skript_4.1.html",
    "href": "Skript_4.1.html",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Zunächst ein paar Begriffserläuterungen: Bei der quantitativen Datenerhebung misst man die Ausprägung eines bestimmten Merkmals. Ein Merkmal kann zum Beispiel das Alter einer Person sein. Die Person ist dann der Merkmalsträger. Die Merkmalsausprägung bzw. der Messwert ist die konkrete Altersangabe, z.B. 23 Jahre. Die Menge aller Merkmalsträger, über die man durch die Untersuchung zu Erkenntnissen kommen will, heißt Grundgesamtheit. Oft kann man aber gar nicht oder nur sehr schwer die ganze Grundgesamtheit heranziehen und muss sich mit einer Teilmenge begnügen: Der Stichprobe. Die Daten sind erhoben, die Arbeitsumgebung ist eingerichtet, wir haben die Datensätze geladen und wissen, wie wir sie bearbeiten und visualisieren können. Nun wird es Zeit, sich einen ersten Eindruck von den statistischen Eigenschaften unserer Daten zu verschaffen. Von diesen Eigenschaften hängt ab, welche Methoden wir auf sie anwenden können und somit, welche Fragen wir mit ihnen überhaupt beantworten können. Dabei interessieren uns zunächst zwei Aspekte, erstens: Welche Methoden passen zur Beschaffenheit der Daten und zweitens: Welche inhaltlichen Eigenschaften können wir darauf aufbauend in den Daten erkennen? Ersteres ist abhängig vom Skalenniveau (nominal, ordinal, kardinal), für letzteres schauen wir uns einige der wichtigsten Maßzahlen (Parameter) an.\n\n\nDurch eine Messung wird bestimmten Eigenschaften eines Merkmalsträgers ein bestimmter Wert zugeordnet. Dazu bedient man sich einer festgelegten Skala, damit alle Ergebnisse miteinander vergleichbar sind. Wir kennen solche Skalen aus unserem Alltag. Manche bestehen aus Zahlen (z.B. Maßbänder, Uhren), andere aus Symbolen (z.B. Vereinslogos). Das Niveau einer Skala hängt von ihrem Informationsgehalt ab. Je höher das Skalenniveau, desto vielfältiger ist die Auswahl an Methoden, die man auf die Daten anwenden kann und desto höher ist ihr Informationsgehalt. Allerdings neigen Methoden, die höhere Skalenniveaus erfordern, oft dazu, anfälliger gegenüber Ausreißern zu sein. Bei der Einschätzung des Skalenniveaus ist es hilfreich zu prüfen, ob ein Merkmal diskret oder kontinuierlich ist. Diskret ist ein Merkmal, wenn es abzählbar viele Ausprägungen annehmen kann, z.B. die Anzahl an Haustieren: Unter 2 Katzen kann sich jeder was vorstellen, bei 1,5 Katzen wird es hingegen schwierig. Dagegen gibt es bei einem kontinuierlichen Merkmal zwischen zwei Ausprägungen unendlich viele weitere, z.B. bei der Körpergröße: Hier sind nicht nur 1, 2 oder 199 cm denkbar, sondern auch jeder beliebig genaue weitere Wert dazwischen. Dafür macht eine Körpergröße von -5 cm wieder keinen Sinn.\n\n\nEs kann nur eine Aussage darüber gemacht werden, ob zwei Objekte gleich sind, aber es gibt keine sinnvolle hierarchische Einteilung, d.h. es kann keine Rangfolge im Sinne von “x ist größer/kleiner als y” erstellt werden. Beispiele: Geschlecht, Nationalität. Diese Skala setzt diskrete Merkmale voraus.\n\n\n\nEs kann nicht nur festgestellt werden, ob sich zwei Objekte gleichen, sondern man kann ihnen Eigenschaften zuordnen, anhand derer sie sich in eine bestimmte Reihenfolge bringen lassen. Sie können also auch sortiert werden. Das kann sowohl diskrete Messwerte beinhalten, die nur nach Rängen geordnet werden können, als auch kontinuierliche Messwerte, bei denen man die Differenz zwischen zwei Werten genau bestimmen kann. Beispiele: Schulnoten,\n\n\n\n\nIntervall: Man kann die Abstände zwischen zwei Messwerten genau berechnen. Es gibt allerdings keinen natürlichen Nullpunkt. Beispiele: Temperatur in Grad Celsius (willkürlich festgelegter Nullpunkt).\nVerhältnis: Wie die Intervallskala, aber es gibt einen natürlichen Nullpunkt. Beispiele: Distanz zwischen zwei Orten, Körpergröße.\n\n\n\n\n\nZur Beschreibung insbesondere nominaler Merkmale ist der Begriff der Häufigkeit wichtig.\n\n\nDie absolute Häufigkeit gibt an, wie oft eine bestimmte Merkmalsausprägung im Datensatz vorkommt. Die absolute Häufigkeit kann folglich nur eine natürliche Zahl sein.\n\n\n\nDie relative Häufigkeit gibt den Anteil eines bestimmten Messwertes im Datensatz an. Sie berechnet sich, indem man die absolute Häufigkeit des jeweiligen Messwertes durch die Gesamtgröße des Datensatzes teilt. Die relativen Häufigkeiten summieren sich also zu 1 auf.\n\n\n\n\nWir werden hier, abhängig vom Skalenniveau, zwei verschiedene Arten von Maßzahlen betrachten. Das ist zum einen das Lagemaß, das uns etwas über die zentrale Tendenz der Daten sagt, also wo sich besonders viele Messwerte häufen bzw. wo der zentrale Punkt ist, um den sich die Messwerte gruppieren. Zum anderen schauen wir uns verschiedene Streuungsparameter an, mit deren Hilfe wir einschätzen können, wie stark unsere Messwerte vom Lageparameter abweichen.\n\n\n\nIm Folgenden werden wir aus dem Allbus-2021-Datensatz ein paar Beispiele herausgreifen, um die Berechnung und Visualisierung von Häufigkeiten und Parametern zu demonstrieren. Dazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(tidyverse, ggplot2, haven, dplyr) \n\nDann legen wir den Visualisierungshintergrund fest:\n\ntheme_set(theme_classic()) \n\nNun laden wir den Allbus-Datensatz:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nBei den Häufigkeiten beschränken wir uns auf einen nominal skalierten Datensatz: Das Geschlecht (“sex”).\nAnschließend konvertieren wir die Daten zu Zahlenwerten und entfernen fehlerhafte Daten:\n\nallbus_messniveau_bsp &lt;- subset(daten, select=c(\"sex\")) %&gt;%\n  mutate(across(c(\"sex\"), ~ as.numeric(.))) %&gt;%\n  mutate(across(c(\"sex\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%\n  na.omit()\n\n\n\n\nDie absoluten Häufigkeiten lassen sich einfach mit der table-Funktion abfragen. Im folgenden Codebeispiel schauen wir uns dazu die Häufigkeiten im Datensatz “sex” an. Uns werden zwei Zeilen ausgegeben: Die erste Zeile enthält den Tabellenkopf (1: männlich, 2: weiblich, 3: divers). Die zweite Zeile enthält zu jeder Merkmalsausprägung die zugehörige Anzahl (= absolute Häufigkeit):\n\nabs_freq_sex &lt;- table(allbus_messniveau_bsp$sex) \nlength_sex &lt;- length(allbus_messniveau_bsp$sex)\n\nabs_freq_sex        # Ausgabe der Tabelle\n\n\n   1    2    3 \n2614 2705    3 \n\n\nInsgesamt summieren sie sich (wie erwartet) zu 5322 Merkmalsträgern auf, was auch der Größe des Datensatzes entspricht:\n\nsum(abs_freq_sex)   # Ausgabe der Summe der absoluten Häufigkeiten\n\n[1] 5322\n\nlength_sex          # Ausgabe der Gesamtanzahl an Merkmalsträgern im Datensatz\n\n[1] 5322\n\n\nDie relativen Häufigkeiten können wir uns ausgeben lassen, indem wir den Inhalt der Tabelle durch die Gesamtanzahl der Merkmalsträger teilen:\n\nrel_freq_sex &lt;- abs_freq_sex / length_sex # Berechnung der relativen Häufigkeiten\nrel_freq_sex        # Ausgabe der Tabelle mit den relativen Häufigkeiten\n\n\n           1            2            3 \n0.4911687336 0.5082675686 0.0005636979 \n\n\nErwartungsgemäß summieren sich die relativen Häufigkeiten zu 1 auf:\n\nsum(rel_freq_sex) # Ausgabe der Summe der relativen Häufigkeiten\n\n[1] 1\n\n\nEine Möglichkeit der Darstellung von Häufigkeiten ist das Kuchendiagramm. Zu diesem Zweck erstellen wir einen eigenen Dataframe:\n\ndf.sex &lt;- data.frame(\n  sex=c(\"male\", \"female\", \"diverse\"), \n  frequency=abs_freq_sex\n)\n\nggplot(df.sex, aes(x=\"\", y=frequency.Freq, fill=sex)) +\n  geom_bar(stat=\"identity\") +\n  coord_polar(\"y\")\n\n\n\n\nAuf den ersten Blick lässt sich so ein generelles Bild des Datensatzes machen: Es sind in etwa so viele Männer wie Frauen im Datensatz, wobei Frauen etwas stärker vertreten sind. Personen, die sich weder als Mann noch als Frau identifizieren, kommen nur in sehr geringer Zahl vor."
  },
  {
    "objectID": "Skript_4.2.html",
    "href": "Skript_4.2.html",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "",
    "text": "Berechnung von Lageparametern\nLaden wir zunächst wieder unsere Daten: Da wir uns drei verschiedene Skalenniveaus ansehen wollen, verwenden wir Daten, die jeweils ein solches repräsentieren. Wir laden daher Daten zu Geschlecht (“sex”), Vertrauen in die Bundesregierung (“pt12”) und Netto-Einkommen (“di01a”).\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(tidyverse, ggplot2, haven, dplyr)  \n\n\ntheme_set(theme_classic()) \n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\") \n\nallbus_df &lt;- subset(daten, select=c(\"sex\", \"pt12\", \"di01a\")) %&gt;%          \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ as.numeric(.))) %&gt;%                                     \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%          \n  na.omit()   \n\ncolnames(allbus_df) &lt;- c(\"Geschlecht\", \"VertrauenBR\", \"Einkommen\") # Umbenennen der Spaltennamen"
  },
  {
    "objectID": "Skript_4.2.html#berechnung",
    "href": "Skript_4.2.html#berechnung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung",
    "href": "Skript_4.2.html#visualisierung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation",
    "href": "Skript_4.2.html#interpretation",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-1",
    "href": "Skript_4.2.html#berechnung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-1",
    "href": "Skript_4.2.html#visualisierung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-1",
    "href": "Skript_4.2.html#interpretation-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-2",
    "href": "Skript_4.2.html#berechnung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-2",
    "href": "Skript_4.2.html#visualisierung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-2",
    "href": "Skript_4.2.html#interpretation-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html",
    "href": "Skript_4.3.html",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "",
    "text": "Im letzten Abschnitt haben wir uns angesehen, mit welchen Maßen sich etwas darüber sagen lässt, um welche Punkte sich die Masse der Messwerte in unserem Datensatz konzentriert. Jetzt wollen wir wissen, wie stark die Messwerte streuen, also wie stark die Messwerte vom Lageparameter abweichen. Zunächst laden wir die benötigten Daten:\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(tidyverse, ggplot2, haven, dplyr)  \n\n\ntheme_set(theme_classic()) \n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\") \n\nallbus_df &lt;- subset(daten, select=c(\"sex\", \"pt12\", \"di01a\")) %&gt;%          \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ as.numeric(.))) %&gt;%                                     \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%          \n  na.omit()   \n\ncolnames(allbus_df) &lt;- c(\"Geschlecht\", \"VertrauenBR\", \"Einkommen\") # Umbenennen der Spaltennamen"
  },
  {
    "objectID": "Skript_4.3.html#berechnung",
    "href": "Skript_4.3.html#berechnung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung",
    "href": "Skript_4.3.html#visualisierung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation",
    "href": "Skript_4.3.html#interpretation",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-1",
    "href": "Skript_4.3.html#berechnung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-1",
    "href": "Skript_4.3.html#visualisierung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-1",
    "href": "Skript_4.3.html#interpretation-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-2",
    "href": "Skript_4.3.html#berechnung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-2",
    "href": "Skript_4.3.html#visualisierung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-2",
    "href": "Skript_4.3.html#interpretation-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html",
    "href": "Skript_4.4.html",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "",
    "text": "Wir haben jetzt eine Menge verschiedener Methoden kennengelernt, mit denen wir uns einen Eindruck davon machen können, wie unsere Daten verteilt sind. Um den Begriff der Verteilung an sich haben wir dagegen bis hierher einen Bogen gemacht. Dabei spielt die Art der Verteilung für viele weiterführende Anwendungen eine große Rolle. Manche statistische Verfahren setzen spezifische Verteilungen voraus.\nStatistische Verteilungen stellen eine Verbindung her zwischen den empirisch gewonnenen Daten und auf Wahrscheinlichkeit basierenden Aussagen, die wir daraus ableiten wollen. Dabei steht die Frage im Zentrum, mit welcher Wahrscheinlichkeit ein bestimmtes Zufallsereignis eintritt bzw. wie wahrscheinlich es ist, dass eine zufällig gezogene Stichprobe so ausfällt, wie es beobachtet wurde. Können wir annehmen, dass ein Merkmal auf eine bestimmte Weise verteilt ist, können wir eine Prognose abgeben, in welchem Rahmen eine Stichprobe liegen wird. Interessant wird es insbesondere dann, wenn sich nachher zeigt, dass die theoretische Schätzung falsch war und völlig andere Ergebnisse herauskommen. Denn das bedeutet, dass unsere Daten von Faktoren beeinflusst werden, die wir nicht mit eingerechnet haben.\nWir hätten also gerne eine Funktion, die es uns ermöglicht, alle denkbaren Stichproben auf einen Wahrscheinlichkeitswert zwischen 0 und 1 abzubilden. Wenn abzählbar viele Ereignisse eintreten können (z.B. beim Würfeln oder Münzwurf), spricht man von Wahrscheinlichkeitsfunktionen. Bei stetigen Verteilungen (z.B. wenn Zeiträume betrachtet werden) verwendet man den Begriff Dichtefunktion. In beiden Fällen summieren sich alle Funktionswerte zu 1 auf. Eine kumulierte Wahrscheinlichkeits- bzw. Dichtefunktion nennt man Verteilungsfunktion. D.h. bei der Verteilungsfunktion werden die Funktionswerte der Wahrscheinlichkeits summiert bzw. die Dichtefunktion integriert.\nSchauen wir uns als Beispiel das Histogramm der Allbus-Einkommensverteilung aus den vorherigen Abschnitten an. Dazu laden wir zunächst die Daten und berechnen einige Parameter:\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(tidyverse, ggplot2, haven, dplyr, fitdistrplus)  \n\n\ntheme_set(theme_classic()) \n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\") \n\nallbus_df &lt;- subset(daten, select=c(\"sex\", \"pt12\", \"di01a\")) %&gt;%          \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ as.numeric(.))) %&gt;%                                     \n  mutate(across(c(\"sex\", \"pt12\", \"di01a\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%          \n  na.omit()   \n\ncolnames(allbus_df) &lt;- c(\"Geschlecht\", \"VertrauenBR\", \"Einkommen\") # Umbenennen der Spaltennamen\n# Funktion zur Berechnung des geometrischen Mittels:\ngeom.mean &lt;- function(vector){\n  exp(mean(log(vector)))\n}\nget.mode &lt;- function(vector){\n  frequencies = table(vector) \n  max.freq &lt;- max(frequencies)  \n  where.max &lt;-frequencies == max.freq \n  modus &lt;- names(frequencies[where.max]) \n  return(modus)\n}\n# Berechnung von arithmetischem und geometrischem Mittel, Median, Modus und \n# Standardabweichung:  \nmean.einkommen &lt;- mean(allbus_df$Einkommen)\ngeom.einkommen &lt;- geom.mean(allbus_df$Einkommen + 43) \nmedian.einkommen &lt;- median(allbus_df$Einkommen)\nmodus.einkommen &lt;- get.mode(allbus_df$Einkommen)\nsd(allbus_df$Einkommen)\n\n[1] 1610.501\nWir plotten zunächst unsere Lageparameter im Histogramm:\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus.einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = geom.einkommen, color = \"yellow\", linetype = \"dashed\", linewidth = 1)\nZunächst einmal sieht man, dass sich die Messwerte in einem bestimmten Bereich häufen und eine zumindest annähernde Glockenform aufweisen. Der höchste Punkt deckt sich in etwa mit dem Mittelwert, hier vor allem dem Median (blau) und dem arithmetischen Mittel (rot), während der Modus (grün) etwas abseits liegt. Sie verteilen sich nicht gleichmäßig, was bei Einkommensdaten auch nicht zu erwarten gewesen wäre. So ganz symmetrisch sieht die Verteilung aber auch nicht aus, sondern wirkt etwas nach links gequetscht. Man nennt das auch “rechtsschief” bzw. “linkssteil”. Ein Indiz hierfür ist auch, dass das arithmetische Mittel größer ist als der Median. Das nach rechts gequetschte Gegenstück hieße “linksschief” bzw. “rechtssteil”. Diese Asymmetrie ist auch dadurch bedingt, dass ein erheblicher Teil der Studienteilnehmenden im Datensatz nicht vorkommt. Leute, die keine Angabe gemacht oder angegeben haben, dass sie über kein eigenes Einkommen verfügen, wurden beim Laden herausgefiltert. Die annähernde Glockenform stimmt jedoch hoffnungsvoll, dass der Datensatz einer Verteilung folgt, die in der Statistik von besonderer Bedeutung ist und die wir im Folgenden genauer betrachten werden: Die Normalverteilung."
  },
  {
    "objectID": "Skript_4.4.html#berechnung",
    "href": "Skript_4.4.html#berechnung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung",
    "href": "Skript_4.4.html#visualisierung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "0.2 Visualisierung",
    "text": "0.2 Visualisierung\nDie folgenden fünf Codebeispiele ergeben Visualisierungen der Normalverteilung bei unterschiedlichen Stichprobengrößen von \\(n = 10\\) bis \\(n = 100000\\). Dabei wird jeweils eine Zufallsstichprobe gezogen und die Verteilung der “Messwerte” als Histogramm ausgegeben. Je größer die Stichprobe, desto stärker sollte sich dabei die Verteilung der Zufallswerte der Dichtefunktion der Normalverteilung annähern, die hier rot dargestellt ist. Mittelwert und Standardabweichung sind so gewählt, dass die Standardnormalverteilung herauskommt. Wiederholen Sie die einzelnen Beispiele mehrmals, dann sollte jedesmal eine andere Stichproben-Verteilung herauskommen:\n\n#Zufalls-Stichprobe mit 10 Messwerten:\n\nrandom.data &lt;- rnorm(10, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 10 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\nrandom.data &lt;- rnorm(100, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 100 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\n\n\n\n\nrandom.data &lt;- rnorm(1000, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 1000 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\n\n\n\n\nrandom.data &lt;- rnorm(10000, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 10000 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")\n\n\n\n\n\nrandom.data &lt;- rnorm(100000, mean = 0, sd = 1) # Ziehen der Zufallsstichprobe\n# Histogramm der Verteilung:\nggplot(data = data.frame(x = random.data), aes(x)) +\n  geom_histogram(aes(y=..density..), binwidth = 0.1, fill = \"gray\", color = \"black\", alpha = 0.7) +\n  labs(\n    title = \"Zufallsstichprobe mit 100000 Messwerten\", x = \"Messwerte\", y = \"Häufigkeit\") +\nstat_function(fun=dnorm, args=list(mean=0, sd=1), colour=\"red\")"
  },
  {
    "objectID": "Skript_4.4.html#interpretation",
    "href": "Skript_4.4.html#interpretation",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "0.3 Interpretation",
    "text": "0.3 Interpretation\nAus der Visualisierung der unterschiedlichen Stichprobengrößen sollte ersichtlich geworden sein, warum ausreichend große Stichproben in der Statistik so wichtig sind. Erst ab einer ausreichend großen Anzahl Messwerten kann überhaupt festgestellt werden, ob die Daten der Normalverteilung (oder irgendeiner anderen in diesem Kapitel beschriebenen Verteilung) folgen.\nWenn man weiß, dass ein bestimmtes Merkmal normalverteilt ist, kann man das nutzen, um mit relativ wenig Aufwand Berechnungen anzustellen. Angenommen, für unsere Einkommensdaten träfe das auch zu, dann könnte man z.B. mithilfe der Normalverteilung ausrechnen, wieviel Prozent der Population theoretisch über maximal 1500 Euro Netto-Einkommen im Monat verfügen. Dazu verwenden wir die Verteilungsfunktion der Normalverteilung, da wir an einem kumulierten Wert interessiert sind, nämlich allen möglichen Einkommenswerten bis maximal 1500 Euro. Wir übergeben der Funktion Mittelwert und Standardabweichung unserer Daten sowie die besagte Einkommensobergrenze:\n\nincome_mean &lt;- mean(allbus_df$Einkommen) \nincome_sd &lt;- sd(allbus_df$Einkommen)\nincome_median &lt;- median(allbus_df$Einkommen)\n# Die Verteilungsfunktion der Normalverteilung: \"p\" + \"norm\": \npnorm(1500, mean=income_mean, sd=income_sd) \n\n[1] 0.2786057\n\n\nWir bekommen als Ergebnis ca. 0.278. Das vergleichen wir mit dem 27,8%-Quantil unserer Messwerte:\n\nquantile(allbus_df$Einkommen, probs = c(0.278))\n\n27.8% \n 1500 \n\n\nZur Erinnerung: Das 27,8%-Quantil teilt die untersten 27,8% vom Rest der Messwerte. Interessanterweise liegt die Grenze genau bei 1500 Euro, was exakt der Vorhersage entspricht. Die Übereinstimmung wird aber deutlich schwächer, wenn wir das mit dem (100-27,8)%-Quantil vergleichen:\n\nq &lt;- quantile(allbus_df$Einkommen, probs = c(1 - 0.278))\nq\n\n   72.2% \n2977.378 \n\n\n\npnorm(2977.378, mean=income_mean, sd=income_sd)\n\n[1] 0.6294329\n\n\nUnsere Messwerte ergeben für das 72,2%-Quantil einen Wert von ca. 2977 Euro. Kalkulieren wir für das selbe Quantil bei der Normalverteilung das zu erwartende Einkommen, werden uns dagegen ca. 3393 Euro angegeben:\n\nqnorm(0.722, mean=income_mean, sd=income_sd)\n\n[1] 3393.598\n\n\nDie Datenlage weicht folglich um ca. 416 Euro von der theoretischen Annahme ab und fällt deutlich geringer aus. Ein Umstand, der sich auch grafisch widerspiegelt, wenn wir die Normalverteilung in unser Histogramm einzeichnen: Die blaue Kurve basiert auf dem Median als Mittelwert, die rote auf dem arithmetischen Mittel, die gelbe auf dem geometrischen Mittel.\n\nggplot(allbus_df, aes(x = Einkommen + 42)) + \n    geom_histogram(aes(y =..density..),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = income_mean, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = income_median, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\nstat_function(fun=dnorm, args=list(mean=income_mean, sd=income_sd), colour=\"red\") +   # Einfügen der Normalcerteilung mit arithm. mittel\nstat_function(fun=dnorm, args=list(mean=income_median, sd=income_sd), colour=\"blue\") +  # Einfügen der NV mit Median\n  stat_function(fun=dnorm, args=list(mean=geom.mean(allbus_df$Einkommen + 42), sd=income_sd), colour=\"yellow\") +\nlabs(x = \"Einkommen\", y = \"Häufigkeit\")\n\n\n\n\nMan sieht darin zweierlei: Zum einen gibt es zwar eine gewisse Übereinstimmung, aber doch auch deutliche Unterschiede zwischen den Messwerten und der Normalverteilung und zum zweiten wirken Median und geometrisches Mittel etwas genauer als das arithmetische Mittel. Hier zeigt sich die stärkere Robustheit des Medians gegenüber Ausreißern.\nEine andere Möglichkeit, die Daten mit visuellen Methoden auf Normalverteilung zu prüfen, ist ein sogenannter Q-Q-Plot. Dabei werden die Quantile der theoretischen Verteilung auf einer Achse aufgetragen und die dazu korrespondierenden Quantile der empirischen Daten auf der anderen. Sind die Daten normalverteilt, sollten die Punkte im Plot alle sehr dicht an einer gemeinsamen Linie liegen. Glücklicherweise gibt es auch für diesen Plot eine Funktion:\n\n#shapiro.test(allbus_messniveau_bsp$di01a)\nqqnorm(allbus_df$Einkommen) # Erstellen des Q-Q-Plots\nqqline(allbus_df$Einkommen) # Einfügen der Linie, auf der die Punkte liegen sollten\n\n\n\n\nWie leicht zu sehen ist, weichen die Daten sehr stark von der Linie ab. Der Bereich unterhalb von ca. 3000 Euro Einkommen scheint zumindest teilweise als normalverteilt interpretierbar zu sein, während für höhere Werte die Ergebnisse massiv abweichen. Allerdings muss auch dazugesagt werden, dass ab 5000 Euro aufwärts die Anzahl an Messwerten deutlich abnimmt.\nEs gibt Studien, die nahelegen, dass beim Einkommen durch Logarithmierung eine bessere Anpassung an die Normalverteilung erreicht werden kann. Da dies ohnehin eine vielversprechende Methode ist, um die Verteilung empirischer Daten einer Normalverteilung ähnlicher zu machen, probieren wir das einmal aus und werfen dann einen Blick auf den neuen Q-Q-Plot:\n\nlog.income = log(allbus_df$Einkommen + 42) # 42 wird hier addiert, um negative Messwerte zu vermeiden.\n\nqqnorm(log.income)\nqqline(log.income)\n\n\n\n\nAuch hier sieht man noch immer ein paar deutliche Ausreißer, aber verglichen mit vorher liegen die einzelnen Werte nun sehr viel dichter an der Linie.\nWir belassen es an dieser Stelle dabei, angesichts unserer visuellen Mittel davon auszugehen, dass sich die Einkommensdaten zumindest innerhalb eines bestimmten Bereichs als lognormalverteilt ansehen lassen und damit nicht normalverteilt sind."
  },
  {
    "objectID": "Skript_4.4.html#berechnung-1",
    "href": "Skript_4.4.html#berechnung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-1",
    "href": "Skript_4.4.html#visualisierung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-1",
    "href": "Skript_4.4.html#interpretation-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_5.1.html",
    "href": "Skript_5.1.html",
    "title": "Die Messung von latenten Variablen",
    "section": "",
    "text": "Eine überfüllte & scheinbar unstrukturierte Stadt an der Oberfläche, Bild generiert von Midjourney\n\n\n\n1 Einführung in die Messung von latenten Variablen\nIn der Kommunikationswissenschaft, beziehungsweise in den Sozialwissenschaften allgemein, bezieht sich der Begriff latente Variable auf eine nicht direkt beobachtbare Eigenschaft oder einen nicht direkt messbaren Faktor, der sich jedoch durch mehrere beobachtbare Variablen (teilweise auch als Indikatoren bezeichnet) manifestiert. Latente Variablen sind theoretische Konstrukte, die nicht direkt gemessen werden können. Als Wissenschaftler:innen gehen wir allerdings davon aus, dass diese Konstrukte existieren und wir somit komplexe gesellschaftliche Phänomene mit der Hilfe von latenten Variablen besser verstehen können.\nWenn wir uns jetzt ganz konkret mit dem Vertrauen in gesellschaftliche Institutionen beschäftigen, ist Vertrauen eine latente Variable. Wir können Vertrauen in gesellschaftliche Institutionen nicht direkt beobachten, wir sehen beispielsweise einem Menschen nicht an, ob er oder sie dem Bundesverfassungsgericht als eine von mehreren gesellschaftlichen Institutionen stark oder nicht so stark vertraut. Wenn wir uns als Forscher:innen fragen, inwieweit die Menschen in Deutschland den gesellschaftlichen Institutionen vertrauen, müssen wir zunächst theoretisch klären was wir unter dem Vertrauen in gesellschaftliche Institutionen verstehen.\nDas Vertrauen in gesellschaftliche Institutionen ist ein abstraktes und komplexes theoretisches Konstrukt, das sich aus verschiedenen Bestandteilen zusammensetzt. Diese einzelnen Bestandteile (Vertrauen in das Bundesverfassungsgericht, Vertrauen in den Bundestag, etc.) können wir durch sogenannte Indikatoren beobachten und damit messbar machen (in einer Befragung wäre das bspw. die Frage nach dem Vertrauen in das Bundesverfassungsgericht). Auf der Grundlage dieser Indikatoren versuchen wir dann auf das Vertrauen in gesellschaftliche Institutionen zu schließen.\nLatente Variablen spielen eine wichtige Rolle in der statistischen Modellierung und Analyse, insbesondere in der Faktorenanalyse. Durch das Einbeziehen von latenten Variablen in die Analyse, können komplexe Beziehungen und Zusammenhänge zwischen verschiedenen Variablen besser verstanden und erklärt werden. Zusätzlich ist eine Reduktion von Komplexität möglich, indem mehrere beobachtbare Variablen (bzw. Indikatoren) in einer latenten Variable zusammengefasst werden, was zu einem besseren Verständnis der zugrunde liegenden Phänomene führen kann. Der Prozess des Zusammenfassens wird häufig auch als Indexbildung bezeichnet.\nDie Indexbildung umfasst drei relevante Schritte: (1) Zunächst muss ein theoretisches Konstrukt entwickelt werden. In unserem Fall würde das bedeuten, dass wir ein theoretisches Verständnis von Vertrauen in gesellschaftliche Institutionen entwickeln und uns klar werden was wir darunter verstehen. (2) Im nächsten Schritt müssten wir klären, ob sich dieses theoretisches Konstrukt in Form einer latenten Variable auch in unseren Daten finden lässt, hierfür werden wir eine explorative Faktorenanalyse durchführen. (3) Als letzten Schritt führen wir die eigentliche Indexbidlung durch. Hier berechnen wir eine neue Variable, den Index für Vertrauen in gesellschaftliche Institutionen und Überprüfen dessen Güte."
  },
  {
    "objectID": "Skript_5.2.html",
    "href": "Skript_5.2.html",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Das Entdecken der zugrunde liegenden Struktur der Stadt, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_5.3.html",
    "href": "Skript_5.3.html",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Validieren, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_6.1.html",
    "href": "Skript_6.1.html",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "Statistische Tests erlauben es uns, auf der Basis von quantitativen Daten eine begründete Entscheidung über die Gültigkeit oder Ungültigkeit einer Hypothese zu treffen.\nBei den hier verwendeten Beispielen geht es darum, Unterschiede zwischen zwei oder mehr Stichproben (oder Teilstichproben) statistisch zu überprüfen.\nEin statistischer Test liefert Aufschluss darüber, wie wahrscheinlich es ist, dass wir die Nullhypothese verwerfen und/oder die alternative Hypothese annehmen können.\nKönnen wir aufgrund des Testausgangs davon ausgehen, dass ein (nicht zufälliger) Unterschied zwischen zwei Stichproben besteht, so sprechen wir von einem signifikanten (überzufälligen) Unterschied."
  },
  {
    "objectID": "Skript_6.2.html",
    "href": "Skript_6.2.html",
    "title": "Bestimmen von Unterschieden in der Varianz mit Kreuztabellen und dem Chi-Quadrat Test",
    "section": "",
    "text": "Bestimmen von Unterschieden in der Varianz\n\n1 Kreuztabellen und Chi-Quadrat Test"
  },
  {
    "objectID": "Skript_6.3.html",
    "href": "Skript_6.3.html",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "Grundsätzlich unterscheiden wir zwischen drei verschiedenen Arten von t-tests:\n\nEinstichprobentest (one-sample-test),\nt-Test für unabhängige Stichproben (two-paired-test) sowie\nt-Test für verbundene Stichprobene (paired test)\n\nDer Einstichprobentest\nDer Einstichprobentest oder auch one-sample-test vergleicht den Mittelwert einer Gruppe mit einem bereits bekannten feststehenden Wert. Wir können so beispielsweise überprüfen, ob das Einkommen in einer Stichprobe dem deutschlandweiten Mittelwert entspricht, sofern dieser uns vorab bekannt ist.\nt-test für unabhängige Stichproben\nMit dem t-test für unabhängige Stichproben überprüfen wir, ob sich die Mittelwerte in zwei gemessenen Stichproben unterscheiden. So könnten wir beispielsweise testen, ob sich das Einkommen von Männern und Frauen unterscheidet.\nt-test für verbundene Stichproben\n\n\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n1p_load(tidyverse, ggplot2, haven, mosaic, knitr,effectsize, car, broom)\n2theme_set(theme_classic())\n\n\n1\n\nMittels p_load laden wir alle benötigten Pakete gleichzeitig.\n\n2\n\nWir legen allgemein den Hintergrund theme_classic fest.\n\n\n\n\nAnschließend laden wir die Daten aus dem Allbus:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n\n\nUm mit dem Datensatz zu arbeiten benötigen wir einige grundlegende Schritte des Datenmanagements für ausführliche Erklärungen siehe hier. Für unseren t-test möchten wir uns anschauen, wie sich das Geschlecht der Befragten auf ihr Vertrauen in XX auswirkt.\n\ndat &lt;- daten %&gt;% \n  filter(., sex == 1 | sex == 2) %&gt;%  #&gt;1\n  rename(., trustpol = pt15) #&gt;2\n\n\nWir filtern die Fälle, sodass alle Befragten entweder 1=männlich oder 2=weiblich sind.\nDie Variable Vertrauen in die Polizei(pt15) benennen wir in trustpol\n\n\n\n\n\n# Deskription der Abhängigen Variablen\n#des_stat &lt;- (favstats(AV ~ UV, data = daten))\n#kable (des_stat,\n#      col.names = c(label_UV,\"Minimum\", \"1.Quartil\", \n#                    \"Median\", \"3.Quartil\", \n#                    \"Maximum\", \"M\", \"SD\", \"N\",\n#                    \"Fehlend\"),\n#      digits = 2)"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben",
    "text": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben\n\n1.1.1 t-Test für unabhängige Stichproben\n\n\n1.1.2 Mann-Whitney\n\n\n1.1.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n1.1.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n\nCode\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\n\nCode\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n\n1.1.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n1.1.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n\nCode\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n\nCode\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n\nCode\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n1.1.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\nCode\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n1.1.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\nCode\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\n\nCode\nprint(fit)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n1.1.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt.\n\n\n\n1.1.4 Kruskal Wallis\n\n\n1.1.5 mehrfaktorielle Varianzanalyse\n\n1.1.5.1 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\n\nCode\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\n\nCode\nprint(fit2)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n\n1.1.5.2 Post-Hoc Tests\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n1.1.5.3 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n\nCode\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\n\n\nCode\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben",
    "text": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben\n\n1.2.1 t-Test für verbundene Stichproben\n\n\n1.2.2 Wilcoxon"
  },
  {
    "objectID": "Skript_6.4.html",
    "href": "Skript_6.4.html",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Die Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'rootSolve', 'lmom', 'expm', 'Exact', 'gld'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'rootSolve' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'lmom' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'expm' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Exact' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gld' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'DescTools' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nDescTools installed\n\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'multcompView', 'gmp', 'Rmpfr', 'kSamples', 'BWStest'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'multcompView' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gmp' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Rmpfr' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'kSamples' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'BWStest' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'PMCMRplus' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nPMCMRplus installed\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "Skript_7.1.html",
    "href": "Skript_7.1.html",
    "title": "Einführung",
    "section": "",
    "text": "1 Einführung in das Überprüfen von Zusammenhängen\nWillkommen zu unserem Kapitel über die statistische Überprüfung von Zusammenhängen mit Hilfe von R. In der datengetriebenen Welt von heute ist es von großer Bedeutung, statistische Zusammenhänge zu verstehen und zu validieren. In diesem Kapitel werden wir Ihnen zeigen, wie Sie mithilfe von R verschiedene statistische Tests durchführen können, um Zusammenhänge zwischen Variablen zu untersuchen.\nDer erste Schritt besteht darin, die Daten in R zu importieren und zu explorieren. Wir werden Ihnen zeigen, wie Sie die Daten visualisieren und grundlegende statistische Kennzahlen berechnen können, um einen ersten Eindruck von den vorliegenden Zusammenhängen zu bekommen. Anschließend werden wir auf verschiedene statistische Tests eingehen, darunter den Korrelationstest, den Chi-Quadrat-Test und verschiedene Formen der Regression. Sie lernen, wie Sie diese Tests in R implementieren, die Ergebnisse interpretieren und fundierte Schlussfolgerungen ziehen können.\nDarüber hinaus werden wir auf wichtige Konzepte wie Signifikanzniveau, p-Wert und Konfidenzintervalle eingehen, um Ihnen ein solides Verständnis dafür zu vermitteln, wie statistische Zusammenhänge bewertet werden können. Durch die Anwendung dieser Methoden werden Sie in der Lage sein, Ihre Daten genau zu analysieren, potenzielle Zusammenhänge zu identifizieren und deren Bedeutung zu bewerten. Tauchen Sie ein in die spannende Welt der statistischen Überprüfung von Zusammenhängen mit R und erweitern Sie Ihr analytisches Toolkit!\n(Es handelt sich bei dem Text um einen Platzhalter, erstellt von chatGDP)"
  },
  {
    "objectID": "Skript_7.2.html",
    "href": "Skript_7.2.html",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei zwei Variablen"
  },
  {
    "objectID": "Skript_7.2.html#korrelation",
    "href": "Skript_7.2.html#korrelation",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.1 Korrelation",
    "text": "3.1 Korrelation"
  },
  {
    "objectID": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "href": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse",
    "text": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse\nIn diesem Notebook wird die einfache lineare Regression auf Grundlage der ESS-Daten vorgestellt. In der nächsten Sitzung gehen wir näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen die multiple lineare Regression kennen.\n\n\n\nPicture generated by Midjourney\n\n\n\n3.2.1 Anwendungsbereich der linearen Regression\nDie einfache lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen zwei metrischen Variablen besteht. Sie wird daher auch als bivariate Regression bezeichnet.\nZiel ist es, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?\n\n\n3.2.2 Vorbereitung und Laden der Daten\nZunächst laden wir die Pakete des tidyverse. Weiterhin laden wir das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt.\nDen Datensatz finet ihr hier.\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\") \n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n3.2.3 Ziel der Analyse\nMit Hilfe der Regression wollen wir die Annahme prüfen, dass das Alter (agea) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) hat. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n3.2.4 Data Management\nDamit der Output etwas nachvollziehbarer wird, benenne ich die Variablen mit dem rename-Befehl zunächst einmal um. Dann nutze ich den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifiziere ich, auf welche Variablen sich der Befehl beziehen soll). Das alles weise ich einem neuen Datenobjekt zu: daten_mod\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm) %&gt;% \n  drop_na(c(alter, internetnutzung)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gndr   alter marsts   edubde1 eduade2 eduade3 nwspol netusoft\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1       835 FR    Male      16 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 2  10006306 DE    Female    39 None of… Abitur… Diplom… Kein b…     25 Every d…\n 3  10008723 DE    Male      16 None of… (Noch)… Kein H… Kein b…     30 Most da…\n 4       231 SE    Female    80 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       270 Most da…\n 5 100003876 GB    Male      33 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 6       408 FR    Female    30 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 7  10009322 DE    Male      72 &lt;NA&gt;     Abitur… Diplom… Kein b…     90 Every d…\n 8 100003453 GB    Male      45 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 9      1126 SE    Female    60 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n10 100001390 GB    Female    77 Widowed… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        10 Every d…\n# ℹ 90 more rows\n# ℹ 156 more variables: internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;,\n#   pplhlp &lt;dbl&gt;, polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;,\n#   psppipla &lt;fct&gt;, cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;,\n#   trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;, trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;,\n#   vote &lt;fct&gt;, prtvede1 &lt;fct&gt;, prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;,\n#   wrkorg &lt;fct&gt;, badge &lt;fct&gt;, sgnptit &lt;fct&gt;, pbldmn &lt;fct&gt;, bctprd &lt;fct&gt;, …\n\n\n\n\n3.2.5 Prüfung der Voraussetzungen 1: Grafische Darstellung des Zusammenhangs der beiden Variablen, um die Annahme von Linearität zu prüfen\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und Internetnutzung. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Internetznutzung) und x (=Alter) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und Internetnutzung\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")\n\n\n\n\n\n\n3.2.6 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen schwachen negativen (aber linearen!) Zusammenhang zwischen Alter und Internetnutzung nahe: mit zunehmendem Alter sinkt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\nNicht wundern: Weil wir oben ein Zufallssample gezogen haben, sieht die Grafik bei Ihnen allen etwas anders aus. Sie kann dadurch auch so ausfallen, dass der lineare Zusammenhang nicht (gut) sichtbar ist – vor allem dann, wenn Ausreißer das Ergebnis massiv verzerren (z.B. wenn ein oder zwei ältere Nutzer mit [unrealistisch?] hoher Nutzungsdauer in ihrer Zufallstichprobe gelandet sind).\n\n\n3.2.7 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regressionsanalyse prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: Internetznutzung), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (Internetnutzung) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n3.2.8 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nmodel\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nCoefficients:\n(Intercept)        alter  \n    378.194       -3.982  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  378.194     59.747   6.330 0.0000000074 ***\nalter         -3.982      1.198  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.9 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängige Variable “alter” zu erklären.\n\n3.2.9.1 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n3.2.9.2 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n3.2.9.3 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n3.2.9.4 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n3.2.9.5 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n3.2.9.6 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (14,7) Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n3.2.9.7 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n3.2.9.8 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\n3.2.10 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (z.B. -4.642) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Die UV beeinflusst die AV, R2 = .24, F(1, 116) = 4.71, p = .003.\n\n\n\n\n\n3.2.11 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (Internetnutzung) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\npredict.lm(model, data.frame(alter = 25))\n\n       1 \n278.6328 \n\npredict.lm(model, data.frame(alter = 75))\n\n       1 \n79.51086 \n\n\n\n\n3.2.12 Inhaltliche Interpretation\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von (306) Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von (74) Minuten auf. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(alter = c(25, 75)))\n\n        1         2 \n278.63275  79.51086 \n\n\n\n\n3.2.13 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable Internetkonsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen Internetkonsum von (X) Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\nfitted(model) \n\n        1         2         3         4         5         6         7         8 \n314.47470 222.87862 314.47470  59.59867 246.77325 258.72057  91.45818 198.98400 \n        9        10        11        12        13        14        15        16 \n139.24743  71.54599 163.14206 195.00156 254.73813 242.79081 306.50982 175.08937 \n       17        18        19        20        21        22        23        24 \n131.28255 131.28255 155.17718 218.89619 111.37036 202.96644 119.33524  99.42305 \n       25        26        27        28        29        30        31        32 \n242.79081 274.65032 103.40549 103.40549 155.17718  99.42305 210.93131 167.12449 \n       33        34        35        36        37        38        39        40 \n310.49226 179.07181 107.38793 131.28255 222.87862  67.56355 198.98400 302.52738 \n       41        42        43        44        45        46        47        48 \n270.66788 278.63275 187.03668  99.42305 234.82594 226.86106 246.77325 262.70300 \n       49        50        51        52        53        54        55        56 \n115.35280  71.54599 163.14206 274.65032 155.17718 163.14206 187.03668 163.14206 \n       57        58        59        60        61        62        63        64 \n234.82594 226.86106  99.42305 250.75569 115.35280 175.08937 183.05425 202.96644 \n       65        66        67        68        69        70        71        72 \n159.15962 123.31768 123.31768 143.22987 155.17718 254.73813 242.79081 278.63275 \n       73        74        75        76        77        78        79        80 \n210.93131 151.19474 270.66788 234.82594 191.01912 119.33524 310.49226 226.86106 \n       81        82        83        84        85        86        87        88 \n159.15962 206.94887 270.66788 250.75569 147.21231 238.80838 230.84350 103.40549 \n       89        90        91        92        93        94        95        96 \n147.21231 159.15962 167.12449 151.19474 302.52738 210.93131 187.03668 119.33524 \n       97        98        99       100 \n175.08937 266.68544 302.52738 198.98400 \n\n\nNun haben wir aber im Rahmen unserer Befragung die Internetnutzung der Befragten aber ja schon erhoben. Wozu dient das dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nresiduals.lm(model)\n\n          1           2           3           4           5           6 \n-194.474695 -162.878625 -269.474695  -29.598673 -126.773252 -228.720565 \n          7           8           9          10          11          12 \n -31.458176  -78.983998 -109.247430  -11.545986  -43.142057  404.998440 \n         13          14          15          16          17          18 \n-194.738127  237.209186  -96.509820  304.910630  -41.282554  -71.282554 \n         19          20          21          22          23          24 \n -35.177181  -38.896187  -51.370365 -142.966435   90.664759  -49.423051 \n         25          26          27          28          29          30 \n-240.790814   25.349683  -13.405489   16.594511 -125.177181  -84.423051 \n         31          32          33          34          35          36 \n -30.931311  132.875505 -130.492257  -89.071808   72.612073  -11.282554 \n         37          38          39          40          41          42 \n -42.878625  112.436451  401.016002   -2.527382  629.332121  921.367245 \n         43          44          45          46          47          48 \n 112.963316  -84.423051 -114.825938 -136.861063  353.226748   37.296997 \n         49          50          51          52          53          54 \n 184.647197  108.454014   16.857943  385.349683 -125.177181 -133.142057 \n         55          56          57          58          59          60 \n-172.036684  -73.142057  365.174062 -166.861063  -39.423051 -205.755690 \n         61          62          63          64          65          66 \n 184.647197 -115.089370  -63.054246  -62.966435  -99.159619  176.682322 \n         67          68          69          70          71          72 \n-108.317678 -118.229868  -35.177181  -74.738127  -32.790814   81.367245 \n         73          74          75          76          77          78 \n-180.931311  -31.194743  449.332121 -114.825938  348.980878  -29.335241 \n         79          80          81          82          83          84 \n-280.492257 -166.861063  -99.159619  -86.948873  -30.667879  -10.755690 \n         85          86          87          88          89          90 \n -27.212306 -118.808376 -200.843500  -73.405489   92.787694 -114.159619 \n         91          92          93          94          95          96 \n  12.875505  328.805257 -212.527382   29.068689 -127.036684  120.664759 \n         97          98          99         100 \n-165.089370  273.314559  -92.527382 -108.983998 \n\n\n\n\n3.2.14 Inhaltliche Interpretation\nFür unseren Fall Nummer 3 beträgt die Abweichung der Prognose von der Beobachtung (121) Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 15 Prozent nicht besonders groß ist.\n\n\n3.2.15 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten_mod$vorhersage &lt;- predict(model) \ndaten_mod$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n  geom_segment(aes(xend = alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Internetnutzung\") + # Titel\n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\") # Achsen-Beschriftung\n\n\n\n\n\n\n3.2.16 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Standardized Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 378.1937           NA    59.7466   6.330 0.0000000074 ***\nalter        -3.9824      -0.3183     1.1979  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.17 Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   378.         NA         59.7       6.33 0.00000000740\n2 alter          -3.98       -0.318      1.20     -3.32 0.00125      \n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.101        0.0922  198.      11.1 0.00125     1  -670. 1346. 1353.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model)\n\n# A tibble: 100 × 8\n   internetnutzung alter .fitted .resid   .hat .sigma   .cooksd .std.resid\n             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1             120    16   314.  -194.  0.0452   198. 0.0239       -1.00  \n 2              60    39   223.  -163.  0.0124   199. 0.00428      -0.827 \n 3              45    16   314.  -269.  0.0452   197. 0.0458       -1.39  \n 4              30    80    59.6  -29.6 0.0496   199. 0.000613     -0.153 \n 5             120    33   247.  -127.  0.0172   199. 0.00364      -0.645 \n 6              30    30   259.  -229.  0.0206   198. 0.0143       -1.17  \n 7              60    72    91.5  -31.5 0.0327   199. 0.000441     -0.161 \n 8             120    45   199.   -79.0 0.0102   199. 0.000823     -0.400 \n 9              30    60   139.  -109.  0.0161   199. 0.00253      -0.556 \n10              60    77    71.5  -11.5 0.0428   199. 0.0000792    -0.0595\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "href": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden",
    "text": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.4 Prüfung der Voraussetzungen",
    "text": "3.4 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "href": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.5 Berechnung und Interpretation einer einfachen linearen Regression",
    "text": "3.5 Berechnung und Interpretation einer einfachen linearen Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird von dem Zentrum für Medien, Kommunikations- und Informationsforschung angeboten. Wir freuen uns, Ihnen die Neugestaltung dieses Kurses durch die Förderung des SKILL-Projekts der Universität Bremen präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie die Programmiersprache R,für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Diese Webseite bündelt hierbei das für den Kurs bentötigte Wissen und Materialien. Wir vermitteln Ihnen anhand von Erklärtexten, Beispielen und Erklärvideos die notwendigen Kenntnisse, damit Sie im Rahmen dieses Kurses erfolgreich Ihr eigenes Forschungsprojekt durchführen können. Dabei lassen wir Sie nicht alleine, sondern begleiten Sie durch den Prozess.\nIn den kommenden Kapiteln werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln, welche Sie im Anschluss direkt im Rahmen Ihres eigenen Forschungsprojekts anwenden können und somit ihr theoretisches Verständnis festigen und um praktische Erfahrungen erweitern können.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Methoden eingehen, darunter Hypothesentests, Faktorenanalyse, lineare Regression, und vieles mehr. Wir freuen uns darauf, Sie in unserem Kurs begrüßen zu dürfen!\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "href": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.1 t-Test für unabhängige Stichproben",
    "text": "2.1 t-Test für unabhängige Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#mann-whitney",
    "href": "Skript_6.3.html#mann-whitney",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.2 Mann-Whitney",
    "text": "2.2 Mann-Whitney"
  },
  {
    "objectID": "Skript_6.3.html#die-varianzanalyse",
    "href": "Skript_6.3.html#die-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.3 Die Varianzanalyse",
    "text": "2.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n2.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n2.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n2.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n2.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n2.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n2.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.3.html#kruskal-wallis",
    "href": "Skript_6.3.html#kruskal-wallis",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.4 Kruskal Wallis",
    "text": "2.4 Kruskal Wallis"
  },
  {
    "objectID": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.5 Mehrfaktorielle Varianzanalyse",
    "text": "2.5 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n2.5.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n2.5.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "href": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.1 t-Test für verbundene Stichproben",
    "text": "3.1 t-Test für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#wilcoxon",
    "href": "Skript_6.3.html#wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.2 Wilcoxon",
    "text": "3.2 Wilcoxon"
  },
  {
    "objectID": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "href": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.2 Multiple Regression mit Dummy-Codierung",
    "text": "1.2 Multiple Regression mit Dummy-Codierung\n\n1.2.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Geschlecht und Internetnutzung\nNun wollen wir noch Geschlecht (gndr) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei Gender haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist! Es handelt sich vielmehr um eine kategoriale Variable. Wie Sie schon gelernt haben, können Sie diese mit einem “Trick” ebenfalls in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Wir wollen uns hier mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl erst einmal umcodieren.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n1.2.2 Dummy Codierung der Variable Gender\n\ndaten_mod2 &lt;- daten_mod %&gt;%\nmutate(gender_r  = recode(gender, 'Male'='0', 'Female'='1')) %&gt;% # Recodierung der Var Gender zur Dummy-Variable\n  mutate(gender_r = as.numeric(as.character(gender_r))) # Variable als numerischen Wert behandeln\ndaten_mod2\n\n# A tibble: 100 × 167\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 159 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\ntable(daten_mod2$gender_r)\n\n\n 0  1 \n56 44 \n\nsummary(daten_mod2$gender_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    0.44    1.00    1.00 \n\nclass(daten_mod2$gender_r)\n\n[1] \"numeric\"\n\n\n\n\n1.2.3 Regressionsmodell zum Zusammenhang von Alter, Nachrichtenrezeptiion, Geschlecht und Internetnutzung spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable gender_r ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel_m2 &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption + gender_r, data = daten_mod2)\nsummary(lm.beta(model_m2))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption + \n    gender_r, data = daten_mod2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-202.12  -94.50  -51.31   32.24  471.47 \n\nCoefficients:\n                                 Estimate Standardized Std. Error t value\n(Intercept)                     265.27637           NA   45.47227   5.834\nalter                            -1.93149     -0.20696    0.92960  -2.078\npolitische_Nachrichtenrezeption   0.26651      0.20918    0.12545   2.124\ngender_r                        -18.36732     -0.06002   30.78140  -0.597\n                                   Pr(&gt;|t|)    \n(Intercept)                     0.000000073 ***\nalter                                0.0404 *  \npolitische_Nachrichtenrezeption      0.0362 *  \ngender_r                             0.5521    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147.5 on 96 degrees of freedom\nMultiple R-squared:  0.09507,   Adjusted R-squared:  0.06679 \nF-statistic: 3.362 on 3 and 96 DF,  p-value: 0.02189\n\n\n\n\n1.2.4 Inhaltliche Interpretation\nGender hat hier keinen signifikanten Einfluss auf den Internetkonsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy- Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (9,8) Minuten geringeren Internetkonsum als Männer (wobei dieser Befund statistisch ja (nicht) signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.5 Vorhersage des multivariaten Modells für die tägliche Internetnutzung durch Alter, Nachrichtenrezeption und Geschlecht\n\npredict.lm(model_m2, data.frame(alter = c(25, 75), gender_r = c(0,1), politische_Nachrichtenrezeption = c(5, 10)))\n\n       1        2 \n218.3217 104.7124 \n\n\n\n\n1.2.6 Inhaltliche Interpretation\nEine männliche Person, die 25 Jahre alt ist und 5 Minuten pro Tag politische Nachrichten rezipiert hat, einen prognostizierten Internetkonsum von 293 Minuten. Eine weibliche Person, die 75 Jahre alt ist und ebenfalls 5 Minuten pro Tag politische Nachrichten rezipiert, hat einen prognostizierten Internetkonsum von 41 Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.7 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie diese ausführen, haben Sie ja heute zu Anfang der Sitzung gelernt (siehe oben). Zusätzlich müssen Sie bei einer multiplen Regresssion noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model_m2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                            Term  VIF    VIF 95% CI Increased SE Tolerance\n                           alter 1.05 [1.00,  3.50]         1.03      0.95\n politische_Nachrichtenrezeption 1.03 [1.00, 26.71]         1.01      0.97\n                        gender_r 1.07 [1.00,  2.29]         1.04      0.93\n Tolerance 95% CI\n     [0.29, 1.00]\n     [0.04, 1.00]\n     [0.44, 1.00]\n\n\n\n\n1.2.8 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5; wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "href": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse",
    "text": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse\n\ndaten_mod3 &lt;- daten %&gt;% \n  select(agea, netustm, cntry) %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm,\n         land = cntry) %&gt;% \n  filter(land %in% c(\"DE\", \"FR\", \"IS\", \"PL\")) %&gt;% \n  drop_na() %&gt;% \n  group_by(land) %&gt;% \n  slice_sample(n = 100) %&gt;% \n  ungroup()\ndaten_mod3\n\n# A tibble: 200 × 3\n   alter internetnutzung land \n   &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;\n 1    59             240 DE   \n 2    35              60 DE   \n 3    38             240 DE   \n 4    20             240 DE   \n 5    51              60 DE   \n 6    64             150 DE   \n 7    60              30 DE   \n 8    28             540 DE   \n 9    21             300 DE   \n10    53              45 DE   \n# ℹ 190 more rows\n\n\n\nggplot(daten_mod3, aes(alter, internetnutzung, colour = land)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(method = lm, formula = \"y ~ x\", se = FALSE) + \n  scale_colour_brewer(palette = \"Set1\") + \n  ggtitle(\"Lineare Regression für Alter und Internetnutzung (vier Länder)\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")"
  },
  {
    "objectID": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "href": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.3 Literatur und Beispiele aus der Praxis",
    "text": "3.3 Literatur und Beispiele aus der Praxis\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link\n\n:::"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird von dem Zentrum für Medien, Kommunikations- und Informationsforschung angeboten. Wir freuen uns, Ihnen die Neugestaltung dieses Kurses durch die Förderung des SKILL-Projekts der Universität Bremen präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie die Programmiersprache R,für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Diese Webseite bündelt hierbei das für den Kurs bentötigte Wissen und Materialien. Wir vermitteln Ihnen anhand von Erklärtexten, Beispielen und Erklärvideos die notwendigen Kenntnisse, damit Sie im Rahmen dieses Kurses erfolgreich Ihr eigenes Forschungsprojekt durchführen können. Dabei lassen wir Sie nicht alleine, sondern begleiten Sie durch den Prozess.\nIn den kommenden Kapiteln werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln, welche Sie im Anschluss direkt im Rahmen Ihres eigenen Forschungsprojekts anwenden können und somit ihr theoretisches Verständnis festigen und um praktische Erfahrungen erweitern können.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Methoden eingehen, darunter Hypothesentests, Faktorenanalyse, lineare Regression, und vieles mehr. Wir freuen uns darauf, Sie in unserem Kurs begrüßen zu dürfen!\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Autoren.html",
    "href": "Autoren.html",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Vita\n\n\n\n\n\nKatharina Maubach, © B. Köhler\n\n\nKatharina Maubach ist seit Dezember 2022 als wissenschaftliche Mitarbeiterin am Zentrum für Medien-, Kommunikations- und Informationsforschung der Universität Bremen im Lab „Politische Kommunikation und Innovative Methoden” tätig. Sie forscht im Rahmen des DFG-geförderten Projektes „Remixing Political News Reception” unter der Leitung von Prof. Stephanie Geise zur Rezeption multimodaler Medieninhalte. Innerhalb des Projektes war sie seit Februar 2021 an der Universität Münster beschäftigt, wo Sie zudem von Januar 2019 bis Januar 2021 im Arbeitsbereich von Prof. Volker Gehrau forschte und lehrte. Ihr Studium der Kommunikationswissenschaft absolvierte sie ebenfalls an der Universität Münster; in ihrer Masterarbeit forschte sie zum Einfluss politischer Nachrichtensatire auf politisches Interesse und Informiertheit.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nRezeptions- und Wirkungsforschung\nKlimakommunikation\nStatistische Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Autoren.html#patrick-zerrer",
    "href": "Autoren.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "Patrick Zerrer",
    "text": "Patrick Zerrer\nVita\n\n\n\n\n\nPatrick Zerrer\n\n\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Er schloss den „B.A. Governance and Public Policy - Staatswissenschaften” an der Universität Passau sowie den „M.A. Öffentliche Kommunikation” an der Friedrich-Schiller-Universität ab. Seine Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation, der Nachrichten- und Mediennutzung sowie der politischen Partizipation mit einem Fokus auf (digitalen) Klimaprotest und -bewegungen. Für seine Forschung nutzer er unterschiedliche Methoden, u.a. digitales (mobiles) Tracking, digitale Spurdaten, Umfragen, (automatisierter) Inhaltsanalysen und Interviews.\nForschungsschwerpunkte\n\nPolitische Kommunikation\n(mobile) Mediennutzungsforschung\nKlimaprotestforschung\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Hintergrund.html",
    "href": "Hintergrund.html",
    "title": "Lernen durch Praxis - Der Weg zu fundierten statistischen Kenntnissen",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nDie Grundidee unseres Lehrkonzeptes ist es, einen Kurs anzubieten, der auf der Verschränkung von praktischem Lernen, grundlegenden Kenntnissen und eigenständigem Forschen basiert. Dabei stellt die erfolgreiche Vermittlung des notwendigen Hintergrundwissens sowie der benötigten Werkzeuge zu vermitteln, damit Sie eigenständig quantitative Forschungsdesigns mit R umsetzen können.\nDer Kurs beginnt mit der Vermittlung von Grundlagen in Bezug auf R und der Logik von quantitativer Forschung. Uns ist es wichtig, dass Sie die grundlegenden Konzepte verstehen, um diese später korrekt in der Praxis anzuwenden. Sie werden unter anderem lernen, wie Sie Daten in R importieren, visualisieren und analysieren können, um ein tieferes Verständnis für Ihre Forschungsfragen zu gewinnen.\nNachdem Sie diese Grundlagen in Form des Data Bootcamps erworben haben, beginnt die begleitete praktische Umsetzung in Form eines eigenen Forschungsprojekts, bei dem Sie Ihre neu erlernten statistischen Kenntnisse direkt und betreut anwenden können. Sie werden Schritt für Schritt lernen, wie Sie Hypothesen aufstellen, mit großen Datensätzen umgehen, wie Sie diese analysieren und die gewonnen Ergebnisse korrekt interpretieren. Durch die praktische Anwendung vertiefen Sie nicht nur Ihr Verständnis, sondern gewinnen auch wertvolle Erfahrungen im Bereich der quantitativen Forschung und der Programmiersprache R.\nUnser Kurs legt großen Wert darauf, dass Sie nicht nur theoretisches Wissen erlangen, sondern dieses Wissen in die Praxis umsetzen können. Wir sind der Überzeugung, dass das eigenständige Durchführen eines Forschungsprojekts Ihnen nicht nur ein tieferes Verständnis für statistische Methoden gibt, sondern auch Ihre analytischen und Problemlösungsfähigkeiten stärkt. Unsere Dozent:innen stehen Ihnen dabei zur Seite und bieten Ihnen bei der Umsetzung Ihres Forschungsprojekts individuelle Unterstützung.\nWir freuen uns darauf Sie bei dieser Lernreise zu begleiten und Sie bei dem Erwerben von statistischen Kenntnissen, analytischen Fähigkeiten und praktischer Erfahrungen zu unterstützen. Wir sind davon überzeugt, dass Sie durch unser praxisorientieres Lernkonzept einen guten und informativen Einstieg in die Welt der quantitativen Forschung haben werden."
  },
  {
    "objectID": "Hintergrund.html#patrick-zerrer",
    "href": "Hintergrund.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Patrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften\". Das Masterstudium „Öffentliche Kommunikation\" an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action\" mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik\" mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_5.2.html#laden-der-nötigen-pakete",
    "href": "Skript_5.2.html#laden-der-nötigen-pakete",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "library(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.2.html#laden-des-datensatzes",
    "href": "Skript_5.2.html#laden-des-datensatzes",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "daten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "href": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "title": "Die Faktorenanalyse",
    "section": "1.2 Deskriptive Statistik für den Teildatensatz",
    "text": "1.2 Deskriptive Statistik für den Teildatensatz\nWir werfen einen kurzen Blick in die deskriptive Statistik für unseren Teildatensatz, um ein besseres Verständnis für die Daten zu erhalten.\n\n1summary(allbus_vertrauen)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n Ver_Gesundheitswesen   Ver_BVerfG    Ver_Bundestag   Ver_Verwaltung \n Min.   :1.000        Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000        1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000  \n Median :5.000        Median :6.000   Median :4.000   Median :5.000  \n Mean   :4.939        Mean   :5.255   Mean   :4.058   Mean   :4.482  \n 3rd Qu.:6.000        3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000        Max.   :7.000   Max.   :7.000   Max.   :7.000  \n Ver_kath_Kirche Ver_evan_Kirche   Ver_Justiz        Ver_TV     \n Min.   :1.000   Min.   :1.00    Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.00    1st Qu.:4.000   1st Qu.:3.000  \n Median :2.000   Median :3.00    Median :5.000   Median :4.000  \n Mean   :2.331   Mean   :3.04    Mean   :4.581   Mean   :3.577  \n 3rd Qu.:3.000   3rd Qu.:4.00    3rd Qu.:6.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.00    Max.   :7.000   Max.   :7.000  \n  Ver_Zeitung       Ver_Uni      Ver_Regierung    Ver_Polizei     Ver_Parteien \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000   1st Qu.:2.00  \n Median :4.000   Median :5.000   Median :4.000   Median :5.000   Median :3.00  \n Mean   :4.012   Mean   :5.203   Mean   :4.054   Mean   :4.948   Mean   :3.19  \n 3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:4.00  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00  \n   Ver_Kom_EU      Ver_EU_Par   \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000  \n Median :4.000   Median :4.000  \n Mean   :3.515   Mean   :3.556  \n 3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000"
  },
  {
    "objectID": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "href": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Hauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben."
  },
  {
    "objectID": "Skript_5.2.html#quellen-für-das-script",
    "href": "Skript_5.2.html#quellen-für-das-script",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Stephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html#laden-der-nötigen-pakete",
    "href": "Skript_5.3.html#laden-der-nötigen-pakete",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Laden der nötigen Pakete",
    "text": "1.1 Laden der nötigen Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, psych, dplyr, htmlTable)\n\nUnd laden im Anschluss den notwendigen Datensatz."
  },
  {
    "objectID": "Skript_5.3.html#laden-des-datensatzes",
    "href": "Skript_5.3.html#laden-des-datensatzes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Laden des Datensatzes",
    "text": "1.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Teildatensatz mit den benötigten Index-Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Index-Variablen\nWir greifen natürlich auf die gleiche Datengrundlage zurück, welche wir auch für die Faktorenanalyse verwendet haben. Was in unserem Fall bedeutet, dass wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nWir bereiten die Daten entsprechend vor, indem wir die fehlenden Werte entfernen und die Variablen in numerische umwandeln.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nWir haben nun alle Daten geladen und die Variablen entsprechend vorbereitet. Wir können eigentlich mit der Indexbidlung beginnen, müssen uns allerdings davor noch entscheiden, welche Art von Index wir bilden möchten."
  },
  {
    "objectID": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "href": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "title": "Reliabilität von Skalen",
    "section": "1.3 Berechnen eines Ungewichteten Summenindex",
    "text": "1.3 Berechnen eines Ungewichteten Summenindex\nWir haben bereits in Kapitel 5.2 mittels der explorativen Faktorenanalyse statistisch getestet, ob wir einen Index aus den genannten Variablen bilden können. Dies ist der Fall. Wir berechnen nur die einfachste Form eines Index, den ungewichteten Summenindex. Das bedeutet, dass wir die Werte pro befragter Person für die genannten Variablen aufsummieren und KEINE Gewichtungen einbauen. Eine Gewichtung wäre bspw. wenn wir eine Variable doppelt zählen würden.\nWir erstellen eine neue Variable vertrauen_ges_inst und summieren die Werte aller Indikatoren pro Fall (befragte Person) auf, bevor wir diese durch die Anzahl der Indikatoren teilen. Auf diese Art und Weise erhalten wir die selben Werteausprägungen, wie in den Indikatoren was uns die Interpretation erleichtert.\n\nindex_vertrauen = allbus_vertrauen %&gt;% \n1  mutate(vertrauen_ges_inst = (Ver_Gesundheitswesen + Ver_BVerfG + Ver_Bundestag + Ver_Verwaltung + Ver_kath_Kirche + Ver_evan_Kirche + Ver_Justiz+ Ver_TV +  Ver_Zeitung + Ver_Uni + Ver_Regierung + Ver_Polizei + Ver_Parteien + Ver_Kom_EU + Ver_EU_Par) / 15)\n2htmlTable(head(index_vertrauen))\n\n\n1\n\nWir bilden mit Hilfe von mutate die neue Variable vertrauen_ges_inst, welche sich aus der Summe der Indikatoren geteilt durch die Anzahl der Indikatoren zusammensetzt.\n\n2\n\nDie htmlTable Funktion ermöglicht uns eine schönere Darstellung der Tabelle. Mit head wählen wir die ersten paar Fälle aus dem Datensatz index_vertrauen aus\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\nvertrauen_ges_inst\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n4.6\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n4.8\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n4.2\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n4.66666666666667\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n4\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n2.8\n\n\n\n\n\nWir können uns noch die deskriptive Statistik für den Index anschauen, diese ist wichtig um den berechneten Index korrekt zu interpretieren.\n\n1summary(index_vertrauen$vertrauen_ges_inst)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.400   4.133   4.049   4.733   7.000"
  },
  {
    "objectID": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "href": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "title": "Reliabilität von Skalen",
    "section": "1.4 Reliabilität des Indizes berechnen",
    "text": "1.4 Reliabilität des Indizes berechnen\nBevor wir diesen Index einsetzen können, müssen wir zunächst noch checken, ob die Variablen auch inhaltlich zusammenpassen. Dazu ermitteln wir Cronbach’s Alpha als Maß der Skalenreliabilität:\n\nindex_vertrauen %&gt;%\n1  select(Ver_Gesundheitswesen:Ver_EU_Par) %&gt;%\n2  psych::alpha(check.keys=TRUE)\n\n\n1\n\nWir wählen mit select alle Variablen zwischen Ver_Gesundheitswesen und Ver_EU_Par aus\n\n2\n\nHier rufen wir das Paket psych auf, nutzen aus diesem die Funktion alpha, um Cronbach’s Alpha zu berechnen\n\n\n\n\n\nReliability analysis   \nCall: psych::alpha(x = ., check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.92      0.92    0.94      0.43  11 0.0021    4  1     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.92  0.92\nDuhachek  0.91  0.92  0.92\n\n Reliability if an item is dropped:\n                     raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r\nVer_Gesundheitswesen      0.92      0.92    0.94      0.44 11.0   0.0021 0.024\nVer_BVerfG                0.91      0.91    0.94      0.43 10.5   0.0022 0.023\nVer_Bundestag             0.91      0.91    0.93      0.41  9.9   0.0024 0.020\nVer_Verwaltung            0.91      0.91    0.94      0.43 10.6   0.0022 0.025\nVer_kath_Kirche           0.92      0.92    0.94      0.45 11.7   0.0020 0.018\nVer_evan_Kirche           0.92      0.92    0.94      0.45 11.3   0.0020 0.021\nVer_Justiz                0.91      0.91    0.93      0.42 10.3   0.0023 0.023\nVer_TV                    0.91      0.91    0.93      0.43 10.7   0.0022 0.023\nVer_Zeitung               0.91      0.91    0.93      0.43 10.5   0.0022 0.023\nVer_Uni                   0.91      0.92    0.94      0.44 10.8   0.0022 0.024\nVer_Regierung             0.91      0.91    0.93      0.41  9.8   0.0024 0.020\nVer_Polizei               0.91      0.91    0.94      0.43 10.7   0.0022 0.024\nVer_Parteien              0.91      0.91    0.93      0.42 10.1   0.0023 0.022\nVer_Kom_EU                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\nVer_EU_Par                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\n                     med.r\nVer_Gesundheitswesen  0.43\nVer_BVerfG            0.41\nVer_Bundestag         0.41\nVer_Verwaltung        0.42\nVer_kath_Kirche       0.43\nVer_evan_Kirche       0.43\nVer_Justiz            0.41\nVer_TV                0.43\nVer_Zeitung           0.41\nVer_Uni               0.42\nVer_Regierung         0.41\nVer_Polizei           0.43\nVer_Parteien          0.41\nVer_Kom_EU            0.41\nVer_EU_Par            0.41\n\n Item statistics \n                        n raw.r std.r r.cor r.drop mean  sd\nVer_Gesundheitswesen 3238  0.58  0.58  0.53   0.51  4.9 1.4\nVer_BVerfG           3238  0.70  0.69  0.67   0.64  5.3 1.5\nVer_Bundestag        3238  0.83  0.82  0.82   0.79  4.1 1.6\nVer_Verwaltung       3238  0.66  0.66  0.62   0.60  4.5 1.3\nVer_kath_Kirche      3238  0.47  0.46  0.42   0.38  2.3 1.5\nVer_evan_Kirche      3238  0.54  0.52  0.49   0.45  3.0 1.7\nVer_Justiz           3238  0.73  0.73  0.70   0.67  4.6 1.5\nVer_TV               3238  0.64  0.65  0.62   0.58  3.6 1.3\nVer_Zeitung          3238  0.67  0.68  0.66   0.62  4.0 1.3\nVer_Uni              3238  0.61  0.63  0.58   0.56  5.2 1.2\nVer_Regierung        3238  0.83  0.83  0.83   0.80  4.1 1.6\nVer_Polizei          3238  0.63  0.64  0.60   0.57  4.9 1.4\nVer_Parteien         3238  0.78  0.77  0.76   0.74  3.2 1.3\nVer_Kom_EU           3238  0.80  0.79  0.81   0.75  3.5 1.5\nVer_EU_Par           3238  0.80  0.79  0.81   0.76  3.6 1.6\n\nNon missing response frequency for each item\n                        1    2    3    4    5    6    7 miss\nVer_Gesundheitswesen 0.02 0.04 0.10 0.17 0.28 0.28 0.11    0\nVer_BVerfG           0.02 0.04 0.07 0.15 0.19 0.28 0.24    0\nVer_Bundestag        0.08 0.10 0.16 0.25 0.24 0.14 0.04    0\nVer_Verwaltung       0.03 0.05 0.13 0.26 0.31 0.18 0.04    0\nVer_kath_Kirche      0.42 0.20 0.16 0.13 0.05 0.03 0.02    0\nVer_evan_Kirche      0.26 0.16 0.19 0.19 0.11 0.07 0.02    0\nVer_Justiz           0.04 0.06 0.13 0.21 0.25 0.24 0.07    0\nVer_TV               0.08 0.13 0.21 0.33 0.18 0.05 0.01    0\nVer_Zeitung          0.05 0.09 0.18 0.30 0.26 0.11 0.01    0\nVer_Uni              0.01 0.02 0.05 0.17 0.29 0.34 0.12    0\nVer_Regierung        0.10 0.10 0.14 0.22 0.24 0.16 0.04    0\nVer_Polizei          0.02 0.04 0.08 0.18 0.28 0.30 0.10    0\nVer_Parteien         0.13 0.18 0.25 0.28 0.13 0.03 0.00    0\nVer_Kom_EU           0.13 0.14 0.19 0.26 0.19 0.08 0.02    0\nVer_EU_Par           0.13 0.14 0.18 0.25 0.19 0.09 0.02    0"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Zur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "href": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.6 Interpretation des Wirksamkeit-Indizes",
    "text": "1.6 Interpretation des Wirksamkeit-Indizes\nDie Werte sind ein gutes Ergebnis. Die Items zeigen eine gute Inter-Item-Korrelation.\nWir können noch nachschauen, ob wir die Skalen-Reliabilität verbessern können, indem wir einzelne Items herauswerfen. Denn der Output von Cronbachs Alpha gibt uns auch hilfreiche Aufschlüsse darüber, welche Items man evtl. ausschließen kann, um Cronbachs Alpha bei ungenügender Höhe noch auf ein mindestens akzeptables Maß zu heben. Diese Information findet sich im Bereich “Reliability if an item is dropped”:. In unserem Fall wird die reliabitlitä aber noch schlechter - wir können nichts mehr verbessern.\nEntsprechend haben wir erfolgreich einen Index für die latente Variable Vertrauen in gesellschaftliche Institutionen gebildet. Wir haben eine theoretische Grundlage gefunden, diese empirisch anhand der Daten des Allbus mittels explorativer Faktorenanalyse geprüft und einen Summenindex berechnet, dessen Qualität wir mittels Cronbachs Alpha zeigen konnten.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der Indexbildung werden meistens direkt im Text angegeben:\n✅ die Art des gebildeten Index (Summenindex, etc.)\n✅ Cronbachs Alpha\n✅ Enthaltene Indikatoren\nDas Format ist normalerweise:\n\nBeispiel: Der Summenindex individuelle Identität umfasst fünf Indikatoren (Ziele und Befriedigung, Regeln und Verantwortung, Gefühle oder Emotionen, Verständnis der Welt, individuelle Identität im Allgemeinen; α = 0,84)."
  },
  {
    "objectID": "Skript_5.3.html#quellen-für-das-script",
    "href": "Skript_5.3.html#quellen-für-das-script",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Quellen für das Script",
    "text": "1.8 Quellen für das Script\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "href": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Video\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten."
  },
  {
    "objectID": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "href": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse",
    "text": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse\nDie Faktorenanalyse bringt, wie jedes statistische Verfahren, eine Reihe von Vorraussetzungen mit. Diese Vorraussetzungen sollten wir kennen und bei der Anwendung der Faktorenanalyse beachten. Viele der Vorraussetzungen beziehen sich auf Pearson-Korrelationskoeffizienten, welcher die statistsiche Grundlage für die Berechnung der Faktoren bildet.\n\nVarianz: Wir sollten sichergehen, dass die Daten aus unserer Stichprobe ausreichend varrieren. Wir werfen hierfür ein Blick in die Daten.\n\n\n1colors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n1\n\nVisuelle Überprüfung mit einem Histogram für die erste Variable Ver_Gesundheitswesen. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nLinearität: Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Variablen. Wenn die tatsächliche Beziehung nicht linear ist, dann verringert sich der Wert von r. Wir können auf Linearität u.a. visuell durch das Betrachten der Daten mittel Streudiagramm prüfen.\n\n\n1ggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen, y = Ver_BVerfG)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"darkgreen\", size = 1)\n\n\n1\n\nVisuelle Überprüfung mit einem Streudiagramm für die erste Variable Ver_Gesundheitswesen & Ver_BVerfG. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nNormalverteilung: Der Pearson-Korrelationskoeffizient setzt eine Normalverteilung voraus. Allerdings finden sich in der Realität fast nie perfekt normalverteilte Daten. Schiefe und Kurtosis sind besonders einflussreich die Ergebnisse der Faktorenanalyse und können im Extremfall artefaktische Ergenbnisse erzeugen.\n\n\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n2\n\nStatistische Überprüfung mittels Shapiro Wilk Test für die erste Variable Ver_Gesundheitswesen. Ein p-Wert unter 0.05 = keine Normalverteilung und ein p-Wert über 0.05 = Normalverteilung\n\n\n\n\n\n\n2shapiro.test(allbus_vertrauen$Ver_Gesundheitswesen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_vertrauen$Ver_Gesundheitswesen\nW = 0.92081, p-value &lt; 2.2e-16\n\n\n\nLevel der Messung: Bei Pearson-Korrelationen wird davon ausgegangen, dass normalverteilte Variablen auf Intervall- oder Verhältnisskalen gemessen werden, d. h. es handelt sich um kontinuierliche Daten mit gleichen Intervallen. Diese Eigenschaften treffen nicht auf ordinale (bspw. Kategorien) oder dochotome (bspw. Wahr-Falsch-Items) Variablen zu, was sich negativ auf Pearson-Korrelationskoeffizieten auswirkt und zu verzerrten Ergebnissen führen kann. Allerdings ist ein beträchtlicher Teil der Daten, mit denen wir zu tun haben, ordinal oder dichotom skaliert, um auch mit diesen Daten arbeiten zu können nutzen wir die polychorische Korrelation, welche robuster Nicht-Normalverteilung ist.\nFehlende Werte: In jeder Studie sollten wir die Anzahl und die Art der fehlenden Werte sowie die Gründe und die Methoden für den Umgang mit diesen Daten angegeben werden.\n\n\nallbus_vertrauen = allbus_vertrauen %&gt;%   \n1  mutate(across(Ver_Gesundheitswesen:Ver_EU_Par, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n2  na.omit()\n\n\n1\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter. Während %in% angibt, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n2\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "title": "Die Faktorenanalyse",
    "section": "1.1 Teildatensatz mit den benötigten Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Variablen\nDie Variablen werden aufgrund ihrer Nützlichkeit als Indikatoren für die zu untersuchende latente Variable ausgewählt. Entsprechend ist es wichtig, dass die Variablen inhaltliche, diskriminante und konvergente Validität aufweisen. Etwas vereinfacht ausgedrückt sollten die Indikatoren über eine inhaltliche Passung zur latenten Variable verfügen, möglichst gut von anderen latenten Variablen abgrenzbar und mit mehreren unterschiedlichen Arten der Messung nachweisbar sein.\nIn unserem Fall möchten wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nFür die statistische Identifizierung einer latenten Variablen bzw. eines Faktors werden mindestens drei gemessene Variablen benötigt, obwohl mehr Indikatoren vorzuziehen sind. Es werden beispielsweise auch vier bis sechs Indikatoren pro Faktor empfohlen. Im Allgemeinen funktioniert die EFA besser, wenn jeder Faktor überdeterminiert ist (d. h. es werden mehrere gemessene Variablen von der zu entdeckenden latenten Variable bzw. Faktor beeinflusst). Unabhängig von der Anzahl sollten Variablen, die voneinander abhängig sind, nicht in eine EFA einbezogen werden.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nNeben der Auswahl der Variablen bzw. Indikatoren müssen auch die Fälle (in unserem Fall die Anzahl der befragten Personen) festgelegt werden. Hier sollten wir uns zunächst fragen, ob die Stichprobe der Teilnehmer:innen in Bezug auf die gemessenen Indikatoren sinnvoll ist? Handelt es sich um eine repräsentative Stichprobe? Bei dem Allbus ist das der Fall und entsprechend können wir davon ausgehen, dass wir eine passende Stichprobe für die Durchführung eine EFA vorliegen haben."
  },
  {
    "objectID": "Skript_5.2.html#referenzen",
    "href": "Skript_5.2.html#referenzen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. https://doi.org/10.1177/0095798418771807"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-würdigung",
    "href": "Skript_5.2.html#danksagung-und-würdigung",
    "title": "Die Faktorenanalyse",
    "section": "1.4 Danksagung und Würdigung",
    "text": "1.4 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-würdigung",
    "href": "Skript_5.3.html#danksagung-und-würdigung",
    "title": "Reliabilität von Skalen",
    "section": "1.9 Danksagung und Würdigung",
    "text": "1.9 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "href": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Verschiedene Arten von Indizes",
    "text": "1.2 Verschiedene Arten von Indizes\nEs gibt eine ganze Reihe von möglichen Arten von Indizes, welche wir theoretisch berechnen könnten.\n\n\n\n\n\n\n\n\nArt des Index\nBildung (Beispiel)\nBeschreibung\n\n\n\n\nUngewichteter additivier Index\nIndex = Indikator_1 + Indikator_2 + Indikator_3\nDie Ausprägungen der Indikatorvariablen werden addiert bzw. zu gemittelt.\n\n\nUngewichteter multiplikativer Index\nIndex = Indikator_1 * Indikator_2 * Indikator_3\nWenn ein Index Mindestausprägungen auf allen Indikatorvariablen voraussetzt sollte multiplikativ zu einem Gesamtindex verknüpft werden.\n\n\nGewichteter additivier Index\nIndex = (2*Indikator_1) + Indikator_2 + Indikator_3\nGewichtete additive Indizes ermöglichen eine differenzierte Behandlung der einzelnen Indikatoren.\n\n\n\nDie Entscheidung, welche Art der Indexbildung gewählt wird sollte vor dem Hintergrund der Daten, sowie der latenten Variable und deren Eigenschaften erfolgen. Beispielsweise würde es für ein Index, welcher die Zufriedenheit mit einer Bahnreise widerspiegelt und aus den Inidkatoren Reisedauer, Service während der Reise, Komfort während der Fahrt gebildet wird, Sinn ergeben einen Ungewichteten multiplikativen Index zu bilden, da bei einer Reisedauer von Null keine Reise stattgefunden hat und somit auch die anderen beiden Indikatoren nicht von Bedeutung sind."
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "1.5 Interpretation von Cronbach’s Alpha",
    "text": "1.5 Interpretation von Cronbach’s Alpha\nZur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-literatur",
    "href": "Skript_5.2.html#danksagung-und-literatur",
    "title": "Die Faktorenanalyse",
    "section": "2.1 Danksagung und Literatur",
    "text": "2.1 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-literatur",
    "href": "Skript_5.3.html#danksagung-und-literatur",
    "title": "Reliabilität von Skalen",
    "section": "1.7 Danksagung und Literatur",
    "text": "1.7 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_1.1.html#installation-von-r-und-rstudio",
    "href": "Skript_1.1.html#installation-von-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "3 Installation von R und RStudio",
    "text": "3 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "href": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "title": "Einführung in R und RStudio",
    "section": "4 Projekte und Ordnerstrukturen",
    "text": "4 Projekte und Ordnerstrukturen\nEinführung in die Logik von R, R Studio Server und das R Environment Einführung in Markdown (Quarto?) Basics der Befehlssyntax in Base R und Tidyverse (im Vergleich), ab dann aber alles in dplyr Laden von Daten und Importieren von anderen Datenformaten Einführung in das Datenmanagement: Projekte und Ordnerstrukturen auf dem PC Öffnen von Datensätzen, Laden von Daten, Importieren von anderen Datensätzen Speichern von Daten aus R in verschiedenen Formaten (Rda, csv etc.)"
  },
  {
    "objectID": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "href": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Pakete als Erweiterung von R",
    "text": "5 Pakete als Erweiterung von R\nInstallieren von Paketen und Laden von Librarys (p_load) Vorschau: Datentypen und deren Charakteristika."
  },
  {
    "objectID": "Skript_1.3.html#r-markdown",
    "href": "Skript_1.3.html#r-markdown",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown",
    "text": "1 R Markdown\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen. Markdown bzw. Quarto-Dokumente bestehen aus drei Bestandteilen\n\n1.1 YAML-Header\n\n\n1.2 Text\n\n\n1.3 Code Chunks\n💡"
  },
  {
    "objectID": "Skript_1.3.html#quarto",
    "href": "Skript_1.3.html#quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Quarto",
    "text": "2 Quarto\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen)."
  },
  {
    "objectID": "Skript_1.3.html#literatur",
    "href": "Skript_1.3.html#literatur",
    "title": "Die Logik von Markdown und Quarto",
    "section": "7 Literatur",
    "text": "7 Literatur\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link\n\n\n📖 Xie, Y., Allaire, J. J., & Grolemund, G. (2020). R markdown: The definitive guide. Chapman; Hall/CRC Link\n\n\n📖 Xie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman and Hall/CRC Link\n\n\n📖 Allaire, J. J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., & Iannone, R. (2020a). rmarkdown: Dynamic documents for r. Link"
  },
  {
    "objectID": "Autoren.html#katharina-maubach",
    "href": "Autoren.html#katharina-maubach",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Vita\n\n\n\n\n\nKatharina Maubach, © B. Köhler\n\n\nKatharina Maubach ist seit Dezember 2022 als wissenschaftliche Mitarbeiterin am Zentrum für Medien-, Kommunikations- und Informationsforschung der Universität Bremen im Lab „Politische Kommunikation und Innovative Methoden” tätig. Sie forscht im Rahmen des DFG-geförderten Projektes „Remixing Political News Reception” unter der Leitung von Prof. Stephanie Geise zur Rezeption multimodaler Medieninhalte. Innerhalb des Projektes war sie seit Februar 2021 an der Universität Münster beschäftigt, wo Sie zudem von Januar 2019 bis Januar 2021 im Arbeitsbereich von Prof. Volker Gehrau forschte und lehrte. Ihr Studium der Kommunikationswissenschaft absolvierte sie ebenfalls an der Universität Münster; in ihrer Masterarbeit forschte sie zum Einfluss politischer Nachrichtensatire auf politisches Interesse und Informiertheit.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nRezeptions- und Wirkungsforschung\nKlimakommunikation\nStatistische Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_1.4.html",
    "href": "Skript_1.4.html",
    "title": "Vorbereiten der R-Umgebung",
    "section": "",
    "text": "Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_1.4.html#section",
    "href": "Skript_1.4.html#section",
    "title": "Die Kunst vom Umgang mit Daten",
    "section": "1.1 ",
    "text": "1.1"
  },
  {
    "objectID": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "href": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.1 Das Anlegen eines R-Projekts",
    "text": "1.1 Das Anlegen eines R-Projekts\nStarten wir zunächst mit dem Anlegen eines sogenannten R-Projekts. RStudio-Projekte ermöglichen es uns alle mit einem Projekt verbundenen Dateien an einem Ort zu speichern. Das umfasst Datensätze, R-Skripte, Ergebnisse, Abbildungen, Berichte usw. Projekte sind bereits in RStudio integriert.\n Das gerade aktive RProjekt sehen wir in der rechts oberen Ecke des Nutzer:innen Interfaces von RStudio. Hier können wir durch durch den Button New Project ein neues Projekt anlegen, hierfür folgen wir einfach dem Menüverlauf. Für einen guten, reproduzierbaren Arbeitsablauf sollten alle Analyse Projekte mit der Erstellung eines Projekts beginnen."
  },
  {
    "objectID": "Skript_1.4.html#das-working-dictonary-wd",
    "href": "Skript_1.4.html#das-working-dictonary-wd",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.2 Das Working-Dictonary (wd)",
    "text": "1.2 Das Working-Dictonary (wd)\nIm Allgemeinen ist das Arbeitsverzeichnis (das Working-Dictonary, wd) der Ort, an dem R nach Dateien (vor allem nach Datensätzen) sucht. Wenn ihr kein Projekt verwenden, müsst ihr wahrscheinlich durch die Funktion setwd oder das Interface (siehe Screenshot) ein Working-Dictonary setzen, bevor ihr euren R-Code ausführen könnt.\n Wenn ihr beispielsweise den Code verwenden möchtet, den wir euch in diesem Kurs zu Verfügung stellen, müsst ihr darauf achten, dass ihr die Dateipfade auf euer Working-Dictonary und Ordnerstruktur anpasst.\n\n1daten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n1\n\nDer Bereich in Anführungszeichen gibt den Dateipfad an. Damit sagt ihr R, wo sich die zu ladende Datei befindet.\n\n\n\n\nIn dem Beispiel ist im Working-Dictonary des Projekts ein Unterordner mit der zu ladenden Datei. Der Dateipfad befindet sich im R-Code in den Anführungszeichen, also \"Datensatz/Allbus_2021.dta\". Der erste Teil \"Datensatz/\" gibt an, dass sich die Datei in einem Ordner befindet, der Datensatz heißt. Der zweite Teil Allbus_2021.dta\" gibt den eigentlichen Dateinamen mit der entsprechend Dateiendung an (also .dta). Wahrscheinlich müsst ihr den R-Code entsprechend anpassen."
  },
  {
    "objectID": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "href": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.3 Das Zugreifen auf Dateien und Datensätze",
    "text": "1.3 Das Zugreifen auf Dateien und Datensätze\nBevor wir allerdings auf Dateien und Datensätze zugreifen können, müssen wir diese zunächst in unseren Projekt-Ordner (also in das Working-Dictonary) verschieben. Falls ihr für diesen Kurs die Desktop-Version von RStudio nutzt, könnt ihr hier einfach die entsprechenden Datein auf eurem lokalen Rechner in das zuvor von euch festgelegte Working-Dictonary kopieren. Wenn ihr die Cloud-Version von RStudio nutzt, müsst ihr hier die Dateien und Datensätze zunächst in die Cloud laden. Hierfür könnt ihr auf den Upload-Button in der unteren rechten Ecke des Interfaces gehen und in dem sich öffnenden Fenster die entsprechende Datei auswählen.\n\n\n\nScreenshot RStudio\n\n\nSobald ihr durch das Klicken von OK bestätigt habt, wird die Datei hochgeladen und erscheint in dem unteren rechten Fenster im Files-Reiter."
  },
  {
    "objectID": "Skript_1.4.html#die-installation-von-paketen",
    "href": "Skript_1.4.html#die-installation-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Installation von Paketen",
    "text": "1.4 Die Installation von Paketen\nWir möchten für unser Analyse-Projekt nicht nur die Standartausführung von R verwenden, welche auch als base R bezeichnet wird, sondnern einige Pakte installieren. R-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Wir können Pakete über CRAN (Comprehensive R Archive Network) - das ist ein zentrales Software-Repository - installieren. R hat den Vorteil, dass es über eine große Zahl an frei verfügbaren und einfach zu installierenden Paketen verfügt. Das hat unter anderem dazu geführt, dass die Programmiersprache im Data Science Bereich verbreitet ist.\nWenn wir ein R-Paket installieren möchten können wir hierfür die Funktion install.packages verwenden, in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\nPaket 'tidyverse' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpyghIHy\\downloaded_packages\n\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und Bestätigt.\n Einige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden."
  },
  {
    "objectID": "Skript_1.4.html#das-laden-von-paketen",
    "href": "Skript_1.4.html#das-laden-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Das Laden von Paketen",
    "text": "1.5 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet.\nDies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen).\n\nlibrary(tidyr)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio"
  },
  {
    "objectID": "Skript_1.4.html#die-session",
    "href": "Skript_1.4.html#die-session",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Session",
    "text": "1.4 Die Session\nDie sogenannte Session wurde bereits mehrmals kurz erwähnt. Bei der Session handelt es sich um eine laufende Instanz des R-Programms, die ihr beenden und neustarten könnt. Das macht insbesondere dann Sinn, wenn ihr die falsche Version eines Pakets installiert habt oder ihr einen sauberen Start für euren R Code haben möchtet. Durch einen Neustart der Session könnt ihr euch sicher sein, dass ihr nicht ausversehen irgendwelche zuvor verwendeten Pakete oder Berechnungen mitnehmt.\nIhr könnt die Session mit der .rs.restartR Funktion in der Konsole neustarten. Alternativ könnt ihr im R Studio Interface auf Sessionund Restart R gehen.\n\n\n\nScreenshot RStudio\n\n\nEin kurze Mitteilung in der Konsole gibt euch an, dass die Session neu gestartet wurde."
  },
  {
    "objectID": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "href": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Hilfe zur Selbsthilfe",
    "text": "1.5 Hilfe zur Selbsthilfe\nIhr werdet im Laufe des Kurses immer wieder mit Fehlermeldungen zu tun haben. Hier ist es wichtig einen kühlen Kopf zu bewahren und möglichst systematisch zu versuchen den Fehler zu finden und zu beheben. Der Umgang mit Fehlern ist ein großer und wichtiger Teil bei jeglicher Programmierung und es gibt hier ein paar Tipps, wie man damit am besten umgeht.\n\nAtmen: Bleib ruhig und atme ein paar mal tief durch, gerade wenn es nicht der erste Fehler ist der dir heute begegnet.\nLesen: Lies aufmerksam die Fehlermeldung.\nCode überprüfen: Schaue nach, ob sich ein Warnzeichen am Rand deines R Codes befindet. Dieses Warnzeichen weist dich auf einen Syntaxfehler (bspw. eine vergessene Klammer oder Komma) hin.\nVariablen überprüfen: Überprüfe kurz, ob du die richtigen Variablen verwendest oder sich ein Tippfehler eingeschlichen hat.\nInformationen suchen: Rufe eine Vignette (mit dem Befehl vignette) oder rufe die Hilfeseite mit einem der Funktion vorgelagerten Fragezeichen auf (mit bspw. ?dpylr()) auf und lies sie dir durch.\nSich im Netz Hilfe suchen: Suche auf Stack Overflow nach deinem Problem, meistens hat jemand auf der Welt das oder ein ähnliches Problem schon einmal gelöst.\nDie KI fragen: Nutze ein KI Chatbot, um dir die Fehlermeldung erklären zu lassen und Lösungsvorschläge zu unterbreiten. Beachte hierbei bitte, dass die Antworten nicht immer korrekt sind und meist Kontextwissen von dir benötigt wird, um die Korrektheit der Antworten zu prüfen Link.\nFrag uns: Frag dein:e Dozent:in :)"
  },
  {
    "objectID": "Skript_1.1.html#was-ist-r",
    "href": "Skript_1.1.html#was-ist-r",
    "title": "Einführung in R und RStudio",
    "section": "1 Was ist R?",
    "text": "1 Was ist R?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten."
  },
  {
    "objectID": "Skript_1.1.html#was-ist-rstudio",
    "href": "Skript_1.1.html#was-ist-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "2 Was ist RStudio?",
    "text": "2 Was ist RStudio?\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten."
  },
  {
    "objectID": "Skript_1.1.html#das-interface-von-rstudio",
    "href": "Skript_1.1.html#das-interface-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Das Interface von RStudio",
    "text": "4 Das Interface von RStudio\nGrundsätzliches zur Oberfläche\nSchaltflächen\nMenüleiste\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "href": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Die Programmoberfläche von RStudio",
    "text": "4 Die Programmoberfläche von RStudio\n\n4.1 Schaltflächen\nInnerhalb von RStudio unterscheiden wir 4 Schaltflächen welche sich beliebig via Drag and Drop verschieben oder auch minimieren lassen.\n\n1 Ist der Bereich in welchen wir Skripte, Markdown und Quarto-Dokumente bearbeiten und ausführen können (siehe auch Markdown und Quarto)\n2 Ist die Konsole. Dies ist der Bereich in welchem wir weiterhin oldschool R angezeigt bekommen. Dieser Bereich kann sehr hilfreich sein, wenn man kurz Befehle benötigt, welche nicht im Skript auftauchen sollen (beispielsweise eine kurze Hilfe zu Funktionen mittels ?)\n3 In diesem Bereich findet sich alles zu den innerhalb von R geladenen Datensätzen. Unter Environment finden sich die Datensätze (Data), die Historie der genutzten Befehle (History), eine Schnittstelle zu Datenbanken (Connection) sowie R-interne Tutorials (Tutorial).\n4 In diesem Bereich finden sich verschiedene Reiter, welche die Organisation der Arbeit mit R erleichtern. Unter File befinden sich alle innerhalb des Ordners oder Projektes befindlichen Dateien. Unter Plots kann man sich die in R erstellten Grafiken anzeigen lassen. Packages zeigt alle in R installierten Pakete an. Hier lassen sich mittels install auch neue Pakete installieren oder über update die aktuellen Pakete updaten. Bei Help können wir eine Hilfeseite aufrufen. Es kann wahlweise direkt innerhalb der Seite nach Hilfen gesucht werden oder mit dem Befehl ?Name der Funktion bzw. ??Name des Packages innerhalb der Konsole. Möchte man nach einem Befehl aus einem Paket suchen nutzt man den Suchbefehle?Name des Packages::Name der Funktion. Unter Viewer finden sich gerenderte Dokumente (beispielsweise ein gerendertes Markdown oder Quarto Dokument) und unter Presentation gerenderte Shiny-Dokumente.\n\n\n4.2 Menüleiste\nZusätzlich zu den Schaltflächen findet sich oben links eine Menüleiste:\n\n\nUnter File können neue Dateien erstellt, geöffnet und gespeichert werden.\nEdit bietet Möglichkeiten der Dateibearbeitung (bspw. Kopieren, Ausschneiden, Rückgängig etc.) falls ihr keine Short-Cuts nutzen wollt.\nCode gibt eine Übersicht über Funktionen innerhalb des Markdown-Dokumentes (bspw. Codechunks einfügen).\nUnter View können die einzelnen Schaltflächen und deren Aufteilung geändert werden. Wahlweise geht dies auch via Drag & Drop.\nPlots vereinfacht den Umgang mit in r erstellten Grafiken. Wahlweise könnt ihr hier auch den Reiter Plots in Schaltfläche 4 nutzen.\nSession hier kann eine R-Session neu gestartet oder beendet werden (siehe auch 1.6 Die Session).\nBuild, Debug und Profile beinhaltet Sonderanwendungen, wie beispielsweise das Debugging von Funktionen oder Fragen nach der Speed-Optimierung von R-Code.\nTools hat viele hilfreiche Funktionen. Hier können unter anderem Pakete installiert und hilfreiche Keyboard Shortcuts ausgegeben werden. Am bedeutsamsten ist hier jedoch der Bereich Global Options, in welchem unter anderem grundlegende Einstellungen zum Speicherort von R und den Paketen, zur Aufteilung und Aussehen von RStudio und zur Funktionalität von R Markdown getroffen werden können.\n\nAuch der Reiter Help kann sehr hilfreich sein. Hier finden sich eine Hilfeseite (siehe auch Reiter Help in Schaltfläche 4), Möglichkeiten der besseren Zugänglichkeit (Accessibility), Cheat Sheets für die Arbeit mit R und erneut eine Übersicht über Shortcuts für die Arbeit mit R."
  },
  {
    "objectID": "Skript_1.1.html#die-logik-von-r",
    "href": "Skript_1.1.html#die-logik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Logik von R",
    "text": "5 Die Logik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen. Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-grundlogik-von-r",
    "href": "Skript_1.1.html#die-grundlogik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Grundlogik von R",
    "text": "5 Die Grundlogik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen (💡 Fun-Fact, R “denkt nicht mit”, also auch wenn für euch klar ist, dass ihr doch die gesamte Zeit mit dem selben Datensatz arbeitet, muss R das immer wieder gesagt bekommen). Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\nDabei haben die meisten Funktionen noch weitere Zusatzoptionen, welche wir nutzen können. Bei dem Befehl mean können wir beispielsweise angeben, ob fehlende Werte in die Berechnung mit einfließen sollen oder nicht. Dies geschieht mit den Zusatz na.rm = TRUE bzw. na.rm = FALSE. Na.rm steht in diesem Fall für NA (=not available, fehlende Fälle) und remove (also entfernen), fragt demnach ob fehlende Fälle aus der Berechnung ausgeschlossen werden sollen. Dabei ist die default-Option, also die Option die in dem Befehl voreingestellt ist, dass fehlende Werte nicht aus der Berechnung ausgeschlossen werden (na.rm=F). Die meisten Befehle haben bestimmte defaults, da dies diese den Normalfall der Nutzung beschreiben und uns beim Programmieren Schreibaufwand ersparen (möchten wir die defaults nutzen, müssen wir immerhin nichts zusätzliches in der Funktion angeben). Allerdings können wir hier auch immer die anderen Optionen nutzen, wir müssen dies nur in unserem Befehl angeben:\n\nmean(data$Alter, na.rm=T)\n\nHier haben wir auch bereits eine weitere Funktionalität von R kennengelernt, nämlich die Operationalisierung einzelner Parameter über T (TRUE) und F (FALSE). Doch woher weiß ich nun als neuer Nutzer, welche Optionen mit bei einzelnen Befehlen zur Verfügung stehen?\nHier hilf ein Blick in die Hilfeseite, welche wir beispielsweise für den Mittelwert mit dem Befehl ?mean() aufrufen können. Für jede Funktion stehen - wie oben bereits erwähnt - Hilfeseiten zur Verfügung. Diese beinhalten zunächst eine kurze Beschreibung Description, anschließend einen Überblick zur Nutzung Usage sowie dem Default des Befehles. Anschließend finden sich die Arguments, dies sind die Möglichkeiten, wie wir die Funktion nutzen können und welche zusätzlichen Optionen zur Verfügung stehen. Oftmals finden sich zudem weitere Erklärungen und Beispiele der Nutzung.\n\n5.1 Kurz-Exkurs: das tidyverse\nIn R selbst findet sich eine Vielzahl von Befehlen. Zusätzlich wird R von den Nutzern immer weiter entwickelt und es kommen neue Funktionen in Form von Paketen hinzu. Eines der meist genutzten Pakete(-universen) stellt dabei das tidyverse dar. Mit den Befehlen und Funktionen dieses Paketes kommt eine etwas andere Programmiersprache, welche uns jedoch die Arbeit mit R erleichtert. Gerade auch bei Fragen des Datenmanagementes ist das tidyverse hilfreich, denn hier kommt ein zweiter Fun-Fact über R:💡 R kann nicht nur manchmal etwas dumm sein (wir erinnern uns, es denkt nicht mit), es ist auch recht vergesslich. Wir haben oben bereits gelernt, dass wir in Befehlen immer den Datensatz und die Variable spezifizieren müssen. Dies stellt kein Problem bei einzelnen Befehlen dar, ist jedoch bei einer Vielzahl von Befehlen etwas nervig. Hier kommt uns die tidyverse Logik zu Nutze, in welcher einmal zu Beginn des Dokumentes der Datensatz spezifziert wird und anschließend alle weiteren Schritte durch eine Pipe %&gt;% verbunden werden. Die Pipe (Shortcut Windows: Ctrl + Shift + M; MAC: Cmd + Shift + M) bedeutet so viel wie “und dann”. Also im Prinzip sagen wir R, nehme diesen Datensatz und dann mache die folgenden Dinge, wobei wir so viele Schritte wie wir möchten jeweils mit Pipes verbinden können. Wir nutzen in unseren Skripten hauptsächlich die tidyverse-Logik, erklären diese daher grundlegender in Kapitel 3.1, wenn wir uns mit den ersten Schritten des Datenmanagements beschäftigen."
  },
  {
    "objectID": "Skript_1.1.html#pakete",
    "href": "Skript_1.1.html#pakete",
    "title": "Einführung in R und RStudio",
    "section": "6 Pakete",
    "text": "6 Pakete\nR-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Diese erweiteren die Funktionen, die bereits in der Standardausführung von R (bekannt als base R) gegeben sind.\n\n6.1 Die Installation von Paketen\nPakete können wahlweise R-intern über CRAN (das steht für Comprehensive R Archive Network und ist das zentrale Software-Repository) oder direkt von GitHub (für sehr neue Pakete, welche noch nicht auf CRAN sind) installiert werden. Im Normalfall installieren wir jedoch direkt von CRAN, da hier eine Vielzahl von Paketen und Funktionen vorhanden sind und die Installation sehr simpel ist.\nWenn wir ein R-Paket von CRAN installieren möchten nutzen wir die Funktion install.packages(), in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen. Für das Paket tidyverse wäre der Befehl wie folgt:\n\ninstall.packages(\"tidyverse\")\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und bestätigt die Anwendung wiederum mit Install.\n\n\n\nScreenshot RStudio\n\n\nEinige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden. Grundsätzlich ist es jedoch immer ratsam, einmal zu checken, ob die Pakete in der aktuellen Version installiert sind, da ansonsten die Funktionen der Pakete nicht funktionieren können.\n\n\n6.2 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet. Dies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen):\n\nlibrary(tidyverse)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio\n\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#was-sind-r-und-rstudio",
    "href": "Skript_1.1.html#was-sind-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "1 Was sind R und RStudio?",
    "text": "1 Was sind R und RStudio?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten.\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten.\n\n1.1 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "href": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Getting Started mit Markdown und Quarto",
    "text": "3 Getting Started mit Markdown und Quarto\n\n3.1 Installation\n\n\n3.2 Dokument laden\n\n\n3.3 Neues Dokument anlegen"
  },
  {
    "objectID": "Skript_1.3.html#r-markdown-und-quarto",
    "href": "Skript_1.3.html#r-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown und Quarto",
    "text": "1 R Markdown und Quarto\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen.\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen). Markdown-Dokumente weisen die Endung .rmd auf wohingegen Quarto-Dokumente den Endung .qmd haben. Beide Dokumenttypen können für unseren Kurszweck gleichwertig genutzt werden. Lediglich bei der Arbeit mit interaktiven, multimedialen oder mehrsprachigen Dokumenten ist Quarto besser geeignet als Markdown."
  },
  {
    "objectID": "Skript_1.3.html#installation-der-programme",
    "href": "Skript_1.3.html#installation-der-programme",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Installation der Programme",
    "text": "2 Installation der Programme\nMarkdown kann innerhalb von R mit dem Befehl install.packages(\"Rmarkdown\") installiert und anschließend geladen werden. Da es sich bei Quarto nicht um ein Package, sondern ein eigenes Interface handelt, muss das Programm extern heruntergeladen und auf dem Rechner installiert werden."
  },
  {
    "objectID": "Skript_1.3.html#bestehende-dokument-laden",
    "href": "Skript_1.3.html#bestehende-dokument-laden",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Bestehende Dokument laden",
    "text": "3 Bestehende Dokument laden\nZum Laden eines Quarto- oder Markdown Dokumentes könnt ihr dieses einfach auf eurem Rechner doppelklicken. Wahlweise könnt ihr auch über File -&gt; Open File (oder Strg + O) innerhalb von RStudio ein Dokument öffnen."
  },
  {
    "objectID": "Skript_1.3.html#neue-dokumente-anlegen",
    "href": "Skript_1.3.html#neue-dokumente-anlegen",
    "title": "Die Logik von Markdown und Quarto",
    "section": "4 Neue Dokumente anlegen",
    "text": "4 Neue Dokumente anlegen\nUm ein neues Dokument anzulegen, könnt ihr einfach über File -&gt; New File das gewünschte Dokument anlegen. Sowohl bei Markdown, als auch bei Quarto öffnet sich dann das Folgende Fenster:\n\n\n\nNeues Dokument anlegen\n\n\nDort könnt ihr euer Dokument benennen (Title), die Autoren festlegen (Author) sowie das Output-Format (HTML, PDF oder WORD) festlegen. Zudem könnt ihr bei Quarto angeben, wie ist das Dokument gerendert werden soll (Knitr vs. Jupyter). Hier könnt ihr die Auswahl auf Knitr belassen. Zuletzt könnt ihr festlegen, ob ihr den visual markdown editor oder den source editor nutzen möchtet. All diese Punkte könnt ihr jedoch auch noch nachfolgend im YAML-Header oder im Menü) ändern."
  },
  {
    "objectID": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "href": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6 Überblick über die Dokumentkomponenten",
    "text": "6 Überblick über die Dokumentkomponenten\nSowohl Markdown, als auch Quarto-Dokumente bestehen aus drei Bestandteilen: dem YAML-Header, Textbereichen und Codebereichen.\n\n6.1 YAML-Header\nInnerhalb des YAML Headers, welcher jeweils von --- umgeben ist, legen wir die Dokumentstruktur fest.\n\n\n\nYAML Header in einem Quarto Dokument\n\n\nDies beinhaltet beispielsweise den Titel des Dokumentes title:, die Autoren author, sowie Spezifikationen zur Dokumentstruktur, wie beispielsweise das Outputformat format: oder auch in Quarto Spezifikationen zum Umgang mit den Codechunks execute: echo : true auf Gesamtdokumentebene.\n\n\n6.2 Text\nIn Markdown und Quarto-Dokumenten können wir Text einbinden und diesen beliebig formatieren. Dazu können wir wahlweise die Source-Variante oder die Visual-Variante nutzen. In der Source-Variante variieren wir Text mittels Syntax. Typische Syntaxbefehle sind:\n\n*kursiv*: jeweils einen Stern vor und nach einem Wort um dieses kursiv zu schreiben\n**fett**: jeweils zwei Sterne vor und nach einem Wort um dieses fett zu schreiben\n#: Rauten für Überschriften, wobei eine Raute die erste Überschrift signalisiert, zwei Rauten die zweite usw.\n![Bildunterschrift](Link des Bildes): um Bilder einzufügen\n[Linktext](url): Um Links einzufügen\n\nMöchten wir übrigens die oben genutzten Symbole im Text nutzen, so können wir mit einem  vor dem jeweiligen Symbol die Formatierung umgehen.\nWahlweise können wir auch den Visual-Modus nutzen, indem wir oben in der Dokumentleiste von Sourceauf Visual umstellen. In diesem Modus erhalten wir ein Word-ähnliches Interface und können Formatoptionen durch Klicken auf die jeweilige Formatierung umsetzen:\n\n\n\nFormatierungsoptionen im Visual Modus\n\n\n\n\n6.3 Code Chunks\nInnerhalb von Markdown und Quarto können wir Codebefehle direkt in unser Dokument innerhalb von sogenannten Codechunks integrieren. Hier können wir alle Arten von Code schreiben sowie diese mit Hilfe von # direkt kommentieren (alles hinter einer Raute wird dabei nicht ausgeführt). Codechunks beginnen mit drei Backticks und einem r in geschweiften Klammern und enden wieder mit drei Backticks:\n\n```{r}\n# Dies ist ein Code Chunk\n```\n\nUm die Codechunks zu erzeugen können wir einfach auf Code -&gt; Insert Codechunk gehen, auf das +C-Symbol in der Dokumentmenüleiste oder den Shortcut Alt + Strg + I nutzen. Innerhalb der Chunks können wir Code schreiben und ausführen. Dies geschieht für eine einzelne Codezeile mit dem Shortcut Strg + Enter und für den gesamten Codechunk mit dem Shortcut Strg + Shift + Enter. Wahlweise könnt ihr auch den kleinen grünen Pfeil in der rechten oberen Ecke des Chunks, Code -&gt; Run Selected Lines oder den Punkt Run in der Quarto-Dokumentleiste auswählen.\nZusätzlich können wir hinter dem {r} angeben, wie R mit dem Code des Chunks umgehen soll. Wir können beispielsweise auswählen, ob R den Code ausführen soll (eval = T/F) ob der Codebereich in unserem Enddokument aufgeführt sein soll (echo = T) oder wir lediglich die Ergebnisse angezeigt wollen (echo = F) oder ob wir beispielsweise Warnungen (warnings = T/F) oder Messages (message = T/F) in unserem Output-Dokument wünschen:\n\n```{r, echo = T}\n# Dies ist ein Code Chunk\n```\n\nWahlweise können wir diese Optionen auch für das Gesamtdokument im YAML-Header festlegen. Dafür nutzen wir den Zusatz execute: und geben anschließend alle unsere Dokumentoptionen (für einen Überblick siehe hier) an.\n\n\n\nCodeoptionen im Quarto Header\n\n\nAchtung: wir nutzen hier : statt = und schreiben true und false statt TRUE/T und FALSE/F."
  },
  {
    "objectID": "Skript_1.3.html#übergreifende-menüpunkte",
    "href": "Skript_1.3.html#übergreifende-menüpunkte",
    "title": "Die Logik von Markdown und Quarto",
    "section": "5 Übergreifende Menüpunkte",
    "text": "5 Übergreifende Menüpunkte\nUnterhalb der allgemeinen RStudio-Menüleiste findet ihr eine eigene Markdown und Quarto-Dokumentleiste:\n\n\n\nDokumentmenüleistet\n\n\nHier könnt ihr euer Dokument speichern (Diskettensymbol), in eurem Dokument suchen und ersetzen (Lupensymbol) oder euer Dokument Rendern (Quarto) oder Knitten (Markdown). Hiermit rendert ihr euer Dokument von R in euer gewünschtes Outputformat (Word, Pdf, oder Html). Zusätzlich könnt ihr mit dem Zahnrad Optionen für den Umgang mit dem gerenderten Dokument und den Codechunks festlegen. Zusätzlich könnt ihr hier neue Codechunks anlegen (+C-Symbol auf der rechten Seite) oder die bestehenden Code-Chunks ausführen (Run-Symbol)"
  },
  {
    "objectID": "Skript_6.4.html#die-varianzanalyse",
    "href": "Skript_6.4.html#die-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Die Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'rootSolve', 'lmom', 'expm', 'Exact', 'gld'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'rootSolve' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'lmom' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'expm' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Exact' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gld' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'DescTools' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nDescTools installed\n\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'multcompView', 'gmp', 'Rmpfr', 'kSamples', 'BWStest'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'multcompView' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gmp' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Rmpfr' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'kSamples' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'BWStest' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'PMCMRplus' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nPMCMRplus installed\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse",
    "text": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n2.1 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden."
  },
  {
    "objectID": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "href": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "title": "Die Varianzanalyse",
    "section": "3 Überprüfung der Homogenität der Fehlervarianzen",
    "text": "3 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen."
  },
  {
    "objectID": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "href": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "title": "Die Varianzanalyse",
    "section": "4 Einfaktorielle Varianzanalyse (ohne Messwiederholung)",
    "text": "4 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests."
  },
  {
    "objectID": "Skript_6.4.html#posthoctests",
    "href": "Skript_6.4.html#posthoctests",
    "title": "Die Varianzanalyse",
    "section": "5 PostHocTests",
    "text": "5 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "href": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "title": "Die Varianzanalyse",
    "section": "6 Exkurs: Kruskal Wallis Test",
    "text": "6 Exkurs: Kruskal Wallis Test"
  },
  {
    "objectID": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "7 Mehrfaktorielle Varianzanalyse",
    "text": "7 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n7.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n7.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon",
    "text": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs: nicht parametrische Tests",
    "text": "3.2 Exkurs: nicht parametrische Tests\nMann-Whitney-U & Wilcoxon"
  },
  {
    "objectID": "Skript_3.1.html",
    "href": "Skript_3.1.html",
    "title": "Die Kunst vom Umgang mit Daten",
    "section": "",
    "text": "Bild von Mohamed Hassan auf Pixabay\n\n\n\n1 Die Kunst vom Umgang mit Daten\nIn einer Ära, in der Daten in nahezu allen Bereichen unseres Lebens eine immer größere Rolle spielen, ist die Fähigkeit, mit Daten effektiv umzugehen, zu einer entscheidenden Kompetenz geworden. Ob in der Wissenschaft, der Wirtschaft oder im Alltag – die Kunst der Datenanalyse ermöglicht es uns, aus einer Fülle von Informationen wertvolle Erkenntnisse zu gewinnen und informierte Entscheidungen zu treffen.\nIn diesem Kapitel werden wir uns eingehend mit den wesentlichen Aspekten des Umgangs mit Daten auseinandersetzen. Wir beginnen mit einer Betrachtung der verschiedenen Datentypen und Strukturen, die uns begegnen. Von numerischen Werten über kategoriale Daten bis hin zu sogenannten logischen Werten – wir werden verstehen, wie wir Daten in R effizient repräsentieren können.\nDie Kunst des Umgangs mit Daten erstreckt sich jedoch weit über das bloße Verständnis von Datentypen hinaus. Wir werden lernen, wie wir Daten auswählen, manipulieren und transformieren können, um sie optimal für unsere Analysezwecke vorzubereiten. Dabei werden wir uns Techniken ansehen, die es ermöglichen, relevante Informationen zu extrahieren und Daten in die gewünschte Form zu bringen.\nEine besonders mächtige Fähigkeit auf unserer Reise ist die Fähigkeit, Daten visuell zu interpretieren. Wir werden in die Welt von ‘ggplot2’ eintauchen, eines R-Pakets zur Erstellung ansprechender und aussagekräftiger Grafiken. Wir werden entdecken, wie wir Daten effektiv visualisieren können, um Muster, Trends und Abhängigkeiten sichtbar zu machen.\nDieses Kapitel bietet Ihnen eine umfassende Einführung in die Kunst des Umgangs mit Daten. Wir werden praktische Fertigkeiten erlernen und gleichzeitig die theoretischen Grundlagen hinter diesen Techniken verstehen."
  },
  {
    "objectID": "Skript_3.2.html#datentypen-in-r",
    "href": "Skript_3.2.html#datentypen-in-r",
    "title": "Datentypen und -strukturen",
    "section": "1.1 Datentypen in R",
    "text": "1.1 Datentypen in R\nIn der Datenanalyse ist die korrekte Interpretation und Handhabung von verschiedenen Datentypen von entscheidender Bedeutung. R bietet eine Vielzahl von Datentypen, die es uns ermöglichen, eine breite Palette von Informationen zu repräsentieren und zu verarbeiten. In diesem Kapitel werden wir uns mit den fünf grundlegenden Datentypen in R vertraut machen: Numeric, Integer, Logical, Character und Factor.\n\n1.1.1 Numeric\nDer Datentyp Numeric repräsentiert dezimale Zahlen. Beispiele für numerische Daten sind 4.5, -12, 0.75 usw. Numerische Daten werden oft für Berechnungen und mathematische Operationen verwendet.\n\n\n1.1.2 Integer\nDer Integer-Datentyp repräsentiert ganze Zahlen. Beachten Sie, dass Ganze Zahlen ebenfalls zum Datentyp NUmeric gehören, da diese als spezielle Art von Dezimalzahlen betrachtet werden - dabei gilt: alle ganzen Zahlen (Integer-Datentyp) sind numerische Daten, aber nicht alle numerischen Daten (Numeric-Datentyp) sind ganze Zahlen. Beispiele für Integer-Daten sind 4, -7, 100 usw.\n\n\n1.1.3 Logical\nDer Logical-Datentyp besteht aus sogenannten booleschen Werten, die genau zwei verschiedene Ausprägungen annehmen können: entweder TRUE (wahr) oder FALSE (falsch). Boolesche Werte werden häufig in logischen Ausdrücken und Bedingungen verwendet, um Entscheidungen zu treffen und Filter anzuwenden.\n\n\n1.1.4 Character\nDer Character-Datentyp repräsentiert Textwerte oder Zeichenketten. Textwerte werden in R in doppelten Anführungszeichen (” “) oder einfachen Anführungszeichen (’ ’) eingeschlossen. Beispiele für Character-Daten sind”Hallo”, “Datenanalyse” usw.\n\n\n1.1.5 Factor\nDer Factor-Datentyp wird für kategoriale Variablen verwendet, die eine begrenzte Anzahl von Merkmalsausprägungen (levels) haben. Ein Factor besteht aus den Merkmalsausprägungen selbst und optionalen Bezeichnungen (labels) für diese Ausprägungen. Factors sind hilfreich, um kategorische Daten in einer strukturierten und sinnvollen Weise zu speichern und zu analysieren.\nIn den kommenden Abschnitten dieses Kapitels werden wir uns ausführlicher mit jedem dieser Datentypen befassen. Wir werden lernen, wie man Daten erfasst, speichert und manipuliert, um aussagekräftige Analysen durchzuführen. Die korrekte Handhabung der Datentypen bildet die Grundlage für jede erfolgreiche Datenanalyse in R."
  },
  {
    "objectID": "Skript_3.2.html#skalenniveaus",
    "href": "Skript_3.2.html#skalenniveaus",
    "title": "Datentypen und -strukturen",
    "section": "1.2 Skalenniveaus",
    "text": "1.2 Skalenniveaus\nBei der quantitativen Datenanalyse ist es von entscheidender Bedeutung, die verschiedenen Skalenniveaus von Variablen zu verstehen. Skalenniveaus geben an, wie die Werte einer Variable gemessen werden und welchen Interpretationsraum sie besitzen. Die Kenntnis dieser Skalenniveaus hilft uns dabei, angemessene statistische Methoden anzuwenden und aussagekräftige Schlussfolgerungen aus unseren Analysen zu ziehen.\n\n1.2.1 Nominales Skalenniveau\nAm niedrigsten in der Hierarchie der Skalenniveaus befindet sich das nominale Skalenniveau. Hierbei werden Werte einer Variable lediglich zur Kategorisierung genutzt, ohne dass eine Reihenfolge oder quantitative Bedeutung besteht. Ein klassisches Beispiel wäre die Kategorisierung von Geschlechtern. Nominalskalierte Variablen können lediglich auf Gleichheit oder Ungleichheit überprüft werden.\n\n\n1.2.2 Ordinales Skalenniveau\nEin Schritt weiter ist das ordinale Skalenniveau. Hierbei können die Werte einer Variable nicht nur kategorisiert werden, sondern es besteht auch eine natürliche Ordnung zwischen den Kategorien. Ein Beispiel hierfür wäre die Bewertung von Produkten auf einer Skala von “schlecht”, “mittel” bis “gut”. Allerdings sind die Abstände zwischen den Kategorien nicht quantitativ interpretierbar.\n\n\n1.2.3 Intervall-Skalenniveau\nDas Intervall-Skalenniveau ermöglicht nicht nur die Kategorisierung und Ordnung von Werten, sondern auch die Quantifizierung von Abständen zwischen den Werten. Bei dieser Skala ist die Differenz zwischen zwei Werten stets konstant, jedoch besitzt der Wert “null” keine inhärente Bedeutung. Ein Beispiel wäre die Temperaturskala in Celsius, bei der eine Temperaturdifferenz von 10 Grad Celsius immer gleichbedeutend ist, aber das Fehlen von Wärme (0 Grad Celsius) nicht bedeutet, dass keine Temperatur vorhanden ist.\n\n\n1.2.4 Verhältnis-Skalenniveau\nDas höchste Skalenniveau ist das Verhältnis-Skalenniveau. Hierbei besitzen die Werte nicht nur eine Ordnung und gleichbleibende Abstände, sondern der Wert “null” hat auch eine klare und inhärente Bedeutung. Dies ermöglicht die Durchführung von mathematischen Operationen wie Multiplikation und Division auf den Werten. Beispiele hierfür sind Gewicht, Größe oder Einkommen.\n\n\n1.2.5 Skalenniveau versus Datentyp\nEs ist wichtig zu beachten, dass die allgemeinen Skalenniveaus nicht eins zu eins den spezifischen Datentypen in R entsprechen. Zum Beispiel können kategoriale Variablen in R als Faktoren dargestellt werden. Diese können das Skalenniveau nominal oder ordinal aufweisen - um welches von den beiden es sich handelt, kann nicht mit R bestimmt werden. Numerische Variablen können ebenfalls unterschiedliche Skalenniveaus haben. Ob eine Variable dabei intervall- oder verhältnisskaliert ist muss durch die Analystin/den Analysten bestimmt werden. Dieser Unterschied zwischen den Konzepten der allgemeinen Skalenniveaus und den Datentypen in R erfordert eine sorgfältige Betrachtung, um sicherzustellen, dass wir unsere Daten angemessen interpretieren und analysieren."
  },
  {
    "objectID": "Skript_3.2.html#datenstrukturen-in-r",
    "href": "Skript_3.2.html#datenstrukturen-in-r",
    "title": "Datentypen und -strukturen",
    "section": "1.3 Datenstrukturen in R",
    "text": "1.3 Datenstrukturen in R\nIn R sind Datenstrukturen essentiell, um Informationen auf organisierte und effiziente Weise zu speichern und zu verarbeiten. In diesem Kapitel werden wir uns mit drei grundlegenden Datenstrukturen vertraut machen: Vektor, Matrix und Datenrahmen.\n\n1.3.1 Der Vektor\nDer Vektor ist die essentiellste und einfachste Datenstruktur in R. Ein Vektor ist eindimensional und enthält eine geordnete Sammlung von Elementen desselben Datentyps, er kann also entweder aus numerischen Werten, Integers (ganzen Zahlen), logischen Werten oder Characters (Textwerten) bestehen. Vektoren sind fundamental für viele Berechnungen und Operationen in R. Vektoren können in R mithilfe der Funktion c() erstellt werden, die die Elemente durch Kommas trennt. Im folgenden Beispiel erstellen wir für verschiedene Datentypen, die wir im vorherigen Kapitel kennengelernt haben, Beispiel-Vektoren und ordnen diesen Vektoren Namen zu.\n\n1a &lt;- c(1,2,3,4,6,-4)\n2b &lt;- c(1L,2L,3L)\n3c &lt;- c(\"one\",\"two\",\"three\")\n4d &lt;- c(TRUE, FALSE, TRUE, FALSE)\n\n\n1\n\nnumeric\n\n2\n\ninteger\n\n3\n\ncharacter\n\n4\n\nlogical\n\n\n\n\nWir haben nun mit der Funktion c() Vektoren erstellt mit verschiedenen Datentypen. Wollen wir einen Vektor des Datentyps numeric erstellen, können wir diesen mit Zahlen befüllen, wie es bei Vektor a der Fall ist. Wollen wir hingegen spezifizieren, dass es sich ausschließlich um ganze Zahlen handelt und der Datentyp des Vektors daher integer ist, müssen wir dies mit einem L hinter den Zahlen angeben, wie wir es für Vektor b gemacht haben. Zur Erstellung eines Vektors des Datentyps character setzen wir die Werte in doppelte oder einfache Anführungszeichen, wie bei Vektor c. Für einen logischen Datentyp, befüllen wir den Vektor mit den Werten TRUE und False, wie in Beispiel d. Wollen wir uns den Datentyp eines Vektors anzeigen lassen, können wir dies mit dem Befehl class() machen.\n\nclass(a)\n\n[1] \"numeric\"\n\nclass(b)\n\n[1] \"integer\"\n\nclass(c)\n\n[1] \"character\"\n\nclass(d)\n\n[1] \"logical\"\n\n\nWollen wir uns bestimmte Werte in einem Vektor anzeigen lassen, können wir diese mit eckigen Klammern in R auswählen. Wollen wir beispielsweise den Wert, der an zweiter Stelle steht im Vektor a, machen wir das wie folgt:\n\na[2]\n\n[1] 2\n\n\nWollen wir den zweiten, dritten und vierten Wert von Vektor a, können wir durch einen Doppelpunkt die Spanne zwischen dem zweiten und vierten Wert angeben:\n\na[2:4]\n\n[1] 2 3 4\n\n\nWollen wir einen Vektor des fünften Datentyps factor erstellen, müssen wir etwas anders vorgehen als bei den anderen. Hier starten wir zunächst mit der Erstellung eines Vektors mit der Funktion c() und wandeln diesen dann anschließend mit der Fuktion factor() zum gewünschten Datentyp um. In folgendem Beispiel erstellen wir einen Vektor, der Daten für die Nutzung von verschiedenen Verkehrsmitteln enthalten soll. Zuerst erstellen wir den numerischen Vektor, der die Verkehrsmittel als Zahlenwerte enthält.\n\nverkehrsmittel &lt;- c(1,2,3,4,2,3,1,2,5,3,2,1,3,2,4,1)\nclass(verkehrsmittel)\n\n[1] \"numeric\"\n\n\nAnschließend können wir den numerischen Vektor in einen des Datentyps factor umwandeln und den verschiedenen Verkehrsmitteln jeweils ein Label zuordnen. Bei der Zuordnung der Labels müssen wir die Reihenfolge beachten: Das erste Label, das wir vergeben, wird dem niedrigsten Zahlenwert (in unserem Fall 1) zugeordnet, das zweite dem zweitniedrigsten (2) und so weiter.\n\nfactor_verkehrsmittel &lt;- factor(verkehrsmittel, labels=c(\"bus\",\"zug\",\"fahrrad\",\"auto\",\"andere\"))\nclass(factor_verkehrsmittel)\n\n[1] \"factor\"\n\n\nLassen wir uns den zuerst erstellten numerischen Vektor verkehrsmittel anzeigen, bekommen wir die numerischen Werte:\n\nverkehrsmittel\n\n [1] 1 2 3 4 2 3 1 2 5 3 2 1 3 2 4 1\n\n\nLassen wir uns hingegen den in einen factor umgewandelten Vektor factor_verkehrsmittel anzeigen, bekommen wir die Werte mit ihren entsprechenden Labeln angezeigt. In der letzten Zeile des Outputs sehen wir außerdem, welche möglichen Merkmalsausprägungen vorkommen können (Levels):\n\nfactor_verkehrsmittel\n\n [1] bus     zug     fahrrad auto    zug     fahrrad bus     zug     andere \n[10] fahrrad zug     bus     fahrrad zug     auto    bus    \nLevels: bus zug fahrrad auto andere\n\n\n\n\n1.3.2 Die Matrix\nNeben der essentiellen Datenstruktur des Vektors gibt es noch weitere Strukturen, die wir kennen müssen, um mit R Daten zu analysieren. Eine davon ist die Matrix, eine zweidimensionale Datenstruktur, die aus Zeilen und Spalten von Elementen besteht. Alle Elemente in einer Matrix müssen denselben Datentyp aufweisen. Matrizen können in R mit der Funktion matrix() erstellt werden. Grundlage hierfür ist wieder zunächst ein Vektor, erstellt mit der c(), der dann mit der Funktion matrix() umgewandelt wird:\n\nvec_1 &lt;- c(1,2,3,4,5,6,7,8,9)\nmatrix_1 &lt;- matrix(vec_1)\nmatrix_1\n\n      [,1]\n [1,]    1\n [2,]    2\n [3,]    3\n [4,]    4\n [5,]    5\n [6,]    6\n [7,]    7\n [8,]    8\n [9,]    9\n\n\nIn diesem Beispiel erstellen wir zunächst mit der Funktion c() einen numerischen Vektor namens vec_1. Diesen wandeln wir mit der Funktion matrix() in eine Matrix um, die wir matrix_1 nennen. Dann lassen wir uns matrix_1 anzeigen. Wir können sehen, dass die erstellte Matrix aus einer Spalte und neun Reihen besteht, die mit den Zahlenwerten des Vektors vec_1 befüllt wurden. Wir können die Anzahl der Spalten und Reihen in der Matrix mit den Argumenten ncol (number of columns = Spaltenanzahl) oder nrow (number of rows = Reihenanzahl) verändern:\n\nmatrix_2 &lt;- matrix(vec_1,ncol=3)\nmatrix_2\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nFür matrix_2 verwenden wir wieder den Vektor vec_1, haben dieses Mal aber angegeben, dass die Matrix drei Spalten (ncol = 3) haben soll. Die Anzahl an Reihen ergibt sich damit automatisch. Anstatt der Spaltenanzahl, können wir aber auch angeben, dass die Matrix drei Reihen haben soll (nrow = 3), in diesem Fall ergibt sich die Anzahl an Spalten automatisch:\n\nmatrix_3 &lt;- matrix(vec_1,nrow=3)\nmatrix_3\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nWir sehen, dass matrix_3 identisch ist mit matrix_2, beide haben jeweils drei Reihen und drei Spalten. Eine Matrix wird also immer befüllt mit allen Werten des Vektors, auf dem sie basiert. Versuchen wir, die Spalten- und Zeilenanzahl so festzulegen, dass die Matrix nicht alle Werte des Vektors enthalten kann - geben wir also eine konfligierende Spalten- und Zeilenanzahl an - erhalten wir eine Warnung:\n\nmatrix_4 &lt;- matrix(vec_1,ncol=3,nrow=2)\nmatrix_4\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n\n1.3.3 Der Datenrahmen\nNeben dem eindimensionalen Vektor und der zweidimensionalen Matrix, gibt es noch eine weitere grundlegende Datenstruktur, mit der wir in der quantitativen Datenanalyse arbeiten: der Datenrahmen. Hierbei handelt es sich um eine zweidimensionale Datenstruktur, die ähnlich einer Tabelle aussieht. Im Gegensatz zur Matrix ermöglicht der Datenrahmen die Speicherung unterschiedlicher Datentypen in verschiedenen Spalten, was besonders hilfreich ist, wenn wir mit realen Datensätzen arbeiten. In einem Datenrahmen repräsentiert eine “Variable” eine einzelne Messgröße oder Eigenschaft. Variablen können verschiedene Datentypen aufweisen (numerische Werte, Texte, logische Werte oder kategoriale Merkmale). Jede Variable wird durch eine Spalte im Datenrahmen dargestellt. Eine “Beobachtung” hingegen entspricht einer einzelnen Einheit, von der wir Daten gesammelt haben. Beobachtungen werden als Zeilen im Datenrahmen angeordnet. Jede Zeile enthält die Datenwerte für jede der zugehörigen Variablen. Beispielsweise könnte eine Beobachtung in einem Datenrahmen zur Erfassung von Studierendendaten Informationen wie den Namen, das Alter und die erreichte Note für einen bestimmten Kurs enthalten. Zusammen repräsentieren Variablen und Beobachtungen die Informationen, die wir in einem Datensatz haben. Der Datenrahmen organisiert diese Informationen in einer tabellarischen Struktur, die es uns ermöglicht, Muster, Trends und Beziehungen zwischen den Variablen und Beobachtungen zu identifizieren. Dies ist der Grundstein quantitative Datenanalysen. Um in R einen Datenrahmen zu erstellen, bilden Vektoren wieder die Grundlage. Diese dienen als Variablen, d.h. als Spalten, in unserem Datenrahmen und werden mit dem Befehl data.frame() zusammengefügt:\n\nname &lt;- c('Esra','Mara','Adam','Luca')\nalter &lt;- c(22, 19, 18, 24)\nnote &lt;- c(1.3, 2.0, 1.7, 2.0)\n\nstudierendendaten &lt;- data.frame(name,alter,note)\n\nIn diesem Beispiel erstellen wir zunächst mit der Funktion c() die Vektoren (Variablen) name, alter und note mit jeweils vier Werten. Anschließend erstellen wir aus diesen einen Datenrahmen mit dem Befehl data.frame(), der die Variablen durch Kommata trennt. Lassen wir uns den erstellten Datenrahmen, den wir studierendendaten genannt haben, anzeigen, sehen wir, dass jede Zeile eine Beobachtung enthält, nämlich den Namen, das Alter und die Note einer Person. Jede Spalte enthält eine der drei Variablen:\n\nstudierendendaten\n\n  name alter note\n1 Esra    22  1.3\n2 Mara    19  2.0\n3 Adam    18  1.7\n4 Luca    24  2.0"
  },
  {
    "objectID": "Skript_3.2.html#zusammenfassung",
    "href": "Skript_3.2.html#zusammenfassung",
    "title": "Datentypen und -strukturen",
    "section": "1.4 Zusammenfassung",
    "text": "1.4 Zusammenfassung\nIn diesem Kapitel haben wir die Grundlagen der Datentypen, Skalenniveaus und Datenstrukturen erkundet, die die Grundlage für unsere quantitative Datenanalyse bilden. Wir haben gelernt, wie unterschiedliche Datentypen uns erlauben, verschiedene Arten von Informationen darzustellen, und wie Skalenniveaus die Bedeutung von Variablenwerten verdeutlichen. Weiterhin haben wir Datenstrukturen wie Vektoren, Matrizen und Datenrahmen kennengelernt, die uns ermöglichen, diese Informationen strukturiert zu speichern und zu verarbeiten. Im folgenden Kapitel werden wir lernen, wie wir Variablen und einzelne Beobachtungen auswählen, manipulieren und transformieren zu können, um sie auf unsere Analysezwecke anzupassen."
  },
  {
    "objectID": "Skript_3.3.html#die-tidyverse-schreibweise-und-der-pipe-operator",
    "href": "Skript_3.3.html#die-tidyverse-schreibweise-und-der-pipe-operator",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.1 Die Tidyverse-Schreibweise und der Pipe-Operator %>%",
    "text": "1.1 Die Tidyverse-Schreibweise und der Pipe-Operator %&gt;%\nIn der tidyverse-Logik verwenden wir eine bestimmte Schreibweise, die es uns ermöglicht, viele Funktionen zu kombinieren und unseren Code trotzdem einfach verständlich und leserlich zu halten. Dabei ist der Pipe-Operator %&gt;% eines der markantesten Merkmale des Tidyverse und dient dazu, komplexe Datenverarbeitungsketten auf eine lesbarere und effizientere Weise zu erstellen. Die Pipe ermöglicht es, die Ergebnisse einer vorherigen Operation als Eingabe für die nächste Operation zu verwenden, ohne explizit Zwischenvariablen erstellen zu müssen. Dies fördert nicht nur die Lesbarkeit des Codes, sondern verringert auch die Wahrscheinlichkeit von Fehlerquellen.\nDie grundlegende Logik der Pipe %&gt;% ist wie folgt:\ndaten %&gt;% operation_1 %&gt;% operation_2 %&gt;% operation_3\nHier ist daten der Ausgangspunkt (Datensatz oder Objekt), auf dem verschiedene Operationen nacheinander ausgeführt werden. Jede Operation erzeugt eine modifizierte Version des vorherigen Objekts, die dann als Eingabe für die nächste Operation verwendet wird. Dies erzeugt eine Kette von Operationen, die von links nach rechts ausgeführt werden."
  },
  {
    "objectID": "Skript_3.3.html#daten-selektion-und-subset-bildung",
    "href": "Skript_3.3.html#daten-selektion-und-subset-bildung",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.2 Daten-Selektion und Subset-Bildung",
    "text": "1.2 Daten-Selektion und Subset-Bildung\nDie Datenanalyse beginnt oft damit, die relevanten Informationen aus einem umfangreichen Datensatz auszuwählen. Hierbei können wir uns auf bestimmte Variablen (Spalten im Datensatz) konzentrieren, die für unsere Fragestellung von Interesse sind. Das “dplyr”-Paket in R bietet uns eine einfache und mächtige Möglichkeit, diese Daten auszuwählen.\n\n1.2.1 Auswahl von Spalten\nDie Funktion select() ermöglicht es uns, Spalten basierend auf ihren Namen auszuwählen. Dies ist besonders hilfreich, wenn wir nur bestimmte Aspekte unserer Daten benötigen. Wollen wir beispielsweise aus unserem ALLBUS-Datensatz ein Subset ziehen, dass nur das Alter der Befragten enthält und die Variablen, die mit Vertrauen in Institutionen zu tun haben, können wir dies wie folgt machen:\n\nvertrauen_institutionen &lt;- daten %&gt;% \n  select(age, pt01, pt02, pt03, pt04, pt06, pt07, pt08, pt09, pt10, pt11, pt12, pt14, pt15, pt19, pt20)\n\nIn diesem Beispiel generieren wir einen neuen Datenrahmen, dem wir den Namen vertrauen_institutionen zuordnen. Innerhalb der Funktion select() geben wir zuerst den Datensatz an, aus dem wir Variablen auswählen wollen. Dieser heißt in unserem Fall daten. Darauf folgt nach einem Komma die Auflistung jener Variablen anhand ihrer Namen im Datensatz, die wir auswählen wollen. Ein Blick in das Codebook des ALLBUS-Datensatzes zeigt uns, welche Variablen mit dem Vertrauen in Institutionen zu tun haben (STRG+F zur Suche im Dokument nach “Vertrauen in Institutionen”, Auflistung der Variablen in der Tabelle auf Seite xiii). Mit dem Befehl View(), oder indem wir auf den generierten Datensatz in unserem Global Environment (RStudio-Schaltfläche 3) klicken, können wir uns das Subset ansehen:\n\nView(vertrauen_institutionen)\n\nAuf eine einzelne Variable in einem Datensatz können wir mit dem Dollar-Zeichen $ zugreifen:\n\nvertrauen_institutionen$age \n\n&lt;labelled&lt;double&gt;[5342]&gt;: ALTER: BEFRAGTE(R)\n  [1]  54  53  89  79  62  23  31  57  68  51  57  85  55  26  38  58  54  45\n [19]  49  26  83  48  48  73  62  25  54  54  51  60  49  57  58  58  39  82\n [37]  77  79  22  77  54  50  23  25  65  56  72  52  68  55  31  79  62  67\n [55]  66  23  83  62  41  57  22  38  69  62  48  64  26  73  49  38  40 -32\n [73]  50  57  42  55  31  55  68  91  63  56  77  56  30  58  60  59  51  25\n [91]  62  59  65  50  36  25  44  59  46  44\n [ reached getOption(\"max.print\") -- omitted 5242 entries ]\n\nLabels:\n value             label\n   -32 NICHT GENERIERBAR\n\n\n\n\n1.2.2 Filtern von Fällen\nNeben der Auswahl von Spalten ist es oft auch erforderlich, Fälle bzw. Zeilen basierend auf bestimmten Bedingungen auszuwählen. Hierfür verwenden wir die Funktion filter(). Angenommen wir möchten für unser Subset vertrauen_institutionen nur die Daten der Personen auswählen, die sehr großes Vertrauen in das Gesundheitswesen (Variable pt01) haben, können wir dies wie folgt machen - aus dem Codebook können wir ableiten, dass “sehr großes Vertrauen” dem Zahlenwert 7 entspricht:\n\nvertrauen_institutionen_filter1 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7)\n\nIn diesem Beispiel haben wir aus dem Subset vertrauen_institutionen erneut ein Subset gezogen, indem wir mit der Funktion filter() nur die Zeilen aus dem ursprünglichen Subset ausgewählt haben, in denen die Variable pt01 (Vertrauen in das Gesundheitswesen) den Wert 7 annimmt. Hier ist es wichtig, ein doppeltes Gleichheitszeichen zu verwenden: pt01 == 7. Wir können auch mehrere Filter gleichzeitig anwenden. Zum Beispiel können wir ein Subset ziehen, dass nur die Daten von Personen enthält, die sehr großes Vertrauen in das Gesundheitswesen haben und gar kein Vertrauen in das Bundesverfassungsgericht (pt02):\n\nvertrauen_institutionen_filter2 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7 & pt02 == 1)\n\nMüssen beide Konditionen zutreffen, damit eine Zeile ausgewählt wird, verbinden wir diese mit dem und-Zeichen &. In unserem Global Environment sehen wir, dass dies auf vier Fälle (4 obs.) zutrifft. Wollen wir hingegen alle Fälle auswählen, bei denen entweder ein sehr großes Vertrauen in das Gesundheitswesen oder gar kein Vertrauen in das Bundesverfassungsgericht besteht, trennen wir durch den senkrechten Strich |:\n\nvertrauen_institutionen_filter3 &lt;- vertrauen_institutionen %&gt;% \n  filter(pt01 == 7 | pt02 == 1)\n\nIn unserem Global Environment sehen wir, dass dies auf 486 Fälle (486 obs.) zutrifft."
  },
  {
    "objectID": "Skript_3.3.html#manipulation-und-transformation-von-daten",
    "href": "Skript_3.3.html#manipulation-und-transformation-von-daten",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.3 Manipulation und Transformation von Daten",
    "text": "1.3 Manipulation und Transformation von Daten\nIn vielen Datenanalyseprojekten ist es notwendig, Variablen zu bearbeiten, um sie für Analysen oder Visualisierungen vorzubereiten. Dieser Prozess kann das Recodieren von Werten, das Umbenennen von Variablen und die Berechnung neuer Variablen umfassen. Das “dplyr”-Paket ermöglicht uns, diese Aufgaben auf eine intuitive Weise zu erledigen. Zum Beispiel verwenden wir die Funktion mutate() für die Transformation von Variablen und das Erstellen neuer Spalten.\n\n1.3.1 Neue Spalten erstellen\nAngenommen wir möchten eine neue Variable (Spalte) für unser Subset vertrauen_institutionen erstellen, die den Wert “großes vertrauen” annehmen soll, wenn die Variable pt01 (Vertrauen in das Gesundheitswesen) einen Wert größer vier (&gt; 4) annimmt und “wenig vertrauen”, wenn pt01 einen anderen Wert annimmt, können wir dies folgermaßen erreichen:\n\ntransformierte_daten &lt;- vertrauen_institutionen %&gt;% \n  mutate(vertrauen_gesundheitswesen = ifelse(pt01 &gt; 4, \"großes vertrauen\", \"wenig vertrauen\"))\n\nIn diesem Beispiel haben wir einen neuen Datensatz generiert und ihm den Namen transformierte_daten zugewiesen. Mit den Funktionen mutate() und ifelse() haben wir basierend auf der Variable pt01 eine neue Spalte im Datensatz erstellt mit dem Namen vertrauen_gesundheitswesen. Innerhalb der Funktion mutate() haben wir dafür zunächst unseren Datensatz angegeben, in diesem Fall vertrauen_institutionen. Nach einem Komma folgt der Name der neuen Variable (Spalte) mit einem Gleichheitszeichen =. In der Funktion ifelse() geben wir erst die Kondition an, die wir testen wollen (pt01 &gt; 4), gefolgt von einem Komma und dem Wert, den die Variable annehmen soll, sofern die getestete Kondition zutrifft. Dann folgt nach einem erneuten Komma der Wert, den die Variable annehmen soll, falls die Kondition nicht zutrifft.\n\n\n1.3.2 Variablen umbenennen\nManchmal ist es sinnvoll, Variablennamen zu ändern, um sie verständlicher zu machen oder um Konventionen zu folgen. Die Funktion rename() ermöglicht das Umbenennen von Variablen:\n\ntransformierte_daten &lt;- transformierte_daten %&gt;%\n  rename(vertrauen_gesundheit = pt01)\n\nHier ändern wir den Namen der Variable pt01 im Datensatz transformierte_daten zu vertrauen_gesundheit."
  },
  {
    "objectID": "Skript_3.3.html#gruppieren-von-daten-und-aggregation",
    "href": "Skript_3.3.html#gruppieren-von-daten-und-aggregation",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.4 Gruppieren von Daten und Aggregation",
    "text": "1.4 Gruppieren von Daten und Aggregation\nEine weitere wichtige Fähigkeit ist das Gruppieren von Fällen basierend auf bestimmten Kategorien. Die Funktion group_by() aus dem dplyr-Paket ermöglicht es, Datensätze nach bestimmten Variablen zu gruppieren. Zum Beispiel könnten wir den Datensatz vertrauen_institutionen nach dem Alter der Befragten gruppieren und das durchschnittliche Vertrauen in das Gesundheitswesen pro Alter berechnen. Wollen wir den Datensatz vertrauen_institutionen nach dem Alter der Befragten gruppieren und das durchschnittliche Vertrauen in das Gesundheitswesen pro Alter berechnen, kombinieren wir die Befehle group_by(), summarize() und mean() nach der tidyverse-Logik wie folgt:\n\naggregierte_daten &lt;- transformierte_daten %&gt;%\n  group_by(age) %&gt;%\n  summarize(durchschnitt_vertrauen = mean(vertrauen_gesundheit))\n\nSehen wir uns den Datensatz aggregierte_daten mit View() oder über unser Global Enviroment an, können wir feststellen, dass dieser aus zwei Variablen (Spalten) besteht: age und durchschnitt_vertrauen. Die Spalte age enthält für jedes Alter, das im Datensatz transformierte_daten vorkommt, eine Zeile. Die Spalte durchschnitt_vertrauen gibt den durchschnittlichen Wert für die Variable vertrauen_gesundheit für jedes Alter an. Den Durchschnitt haben wir mit der Funktion mean() berechnet."
  },
  {
    "objectID": "Skript_3.3.html#umgang-mit-realen-datensätzen-und-fehlenden-werten",
    "href": "Skript_3.3.html#umgang-mit-realen-datensätzen-und-fehlenden-werten",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.5 Umgang mit realen Datensätzen und fehlenden Werten",
    "text": "1.5 Umgang mit realen Datensätzen und fehlenden Werten\nBei der Arbeit mit realen Datensätzen ist es häufig der Fall, dass diese nicht perfekt und sauber sind. Es können verschiedene Probleme auftreten, wie fehlende Daten, inkonsistente Codierungen oder unerwartete Werte. In diesem Abschnitt werden wir uns damit beschäftigen, wie Sie solche Herausforderungen bewältigen können.\nFehlende Daten sind ein häufiges Problem in Datensätzen. In R werden fehlende Werte oft mit dem Wert NA (Not Available) oder NaN (Not a Number) dargestellt. Ist dies der Fall, können wir fehlende Werte mit der Funktion drop_na() entfernen:\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  drop_na(vertrauen_gesundheit)\n\nJe nach Datensatz kann die Codierung fehlender Werte aber stark variieren. Im ALLBUS-Datensatz ist die Variable pt01 (vertrauen_gesundheit) zum Beispiel so codiert, dass fehlende Werte mit einer -9 (keine Angabe), -42 (Datenfehler: Mehrfachnennung) oder -11 (keine Teilnhame an Split A oder B) gekennzeichnet sind. Dadurch wurden durch obenstehenden Code, der drop_na verwendet keinerlei Zeilen gelöscht und der Datensatz vertrauen_institutionen_dropna entspricht vertrauen_institutionen. In diesem Fall müssen wir die spezifischen fehlenden Werte angeben, die wir herausfiltern wollen:\n\nmissing_codes &lt;- c(-9, -42, -11)\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  filter(!vertrauen_gesundheit %in% missing_codes)\n\nHier geben wir zunächst die möglichen Ausprägungen für fehlende Werte an und speichern diese als Vektor missing_codes. Dann wenden wir die Funktion filter() auf den Datensatz vertrauen_institutionen so an, dass nur Zeilen ausgewählt werden, in denen die Variable keinen der Werte in missing_codes annimmt. Das Gegenteil einer Kondition errreichen wir mit einem Ausrufezeichen !. vertrauen_gesundheit %in% missing_code würde alle Fälle filtern, die einen der Werte in missing_codes annehmen. Setzen wir ein ! davor, erhalten wir genau den umgekehrten Fall.\nWir können auch die fehlenden Werte aus allen Vertrauensvariablen löschen:\n\nvertrauen_institutionen_dropna &lt;- transformierte_daten %&gt;%\n  filter(across(vertrauen_gesundheit:pt20, ~ !. %in% missing_codes))\n\nIn dieser Version wird die Funktion across() verwendet, um Operationen auf mehreren Spalten gleichzeitig durchzuführen. Es werden nur die Zeilen beibehalten, in denen keine fehlenden Werte (definiert in missing_codes) in den angegebenen Variablen zwischen vertrauen_gesundheit und pt20 vorkommen."
  },
  {
    "objectID": "Skript_3.3.html#erstellung-von-indizes",
    "href": "Skript_3.3.html#erstellung-von-indizes",
    "title": "Selektion, Manipulation und Transformation",
    "section": "1.6 Erstellung von Indizes",
    "text": "1.6 Erstellung von Indizes\nOft müssen aus vorhandenen Variablen neue abgeleitete Variablen berechnet werden. Dies kann beispielsweise das Berechnen von Indizes oder Skalen sein, um Zusammenfassungen oder Vergleiche zu erleichtern. Wir können zum Beispiel alle Vertrauensvariablen zu einem Index zusammenfassen:\n\nvertrauen_institutionen_index &lt;- vertrauen_institutionen_dropna %&gt;%\n  mutate(index_vertrauen = rowSums(across(vertrauen_gesundheit:pt20)))\n\nHier haben wir eine neue Spalte index_vertrauen erstellt, die die Summe der Vertrauensvariablen enthält. Wir verwenden wieder die Funktion across(), um Operationen auf mehreren Spalten gleichzeitig durchzuführen. Für jede Zeile haben wir mit dem Befehl rowSums() die Werte für alle Variablen von vertrauen_gesundheit bis pt20 addiert und den resultierenden Wert in die neue Spalte geschrieben. Anstatt der Summe können wir auch den Mittelwert der Variablen berechnen und diesen als Index verwenden, dazu teilen wir die Summe, die wir wie im Beispiel zuvor berechnen, durch die Anzahl an Vertrauensvariablen (20):\n\nvertrauen_institutionen_index_2 &lt;- vertrauen_institutionen_dropna %&gt;%\n  mutate(index_vertrauen = rowSums(across(vertrauen_gesundheit:pt20))/20)"
  },
  {
    "objectID": "Skript_4.1.html#das-skalenniveau",
    "href": "Skript_4.1.html#das-skalenniveau",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Durch eine Messung wird bestimmten Eigenschaften eines Merkmalsträgers ein bestimmter Wert zugeordnet. Dazu bedient man sich einer festgelegten Skala, damit alle Ergebnisse miteinander vergleichbar sind. Wir kennen solche Skalen aus unserem Alltag. Manche bestehen aus Zahlen (z.B. Maßbänder, Uhren), andere aus Symbolen (z.B. Vereinslogos). Das Niveau einer Skala hängt von ihrem Informationsgehalt ab. Je höher das Skalenniveau, desto vielfältiger ist die Auswahl an Methoden, die man auf die Daten anwenden kann und desto höher ist ihr Informationsgehalt. Allerdings neigen Methoden, die höhere Skalenniveaus erfordern, oft dazu, anfälliger gegenüber Ausreißern zu sein. Bei der Einschätzung des Skalenniveaus ist es hilfreich zu prüfen, ob ein Merkmal diskret oder kontinuierlich ist. Diskret ist ein Merkmal, wenn es abzählbar viele Ausprägungen annehmen kann, z.B. die Anzahl an Haustieren: Unter 2 Katzen kann sich jeder was vorstellen, bei 1,5 Katzen wird es hingegen schwierig. Dagegen gibt es bei einem kontinuierlichen Merkmal zwischen zwei Ausprägungen unendlich viele weitere, z.B. bei der Körpergröße: Hier sind nicht nur 1, 2 oder 199 cm denkbar, sondern auch jeder beliebig genaue weitere Wert dazwischen. Dafür macht eine Körpergröße von -5 cm wieder keinen Sinn.\n\n\nEs kann nur eine Aussage darüber gemacht werden, ob zwei Objekte gleich sind, aber es gibt keine sinnvolle hierarchische Einteilung, d.h. es kann keine Rangfolge im Sinne von “x ist größer/kleiner als y” erstellt werden. Beispiele: Geschlecht, Nationalität. Diese Skala setzt diskrete Merkmale voraus.\n\n\n\nEs kann nicht nur festgestellt werden, ob sich zwei Objekte gleichen, sondern man kann ihnen Eigenschaften zuordnen, anhand derer sie sich in eine bestimmte Reihenfolge bringen lassen. Sie können also auch sortiert werden. Das kann sowohl diskrete Messwerte beinhalten, die nur nach Rängen geordnet werden können, als auch kontinuierliche Messwerte, bei denen man die Differenz zwischen zwei Werten genau bestimmen kann. Beispiele: Schulnoten,\n\n\n\n\nIntervall: Man kann die Abstände zwischen zwei Messwerten genau berechnen. Es gibt allerdings keinen natürlichen Nullpunkt. Beispiele: Temperatur in Grad Celsius (willkürlich festgelegter Nullpunkt).\nVerhältnis: Wie die Intervallskala, aber es gibt einen natürlichen Nullpunkt. Beispiele: Distanz zwischen zwei Orten, Körpergröße."
  },
  {
    "objectID": "Skript_4.1.html#maßzahlen",
    "href": "Skript_4.1.html#maßzahlen",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Wir werden hier, abhängig vom Skalenniveau, zwei verschiedene Arten von Maßzahlen betrachten. Das ist zum einen das Lagemaß, das uns etwas über die zentrale Tendenz der Daten sagt, also wo sich besonders viele Messwerte häufen bzw. wo der zentrale Punkt ist, um den sich die Messwerte gruppieren. Zum anderen schauen wir uns verschiedene Streuungsparameter an, mit deren Hilfe wir einschätzen können, wie stark unsere Messwerte vom Lageparameter abweichen."
  },
  {
    "objectID": "Skript_4.1.html#häufigkeiten",
    "href": "Skript_4.1.html#häufigkeiten",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Zur Beschreibung insbesondere nominaler Merkmale ist der Begriff der Häufigkeit wichtig.\n\n\nDie absolute Häufigkeit gibt an, wie oft eine bestimmte Merkmalsausprägung im Datensatz vorkommt. Die absolute Häufigkeit kann folglich nur eine natürliche Zahl sein.\n\n\n\nDie relative Häufigkeit gibt den Anteil eines bestimmten Messwertes im Datensatz an. Sie berechnet sich, indem man die absolute Häufigkeit des jeweiligen Messwertes durch die Gesamtgröße des Datensatzes teilt. Die relativen Häufigkeiten summieren sich also zu 1 auf."
  },
  {
    "objectID": "Skript_4.1.html#zu-den-programmierbeispielen",
    "href": "Skript_4.1.html#zu-den-programmierbeispielen",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Im Folgenden werden wir aus dem Allbus-2021-Datensatz ein paar Beispiele herausgreifen, um die Berechnung und Visualisierung von Häufigkeiten und Parametern zu demonstrieren. Dazu installieren und laden wir zunächst die nötigen Pakete mit Hilfe von Pacman und dem p_load-Befehl:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(tidyverse, ggplot2, haven, dplyr) \n\nDann legen wir den Visualisierungshintergrund fest:\n\ntheme_set(theme_classic()) \n\nNun laden wir den Allbus-Datensatz:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\nBei den Häufigkeiten beschränken wir uns auf einen nominal skalierten Datensatz: Das Geschlecht (“sex”).\nAnschließend konvertieren wir die Daten zu Zahlenwerten und entfernen fehlerhafte Daten:\n\nallbus_messniveau_bsp &lt;- subset(daten, select=c(\"sex\")) %&gt;%\n  mutate(across(c(\"sex\"), ~ as.numeric(.))) %&gt;%\n  mutate(across(c(\"sex\"), ~ ifelse(.%in% c(-7, -9, -11, -15, -42, -50 ), NA,.))) %&gt;%\n  na.omit()"
  },
  {
    "objectID": "Skript_4.1.html#berechnung-und-visualisierung-von-häufigkeiten",
    "href": "Skript_4.1.html#berechnung-und-visualisierung-von-häufigkeiten",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "Die absoluten Häufigkeiten lassen sich einfach mit der table-Funktion abfragen. Im folgenden Codebeispiel schauen wir uns dazu die Häufigkeiten im Datensatz “sex” an. Uns werden zwei Zeilen ausgegeben: Die erste Zeile enthält den Tabellenkopf (1: männlich, 2: weiblich, 3: divers). Die zweite Zeile enthält zu jeder Merkmalsausprägung die zugehörige Anzahl (= absolute Häufigkeit):\n\nabs_freq_sex &lt;- table(allbus_messniveau_bsp$sex) \nlength_sex &lt;- length(allbus_messniveau_bsp$sex)\n\nabs_freq_sex        # Ausgabe der Tabelle\n\n\n   1    2    3 \n2614 2705    3 \n\n\nInsgesamt summieren sie sich (wie erwartet) zu 5322 Merkmalsträgern auf, was auch der Größe des Datensatzes entspricht:\n\nsum(abs_freq_sex)   # Ausgabe der Summe der absoluten Häufigkeiten\n\n[1] 5322\n\nlength_sex          # Ausgabe der Gesamtanzahl an Merkmalsträgern im Datensatz\n\n[1] 5322\n\n\nDie relativen Häufigkeiten können wir uns ausgeben lassen, indem wir den Inhalt der Tabelle durch die Gesamtanzahl der Merkmalsträger teilen:\n\nrel_freq_sex &lt;- abs_freq_sex / length_sex # Berechnung der relativen Häufigkeiten\nrel_freq_sex        # Ausgabe der Tabelle mit den relativen Häufigkeiten\n\n\n           1            2            3 \n0.4911687336 0.5082675686 0.0005636979 \n\n\nErwartungsgemäß summieren sich die relativen Häufigkeiten zu 1 auf:\n\nsum(rel_freq_sex) # Ausgabe der Summe der relativen Häufigkeiten\n\n[1] 1\n\n\nEine Möglichkeit der Darstellung von Häufigkeiten ist das Kuchendiagramm. Zu diesem Zweck erstellen wir einen eigenen Dataframe:\n\ndf.sex &lt;- data.frame(\n  sex=c(\"male\", \"female\", \"diverse\"), \n  frequency=abs_freq_sex\n)\n\nggplot(df.sex, aes(x=\"\", y=frequency.Freq, fill=sex)) +\n  geom_bar(stat=\"identity\") +\n  coord_polar(\"y\")\n\n\n\n\nAuf den ersten Blick lässt sich so ein generelles Bild des Datensatzes machen: Es sind in etwa so viele Männer wie Frauen im Datensatz, wobei Frauen etwas stärker vertreten sind. Personen, die sich weder als Mann noch als Frau identifizieren, kommen nur in sehr geringer Zahl vor."
  },
  {
    "objectID": "Skript_4.2.html#der-modus",
    "href": "Skript_4.2.html#der-modus",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1 Der Modus",
    "text": "1 Der Modus\nDer Modus, auch Modalwert genannt, gibt an, welche Ausprägung eines gemessenen Merkmals am häufigsten vorkommt.\n\n1.1 Berechnung\nWir müssen einfach nur zählen, wie oft jede Merkmalsausprägung vorkommt. Diejenige mit dem höchsten Wert (bzw. der größten absoluten Häufigkeit) ist der Modus. In R gibt es keine vorgefertigte Funktion, die diesen Parameter berechnet. Das folgende Code-Beispiel zeigt eine mögliche Lösung speziell für unseren “allbus.df$Geschlecht”-DataFrame.\n\nget.mode &lt;- function(vector){\n\n  # Häufigkeitstabelle erstellen:\n  frequencies = table(vector) \n  \n  # Höhe der größten Häufigkeit ermitteln:\n  max.freq &lt;- max(frequencies)  \n  \n  # Teiltabelle erstellen, die nur die Spalten mit der höchsten Häufigkeit enthält:\n  where.max &lt;-frequencies == max.freq \n  \n  # Namen der verbliebenen Spalten (= Modus) ermitteln:\n  modus &lt;- names(frequencies[where.max]) \n  return(modus)\n}\n#Ausgabe des Modus:\ncat(\"Der Modus lautet \", get.mode(allbus_df$Geschlecht), \".\")\n\nDer Modus lautet  1 ."
  },
  {
    "objectID": "Skript_4.2.html#der-median",
    "href": "Skript_4.2.html#der-median",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2 Der Median",
    "text": "2 Der Median\nFür mindestens ordinal skalierte Messwerte empfielt sich neben dem Modus zusätzlich der Median. Einen der Größe nach aufsteigend sortierten Datensatz teilt der Median genau in der Mitte, es liegen also genauso viele Elemente links wie rechts davon.\n\n2.1 Berechnung\nFür den Fall, dass die Anzahl an Elementen im Datensatz \\(n\\) ungerade ist, entspricht der Median dem Messwert, der genau in der Mitte liegt:\n\\[\nx_{Med} = x_{\\frac{n + 1}{2}}\n\\]\nIst \\(n\\) gerade, kann jedes der beiden Elemente, die in der Mitte liegen, als Median verwendet werden. Es ist aber eher üblich, beide zu addieren und dann durch zwei zu teilen:\n\\[\nx_{Med} = \\frac{1}{2} (x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})\n\\]\nR stellt eine Funktion zur Berechnung des Medians bereit. Wir schauen uns als Beispiel das Vertrauen der Allbus-Befragten in die Bundesregierung an: Im Gegensatz zum Geschlecht können wir hier eine Rangfolge festlegen, jedoch nicht die Abstände dazwischen exakt messen. Wir haben es folglich mit ordinalen Daten zu tun.\nWir verschaffen uns zunächst wieder einen Überblick mit der table-Funktion und sehen sieben verschiedene Werte.\n\ntable(allbus_df$VertrauenBR)\n\n\n  1   2   3   4   5   6   7 \n 68  79 120 169 175 155  32 \n\n\nDie Daten sind bereits von “gar nicht” zu “sehr hoch” sortiert. Wir wenden die median-Funktion an und erhalten “4” als Ausgabe:\n\nmedian.vertrauen &lt;- median(allbus_df$VertrauenBR)\n\n\n\n2.2 Visualisierung\nWenn man sich die Daten als Säulendiagramm bzw. Barplot ausgeben lässt und den Median einzeichnet (im folgenden Codebeispiel die rote gestrichelte Linie), kann man erahnen, dass der Median die sieben Klassen so teilt, dass beidseitig gleich viele abgegebene Stimmen liegen:\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = median.vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#quantile",
    "href": "Skript_4.2.html#quantile",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3 Quantile",
    "text": "3 Quantile\nEin Quantil legt fest, wie viele Werte über bzw. unter einer bestimmten Grenze liegen und teilt den Datensatz damit in zwei Teile. Den bekanntesten Spezialfall haben wir mit dem Median bereits kennengelernt. Die Grenze lag in dem Fall genau in der Mitte, es liegen also 50% unterhalb der Grenze und 50% darüber. Bei einem 31%-Quantil würden hingegen 31% der Werte unter der Grenze liegen und 69% darüber. Wichtige Quantile sind die sogenannten Quartile, zu denen das 25%-Quantil, der Median und das 75%-Quantil zusammengefasst werden. Sie teilen die Gesamtmenge an Messwererten in vier gleich große Teile.\n\n3.1 Berechnung\nDas folgende R-Beispiel gibt die drei Quartile des Vertrauens-Datensatzes aus:\n\nquartile &lt;- quantile(allbus_df$VertrauenBR, probs = c(0.25, 0.5, 0.75))\nquartile\n\n25% 50% 75% \n  3   4   5 \n\n\n\n\n3.2 Visualisierung\nDas folgende Codebeispiel zeichnet neben dem Median (rot) auch das 25%-Quantil (blau) und das 75%-Quantil (grün) in das Säulendiagramm ein:\n\nquantile25 &lt;- quartile[\"25%\"]  #quantile(allbus_df$VertrauenBR, probs=c(0.25)) \nquantile75 &lt;- quartile[\"75%\"]\n\n\nggplot(data=allbus_df, aes(x=VertrauenBR)) +\n  geom_bar() + \n  labs(x=\"Vertrauen in die Bundesregierung\", y=\"Häufigkeit\") + \n  geom_vline(xintercept = quantile25, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.vertrauen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quantile75, color = \"green\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#der-mittelwert",
    "href": "Skript_4.2.html#der-mittelwert",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "4 Der Mittelwert",
    "text": "4 Der Mittelwert\nDer Begriff “Mittelwert” ist etwas ungenau, da es mehrere verschiedene Mittelwerte gibt. Oft ist damit das arithmetische Mittel gemeint. Es lässt sich nur bei mindestens kardinal skalierten Daten anwenden und bezieht die Gewichte der jeweiligen Merkmalsausprägungen mit ein.\n\n4.1 Berechnung\nDas arithmetische Mittel erhält man, indem man alle Messwerte addiert und durch die Gesamtzahl an Messwerten teilt:\n\\[\n\\bar{x} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} = \\frac{1}{n}  \\sum_{i=1}^{n} {x_{i}}\n\\] Schauen wir uns das am Beispiel des Netto-Einkommens der Befragten im Allbus-Datensatz an: Wir können dazu die bereits vorhandene Funktion mean nutzen:\n\nmean.einkommen &lt;- mean(allbus_df$Einkommen)\nmean.einkommen\n\n[1] 2445.346\n\n\n\n\n4.2 Visualisierung\nVergleichen wir das mit dem Median, fällt auf, dass zwischen beiden Lageparametern über 200 Euro Differenz bestehen:\n\nmedian.einkommen &lt;- median(allbus_df$Einkommen)\nmedian.einkommen\n\n[1] 2200\n\n\nDer Modus liegt noch weiter weg:\n\nmodus.einkommen &lt;- get.mode(allbus_df$Einkommen)\nmodus.einkommen\n\n[1] \"3000\"\n\n\nDas liegt daran, dass die Einkommensdaten kontinuierlich sind und es keinen homogenen An- und Abstieg der Häufigkeitsverteilung gibt. Der Einkommenswert, den am meisten Personen exakt gleich angegeben haben, ist deshalb wenig aussagekräftig und der Modus macht nur Sinn, nachdem man die Daten in Form von Einkommensklassen diskretisiert hat.\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =..density..),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Netto-Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus.einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1) \n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus.einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-3",
    "href": "Skript_4.2.html#berechnung-3",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "5 Berechnung",
    "text": "5 Berechnung\nDas arithmetische Mittel erhält man, indem man alle Messwerte addiert und durch die Gesamtzahl an Messwerten teilt:\n\\[\n\\bar{x} = \\frac{x_{1} + x_{2} + \\cdots + x_{n}}{n} = \\frac{1}{n}  \\sum_{i=1}^{n} {x_{i}}\n\\] Schauen wir uns das am Beispiel des Netto-Einkommens der Befragten im Allbus-Datensatz an: Wir können dazu die bereits vorhandene Funktion mean nutzen:\n\nmean(allbus_messniveau_bsp$di01a)\n\n[1] 2445.346\n\n\nVergleichen wir das mit dem Median, fällt auf, dass zwischen beiden Lageparametern ca. 100 Euro Differenz bestehen:\n\nmedian(allbus_messniveau_bsp$di01a)\n\n[1] 2200\n\n\n\nplot(sort(allbus_messniveau_bsp$di01a))\nabline(h = median(allbus_messniveau_bsp$di01a), col = 'red')\nabline(h = mean(allbus_messniveau_bsp$di01a), col = 'orange')\n\n\n\n\n\nhist(allbus_messniveau_bsp$di01a, )\n\n\n\n\n\nggplot(allbus_messniveau_bsp, aes(x=di01a)) + geom_histogram(binwidth=100)\n\n\n\n\nNicht unerwähnt bleiben sollte das geometrische Mittel, das bei der Berechnung des Mittelwerts von prozentualen Veränderungen angewendet wird. Dabei werden die einzelnen Messwerte multipliziert und die n-te Wurzel aus dem Ergebnis gezogen, wobei n die Gesamtzahl an Messwerten ist:\n\\[\nx_{Geom} = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}\n\\]"
  },
  {
    "objectID": "Skript_4.3.html#die-spannweite",
    "href": "Skript_4.3.html#die-spannweite",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "1 Die Spannweite",
    "text": "1 Die Spannweite\nDie Spannweite (auch Range genannt) gibt den Abstand zwischen der kleinsten und der größten Merkmalsausprägung an. Sie ist das einfachste Streuungsmaß, zugleich aber nur wenig aussagekräftig.\n\n1.1 Berechnung\n\\[\nSP = x_{max} - x_{min}  \n\\] Mit der range-Funktion kann man sich in R die beiden Extremwerte als Vektor ausgeben lassen:\n\nrange(allbus_df$Einkommen)\n\n[1]   -41 14100\n\n\nAlternativ kann man auch einfach Das Maximum und das Minimum ermitteln und die Differenz berechnen:\n\neinkommen.max = max(allbus_df$Einkommen) - min(allbus_df$Einkommen)"
  },
  {
    "objectID": "Skript_4.3.html#quantilsabstände",
    "href": "Skript_4.3.html#quantilsabstände",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "2 Quantilsabstände",
    "text": "2 Quantilsabstände\nDieses Maß gibt die Differenz zweier Quantile an. Insbesondere der Quartilsabstand (75%-Quantil - 25%-Quantil) ist hier von Bedeutung.\n\n2.1 Berechnung\nDer Quaantilsabstand berechnet sich für das obere Quantil \\(Q_{o}\\) und das untere Quantil \\(Q_{u}\\) folgendermaßen:\n\\[\nQA = Q_{o} - Q_{u}\n\\]\nFür den Quartilsabstand gibt es eine Funktion in R:\n\n# Berechnung mit der vorgefertigten R-Funktion IQR (Interquartile Range):\nIQR(allbus_df$VertrauenBR)\n\n[1] 2\n\n\nUm den mittleren Quartilsabstand zu ermitteln, kann man das Ergebnis noch durch 2 teilen."
  },
  {
    "objectID": "Skript_4.3.html#varianz-und-standardabweichung",
    "href": "Skript_4.3.html#varianz-und-standardabweichung",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "3 Varianz und Standardabweichung",
    "text": "3 Varianz und Standardabweichung\nEin nützlicheres Maß dafür, wie die einzelnen Merkmalsausprägungen um den Mittelwert verteilt sind - vorausgesetzt, man hat es mit kardinal skalierten Daten zu tun, kann die Varianz sein.\n\n3.1 Berechnung\nEs wird unterschieden zwischen der Varianz der Grundgesamtheit und der Stichprobenvarianz. Bei ersterer berechnet man für jede einzelne Merkmalsausprägung ihre Abweichung vom Mittelwert, quadriert die Ergebnisse und summiert diese. Anschließend teilt man durch die Größe der Grundgesamtheit:\n\\[\nVar = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\] Da man wie gesagt meist nur auf eine Stichprobe zurückgreifen kann, kann man den Mittelwert der Grundgesamtheit durch den Mittelwert der Stichprobe nur schätzen. Um die damit verbundene Verzerrung auszugleichen, kann es sinnvoll sein die Summe der quadrierten Abweichungen nicht durch \\(n\\), sondern durch \\((n-1)\\) zu teilen. So ergibt sich folgende Formel, wobei \\(n\\) hier die Stichprobengröße darstellt:\n\\[\nVar = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2\n\\]\nIn R bekommen wir die Varianz mit der var-Funktion, hier am Beispiel des Netto-Einkommens:\n\nvar(allbus_df$Einkommen)\n\n[1] 2593715\n\n\nWie wir sehen, ist das Ergebnis sehr groß und auf den ersten Blick nicht leicht interpretierbar. Nützlicher ist da die Standardabweichung, die wir einfach dadurch erhalten, dass wir die Quadratwurzel der Varianz ziehen:\n\\[\nSD = \\sqrt{Var} = s =  \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\] R bietet zur Berechnung der Standardabweichung die sd-Funktion an.\n\nsd(allbus_df$Einkommen)\n\n[1] 1610.501"
  },
  {
    "objectID": "Skript_4.3.html#zusammenfassungen-ausgeben-lassen",
    "href": "Skript_4.3.html#zusammenfassungen-ausgeben-lassen",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "4 Zusammenfassungen ausgeben lassen",
    "text": "4 Zusammenfassungen ausgeben lassen\nWenn man nicht jeden Parameter einzeln abfragen will und kardinal skalierte Daten hat, kann man sich die wichtigsten Lageparameter auch als Zusammenfassung ausgeben lassen:\n\nsummary(allbus_df$Einkommen)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    -41    1500    2200    2445    3000   14100"
  },
  {
    "objectID": "Skript_4.4.html#die-normalverteilung",
    "href": "Skript_4.4.html#die-normalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "0.1 Die Normalverteilung",
    "text": "0.1 Die Normalverteilung\nDiese Verteilung ist auch ihrer charakteristischen Form wegen als Glockenkurve oder nach ihrem maßgeblichen Entdecker Carl Friedrich Gauß als Gauß-Verteilung bekannt. Relativ viele andere Verteilungen lassen sich bei ausreichend großer Stichprobengröße mit der Normalverteilung approximieren. Sie ist symmetrisch und der Mittelwert ist ihr Maximum.\nCa. 68% der Werte liegen innerhalb einer Standardabweichung vom Mittelwert entfernt, ca. 95% innerhalb von zwei Standardabweichungen und ca. 99,7%, also fast alle, innerhalb von drei Standardabweichungen.\n\n0.1.1 Berechnung\nDie Dichtefunktion der Normalverteilung wird folgendermaßen berechnet:\n\\[\nf(x,\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\] Dabei ist \\(\\mu\\) der Mittelwert (arithm. Mittel, Median, oder Modus) und \\(\\sigma\\) die Standardabweichung.\nBei einer Normalverteilung mit einem Mittelwert von null und einer Varianz von eins spricht man von einer Standardnormalverteilung:\n\\[\n\\phi(z) = f(z, 0, 1) = \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{- \\frac{z^2}{2}}\n\\]\nNormal- aber nicht standardnormal verteilte Werte lassen sich leicht standardisieren, indem man den Mittelwert von ihnen abzieht und das Ergebnis durch die Standardabweichung teilt:\n\\[\nz = \\frac{Messwert - Mittelwert}{Standardabweichung} = \\frac{x - \\mu}{\\sigma}\n\\] \\(\\mu\\) kann dabei das arithmetische Mittel, der Median oder der Modus sein.\nDer Vollständigkeit halber sei hier auch noch die Formel der Verteilungsfunktion aufgeschrieben:\n\\[\nF(x, \\mu, \\sigma) = \\frac{1}{2} \\left[1 + \\text{erf} \\left (\\frac{x - \\mu}{\\sigma \\sqrt{2}} \\right) \\right]\n\\] \\(erf\\) steht für die Gauß’sche Fehlerfunktion.\nDa R umfangreiche Funktionalitäten bereitstellt, um diese Verteilungen zu berechnen, ist es an dieser Stelle nicht notwendig, sich diese Formel einzuprägen oder irgendwas damit per Hand zu rechnen. R hat eine ganze Reihe an Verteilungen implementiert und stellt zu jeder davon u.a. vier Funktionen bereit:\n\ndie Wahrscheinlichkeits-/Dichtefunktion, beginnend mit dem Buchstaben d,\ndie Verteilungsfunktion, beginnend mit p,\nQuantile, beginnend mit q sowie\nZufallszahlen auf Basis der jeweiligen Verteilung, beginnend mit r."
  },
  {
    "objectID": "Skript_4.4.html#binomialverteilung",
    "href": "Skript_4.4.html#binomialverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Binomialverteilung",
    "text": "1.1 Binomialverteilung\nDiese Verteilung basiert auf Zufallsexperimenten, die genau zwei Versuchsausgänge aufweisen, welche sich gegenseitig ausschließen und konstante Wahrscheinlichkeiten für die beiden Ausgänge haben. Die einzelnen Versuche sollen voneinander unabhängig sein.\nEin bekanntes Beispiel für so ein Zufallsexperiment ist der Münzwurf. Anwendungen in der KMW wären etwa Kaufentscheidungen (Ja/Nein) in der Werbewirkungsforschung oder allgemein Interview-Fragen, auf die es nur Ja/Nein-Antworten gibt.\n\n1.1.1 Berechnung\nBei einer Eintrittswahrscheinlichkeit \\(p\\) für ein bestimmtes Ereignis berechnet sich die Wahrscheinlichkeit, dass das Ereignis nach \\(n\\) Wiederholungen \\(k\\) mal eintritt, mit folgender Formel:\n\\[\nf(k;n,p) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nDas folgende Beispiel zeigt, wie man in R die Wahrscheinlichkeit berechnet, bei zehn Münzwürfen genau 7 mal Kopf zu werfen:\n\nn &lt;- 10 # Festlegen der Gesamtzahl an Würfen \nkopf &lt;- 7 # Festlegen der Anzahl, wie oft Kopf geworfen werden soll \np_kopf &lt;- 0.5 # Festlegen der Wahrscheinlichkeit, Kopf zu werfen (hier: 50%)\nprob &lt;- dbinom(kopf, size=n, prob=p_kopf) \nprob\n\n[1] 0.1171875"
  },
  {
    "objectID": "Skript_4.4.html#hypergeometrische-verteilung",
    "href": "Skript_4.4.html#hypergeometrische-verteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.2 Hypergeometrische Verteilung",
    "text": "1.2 Hypergeometrische Verteilung\nDiese Verteilung kann vorliegen, wenn die Bedingung der Unabhängigkeit der einzelnen Versuche nicht einhaltbar ist.\n\n1.2.1 Berechnung\nEs wird wieder von zwei Ereignissen ausgegangen, die eintreten können. Angenommen, eine Grundgesamtheit setzt sich zusammen aus \\(N + M\\) Ereignissen, wobei \\(M\\) und \\(N\\) sich gegenseitig ausschließen. Aus dieser Grundgesamtheit wird nun eine Stichprobe der Größe \\(k\\) gezogen. Dann berechnet sich die Wahrscheinlichkeit, dass sich in der Stichprobe \\(x\\) mal das Ereignis \\(M\\) eintritt, folgendermaßen: \\[\nf(x, k, M, N) = \\frac{\\binom{M}{x} \\binom{N}{k-x}}{\\binom{N + M}{k}}\n\\]\nMachen wir uns das an einem Beispiel deutlich: In einer Stadt mit ca. 600000 Einwohnern sind ca. 2000 Menschen mit HIV infiziert. Wie groß ist die Wahrscheinlichkeit, dass von 50 Leuten, die man bei einem Ausflug in die Stadt zufällig trifft, kein einziger HIV hat?\n\nn &lt;- 600000 # Grundgesamtheit\nm &lt;- 2000   # Fälle, die eine bestimmte Merkmalsausprägung aufweisen\nk &lt;- 50     # Stichprobengröße\nx &lt;- 0      \nprob &lt;- dhyper(x, m, n, k) \nprob\n\n[1] 0.8467106\n\n\nDas heißt, die Wahrscheinlichkeit, dass mindestens eine Person, der man begegnet, HIV hat, beträgt ca. \\(1 - 0.8467\\), also ca. 15,33 Prozent."
  },
  {
    "objectID": "Skript_4.4.html#poissonverteilung",
    "href": "Skript_4.4.html#poissonverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Poissonverteilung",
    "text": "1.3 Poissonverteilung\nDiese Verteilung kann herangezogen werden, wenn es um das durchschnittliche Eintreten bestimmter Ereignisse innerhalb es festen Zeitintervalls geht. Dabei steht \\(\\lambda\\) für die Rate, mit der ein Ereignis durchschnittlich eintritt.\n\n1.3.1 Berechnung\nDie Poissonverteilung berechnet sich nach folgender Formel: \\[\nf(x;\\lambda) =  \\frac{\\lambda^x e^{-\\lambda} }{x!}\n\\]\nDazu wieder ein Beispiel: Eine Nachrichtenagentur veröffentlicht zu einem bestimmten Thema normalerweise 3 Artikel pro Tag. Wie wahrscheinlich ist es, dass sie an einem Tag 7 Artikel zu demselben Thema veröffentlicht?\n\nx &lt;- 7 \nlambda &lt;- 3 \ndpois(x, lambda)\n\n[1] 0.02160403"
  },
  {
    "objectID": "Skript_4.4.html#gleichverteilung",
    "href": "Skript_4.4.html#gleichverteilung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.4 Gleichverteilung",
    "text": "1.4 Gleichverteilung\nEine Zufallsvariable ist in einem gegebenen Intervall gleichmäßig verteilt. Diese Verteilung ist besonders zur Erzeugung von Zufallszahlen hilfreich.\n\n1.4.1 Berechnung\n\\[\nf(x;min,max) = \\begin{cases}\n\\frac{1}{max-min}, & min \\leq x \\leq max \\\\\n0, & \\text{sonst}\n\\end{cases}\n\\]\n\n# Fünf Zufallszahlen, die im voreingestellten Intervall von 0 bis 1 liegen:\nrunif(5) \n\n[1] 0.9093424 0.1476882 0.2362888 0.9087583 0.5536340\n\n# Drei Zufallszahlen, die im Intervall von -10 bis 10 liegen:\nrunif(3, min=-10, max=10) \n\n[1]  8.0444784 -5.8739108 -0.8097793\n\n\n\n\n1.4.2 Exponentialverteilung\nÄhnlich wie die Poissonverteilung, allerdings stetig.\n\n\n1.4.3 Berechnung\n\\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Beispiel:\nEine Nachrichtenagentur berichtet durchschnittlich alle 30 Tage über Proteste und Widerstandsaktionen in einem Land X. Wie wahrscheinlich ist es, dass zwischen den Nachrichten plötzlich nur noch maximal 5 Tage liegen?\n\nda &lt;- 30 # Durchschnittlicher Abstand in Tagen\nlambda &lt;- 1 / da\nx &lt;- 5 \nprob &lt;- pexp(x, rate=lambda)\nprob\n\n[1] 0.1535183"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-der-streuungsparameter",
    "href": "Skript_4.3.html#visualisierung-der-streuungsparameter",
    "title": "Streuungsmaße: Spannweite, Quartilsabstand, Standartabweichung, Varianz",
    "section": "5 Visualisierung der Streuungsparameter",
    "text": "5 Visualisierung der Streuungsparameter\nIn folgendem Codebeispiel lassen wir uns das Histogramm aus dem letzten Abschnitt ausgeben, allerdings reduziert auf Median (blau, durchgezogene Linie) und das arithmetische Mittel (rot, durchgezogene Linie). Wir ergänzen diese Lageparameter um den Minimal- und den Maximalwert (schwarz, gestrichelt), das untere und obere Quartil (blau, gestrichelt) und die Standardabweichung, hier beiderseits vom Mittelwert aufgetragen(rot, gestrichelt).\n\nmean.einkommen &lt;- mean(allbus_df$Einkommen)\nmedian.einkommen &lt;- median(allbus_df$Einkommen)\nrange.einkommen &lt;- range(allbus_df$Einkommen) \niqr.einkommen &lt;- IQR(allbus_df$Einkommen) \nsd.einkommen &lt;- sd(allbus_df$Einkommen) \n\nquartil25.einkommen &lt;- quantile(allbus_df$Einkommen, 0.25) \nquartil75.einkommen &lt;- quantile(allbus_df$Einkommen, 0.75)\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean.einkommen, color = \"red\", linewidth = 1) +   #, linetype = \"dashed\"\n  geom_vline(xintercept = mean.einkommen - sd.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = mean.einkommen + sd.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.einkommen, color = \"blue\", linewidth = 1) +\n  geom_vline(xintercept = quartil25.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = quartil75.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = range.einkommen[1], color = \"black\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = range.einkommen[2], color = \"black\", linetype = \"dashed\", linewidth = 1) \n\n\n\n\nEs ist sehr deutlich zu erkennen, dass die Spannweite (die Distanz zwischen den beiden schwarzen gestrichelten Linien) wenig hilfreich ist, wenn man erfahren will, wo besonders viele Messwerte liegen. Der Interquartilsabstand scheint hier sehr viel aussagekräftiger zu sein, während die Standardabweichung unterhalb des Mittelwerts mehr Messwerte einzuschließen scheint als oberhalb davon. Dieses Ungleichgewicht wird im Abschnitt über Verteilungen noch eine Rolle spielen."
  },
  {
    "objectID": "Skript_4.2.html#geometrisches-mittel",
    "href": "Skript_4.2.html#geometrisches-mittel",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "5 Geometrisches Mittel",
    "text": "5 Geometrisches Mittel\nNicht unerwähnt bleiben sollte das geometrische Mittel, das bei der Berechnung des Mittelwerts von prozentualen Veränderungen angewendet wird. Dabei werden die einzelnen Messwerte multipliziert und die n-te Wurzel aus dem Ergebnis gezogen, wobei n die Gesamtzahl an Messwerten ist:\n\\[\nx_{Geom} = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}\n\\] ### Berechnung\nIn R gibt es dafür keine eigenständige Funktion, man kann aber die Gleichung umstellen und mit Hilfe einiger anderer eingebauter Funktionen eine simple Alternative erstellen, indem man einen kleinen Trick mit der Exponentialfunktion und dem natürlichen Logarithmus anwendet:\n$$\n\\[\\begin{aligned}\nx_{Geom}  & = (x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}} \\\\\n          & = e^{ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})^{\\frac{1}{n}}} \\\\\n          & = e^{\\frac{1}{n}ln(x_{1} \\cdot x_{2} \\cdot ... \\cdot x_{n})} \\\\\n          & = e^{\\frac{1}{n} \\sum_{i=1}^{n} ln({x_{i})}  }\n\\end{aligned}\\]\n$$\nAuch, wenn die resultierende Formel wenig ansprechend aussieht, kann man bei genauerem Hinsehen das versteckte arithmetische Mittel erkennen und den ganzen Ausdruck in folgenden R-Code umsetzen:\n\ngeom.mean &lt;- function(vector){\n  exp(mean(log(vector)))\n}\n\n\nmin(allbus_df$Einkommen)\n\n[1] -41\n\ngeom.einkommen &lt;- geom.mean(allbus_df$Einkommen + 42) \ngeom.einkommen\n\n[1] 1996.667\n\n\n\nggplot(allbus_df, aes(x = Einkommen)) + \n    geom_histogram(aes(y =after_stat(density)),\n    breaks = seq(-0, 10000, by = 500), \n    colour = \"black\", \n    fill = \"gray\") +\n  labs(x = \"Einkommen\", y = \"Häufigkeit\") +\n  geom_vline(xintercept = mean.einkommen, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median.einkommen, color = \"blue\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = as.numeric(modus.einkommen), color = \"green\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = geom.einkommen, color = \"yellow\", linetype = \"dashed\", linewidth = 1)"
  },
  {
    "objectID": "Skript_4.4.html#lognormalverteilung",
    "href": "Skript_4.4.html#lognormalverteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.1 Lognormalverteilung",
    "text": "1.1 Lognormalverteilung\nWir haben diese Verteilung bereits angesprochen. Für Daten, die einen natürlichen Nullpunkt haben, ist die Lognormalverteilung ein möglicher Kandidat, weil sie nicht symmetrisch ist wie die Normalverteilung. Zur Normalisierung ist sie sehr nützlich. Allerdings sollte man daran denken, dass sie nur mit Werten funktioniert, die größer als Null sind. Man muss also ggf. die Messwerte vorher anpassen.\n\n1.1.1 Berechnung\nDie Formel ähnelt der für die Normalverteilung:\n\\[\nf(x;\\mu,\\sigma) = \\frac{1}{x \\cdot \\sigma \\sqrt{2\\pi}} \\cdot e^{-\\frac{(\\ln(x)-\\mu)^2}{2\\sigma^2}}\n\\] ### Visualisierung\nSchauen wir uns einmal an, wie unsere Einkommensdaten zur Dichtefunktion der Lognormalverteilung passen. Dazu addieren wir zunächst zu jedem Messwert die Zahl 42, damit alle Werte größer Null sind, logarithmieren dann die Messwerte und ermitteln anschließend das arithmetische Mittel und die Standardabweichung:\n\nlog.allbus.df &lt;- data.frame(LogEinkommen=log(allbus_df$Einkommen + 42)) \nlog.mean &lt;- mean(log.allbus.df$LogEinkommen) \nlog.sd &lt;- sd(log.allbus.df$LogEinkommen)\n\ncat(\"Mittelwert: \", log.mean, \"Standardabweichung: \", log.sd)\n\nMittelwert:  7.599235 Standardabweichung:  0.8405187\n\n\nDann plotten wir das Ergebnis in einem Histogramm, in das wir die theoretische Dichte der Lognormalverteilung eintragen, die unseren Parametern entspricht:\n\nggplot(data = allbus_df, aes(x=Einkommen + 42)) +\n  geom_histogram(aes(y=..density..), binwidth = 500, fill = \"gray\", color = \"black\") +\n  labs(\n    title = \"Nettoeinkommen und Lognormalverteilung\", x = \"Messwerte\", y = \"Dichte\") +\nstat_function(fun=dlnorm, args=list(meanlog=log.mean, sdlog=log.sd), colour=\"red\")  #meanlog=log.mean, sdlog=log.sd\n\n\n\n\nDie theoretische Verteilung wirkt schiefer als die empirische, aber insgesamt scheint die Übereinstimmung auch visuell größer zu sein als mit der Normalverteilung."
  },
  {
    "objectID": "Skript_4.4.html#weibull-verteilung",
    "href": "Skript_4.4.html#weibull-verteilung",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.2 Weibull-Verteilung",
    "text": "1.2 Weibull-Verteilung\nDer zweite Kandidat, der sich bei stetigen, asymmetrischen Verteilungen anbieten kann, ist die Weibull-Verteilung. Sie ist sehr vielseitig, bezogen auf die Formen, die sie annehmen kann und wird deshalb oft für die Kalkulation der Lebensdauer von Maschinenteilen u.ä. verwendet. Allerdings kann sie auch bei der Untersuchung von Einkommensungleichheiten von Bedeutung sein.\n\n1.2.1 Berechnung\nDiese Verteilung ist abhängig von einem Streuungsparameter \\(1 / \\lambda\\) und dem Formparameter \\(k\\):\n$$ f(x;,k) = ()^{k-1} e{-(x/)k}\n$$ Durch den Parameter \\(k\\) kann man berücksichtigen, wie sich die Häufigkeit, mit der ein bestimmtes Ereignis auftritt, verändert. Wenn \\(k = 1\\), geht man davon aus, dass sich die Wahrscheinlichkeit, dass ein Ereignis eintritt, kaum verändert. Für \\(k &lt; 1\\) erwartet man, dass Ereignisse über die Zeit seltener auftreten und für \\(k &gt; 1\\), dass sie mit der Zeit zunehmen.\n\n\n1.2.2 Visualisierung\nAuf den Netto-Einkommens-Datensatz angewendet, ergibt sich ein Bild, das wie erwartet ähnlich wie die Lognormal-Verteilung wirkt, wenn auch immer noch mit deutlichen Abweichungen.\n\nfit.weibull &lt;- fitdist(allbus_df$Einkommen + 43, \"weibull\")\nggplot(data = allbus_df, aes(x=Einkommen + 42)) +\n  geom_histogram(aes(y=..density..), binwidth = 500, fill = \"grey\", color = \"black\") +\n  labs(\n    title = \"Netto-Einkommen und Weibull-Verteilung\",\n    x = \"Messwerte\",\n    y = \"Dichte\"\n  ) +\n  stat_function(fun=dweibull, args=list(scale=fit.weibull$estimate[\"scale\"], shape=fit.weibull$estimate[\"shape\"]), colour=\"red\")"
  },
  {
    "objectID": "Skript_4.4.html#weitere-verteilungen",
    "href": "Skript_4.4.html#weitere-verteilungen",
    "title": "Berechnung und Interpretation von Verteilungen",
    "section": "1.3 Weitere Verteilungen",
    "text": "1.3 Weitere Verteilungen\nDamit verlassen wir das konkrete Einkommens-Beispiel und die Suche nach Alternativen zur Normalverteilung und schauen uns ein paar weitere Verteilungen an, die je nach Forschungsdesign ebenfalls wichtig sein können.\n\n1.3.1 Binomialverteilung\nDiese Verteilung basiert auf Zufallsexperimenten, die genau zwei Versuchsausgänge aufweisen, welche sich gegenseitig ausschließen und konstante Wahrscheinlichkeiten für die beiden Ausgänge haben. Die einzelnen Versuche sollen voneinander unabhängig sein.\nEin bekanntes Beispiel für so ein Zufallsexperiment ist der Münzwurf. Anwendungen in der KMW wären etwa Kaufentscheidungen (Ja/Nein) in der Werbewirkungsforschung oder allgemein Interview-Fragen, auf die es nur Ja/Nein-Antworten gibt.\n\n1.3.1.1 Berechnung und Visualisierung\nBei einer Eintrittswahrscheinlichkeit \\(p\\) für ein bestimmtes Ereignis berechnet sich die Wahrscheinlichkeit, dass das Ereignis nach \\(n\\) Wiederholungen \\(k\\) mal eintritt, mit folgender Formel:\n\\[\nf(k;n,p) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nDas folgende Beispiel zeigt, wie man in R die Wahrscheinlichkeit berechnet, bei zehn Münzwürfen genau 7 mal Kopf zu werfen:\n\nn &lt;- 10 # Festlegen der Gesamtzahl an Würfen \nkopf &lt;- 7 # Festlegen der Anzahl, wie oft Kopf geworfen werden soll \np_kopf &lt;- 0.5 # Festlegen der Wahrscheinlichkeit, Kopf zu werfen (hier: 50%)\nprob &lt;- dbinom(kopf, size=n, prob=p_kopf) \nprob\n\n[1] 0.1171875\n\n\n\n\n\n1.3.2 Hypergeometrische Verteilung\nDiese Verteilung kann vorliegen, wenn die Bedingung der Unabhängigkeit der einzelnen Versuche nicht einhaltbar ist.\n\n1.3.2.1 Berechnung\nEs wird wieder von zwei Ereignissen ausgegangen, die eintreten können. Angenommen, eine Grundgesamtheit setzt sich zusammen aus \\(N + M\\) Ereignissen, wobei \\(M\\) und \\(N\\) sich gegenseitig ausschließen. Aus dieser Grundgesamtheit wird nun eine Stichprobe der Größe \\(k\\) gezogen. Dann berechnet sich die Wahrscheinlichkeit, dass sich in der Stichprobe \\(x\\) mal das Ereignis \\(M\\) eintritt, folgendermaßen: \\[\nf(x, k, M, N) = \\frac{\\binom{M}{x} \\binom{N}{k-x}}{\\binom{N + M}{k}}\n\\]\nMachen wir uns das an einem Beispiel deutlich: In einer Stadt mit ca. 600000 Einwohnern sind ca. 2000 Menschen mit HIV infiziert. Wie groß ist die Wahrscheinlichkeit, dass von 50 Leuten, die man bei einem Ausflug in die Stadt zufällig trifft, kein einziger HIV hat?\n\nn &lt;- 600000 # Grundgesamtheit\nm &lt;- 2000   # Fälle, die eine bestimmte Merkmalsausprägung aufweisen\nk &lt;- 50     # Stichprobengröße\nx &lt;- 0      \nprob &lt;- dhyper(x, m, n, k) \nprob\n\n[1] 0.8467106\n\n\nDas heißt, die Wahrscheinlichkeit, dass mindestens eine Person, der man begegnet, HIV hat, beträgt ca. \\(1 - 0.8467\\), also ca. 15,33 Prozent.\n\n\n\n1.3.3 Poissonverteilung\nDiese Verteilung kann herangezogen werden, wenn es um das durchschnittliche Eintreten bestimmter Ereignisse innerhalb es festen Zeitintervalls geht. Dabei steht \\(\\lambda\\) für die Rate, mit der ein Ereignis durchschnittlich eintritt.\n\n1.3.3.1 Berechnung\nDie Poissonverteilung berechnet sich nach folgender Formel: \\[\nf(x;\\lambda) =  \\frac{\\lambda^x e^{-\\lambda} }{x!}\n\\]\nDazu wieder ein Beispiel: Eine Nachrichtenagentur veröffentlicht zu einem bestimmten Thema normalerweise 3 Artikel pro Tag. Wie wahrscheinlich ist es, dass sie an einem Tag 7 Artikel zu demselben Thema veröffentlicht?\n\nx &lt;- 7 \nlambda &lt;- 3 \ndpois(x, lambda)\n\n[1] 0.02160403\n\n\n\n\n\n1.3.4 Gleichverteilung\nEine Zufallsvariable ist in einem gegebenen Intervall gleichmäßig verteilt. Diese Verteilung ist besonders zur Erzeugung von Zufallszahlen hilfreich.\n\n1.3.4.1 Berechnung\n\\[\nf(x;min,max) = \\begin{cases}\n\\frac{1}{max-min}, & min \\leq x \\leq max \\\\\n0, & \\text{sonst}\n\\end{cases}\n\\]\n\n# Fünf Zufallszahlen, die im voreingestellten Intervall von 0 bis 1 liegen:\nrunif(5) \n\n[1] 0.02481623 0.65386128 0.94028002 0.76336627 0.49607065\n\n# Drei Zufallszahlen, die im Intervall von -10 bis 10 liegen:\nrunif(3, min=-10, max=10) \n\n[1] -2.415376  1.154795  3.693968\n\n\n\n\n\n1.3.5 Exponentialverteilung\nÄhnlich wie die Poissonverteilung, allerdings stetig. Sie ist ein Spezialfall der Weibull-Verteilung und wird z.B. bei der Untersuchung von Zeitabständen eingesetzt.\n\n1.3.5.1 Berechnung\nIm Gegensatz zur etwas vielseitigeren Weibull-Verteilung hat sie einen Parameter weniger, da \\(k\\) unveränderlich gleich 1 ist, geht also von weitgehend gleichmäßig auftretenden Ereignissen aus.\n\\[\nf(x;\\lambda) = \\begin{cases}\n\\lambda e^{-\\lambda x}, & x \\geq 0 \\\\\n0, & x &lt; 0\n\\end{cases}\n\\] Beispiel:\nEine Nachrichtenagentur berichtet durchschnittlich alle 30 Tage über Proteste und Widerstandsaktionen in einem Land X. Wie wahrscheinlich ist es, dass zwischen den Nachrichten plötzlich nur noch maximal 5 Tage liegen?\n\nda &lt;- 30 # Durchschnittlicher Abstand in Tagen\nlambda &lt;- 1 / da\nx &lt;- 5 \nprob &lt;- pexp(x, rate=lambda)\nprob\n\n[1] 0.1535183"
  },
  {
    "objectID": "Autoren.html#hannah-marie-büttner",
    "href": "Autoren.html#hannah-marie-büttner",
    "title": "Das Team stellt sich vor",
    "section": "Hannah-Marie Büttner",
    "text": "Hannah-Marie Büttner\nVita\n\n\n\n\n\nHannah-Marie Büttner\n\n\nHannah-Marie Büttner ist als wissenschaftliche Mitarbeiterin der Universität Bremen am Zentrum für Medien-, Kommunikations- und Informationsforschung sowie am Institut für Informationsmanagement tätig. Sie hat einen B.Sc. Sozialwissenschaften an der Universität Trier und einen Research Master Social Sciences an der University of Amsterdam absolviert.Ihre Forschungsinteressen und methodischen Schwerpunkte liegen im Feld der Computational Social Science. Gegenwärtig forscht sie zu Protestbewegungen und deren Social Media Aktivismus, insbesondere auf der Plattform Telegram.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nSoziale Bewegungen\nSoziale Netzwerkanalyse\nComputerbasierte Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ifib, Institut für Informationsmanagement; Am Fallturm 1; TAB; 28359 Bremen\n🎓 Webseite\n✉️ E-Mail"
  },
  {
    "objectID": "Skript_3.4.html#einfache-häufigkeitstabelle",
    "href": "Skript_3.4.html#einfache-häufigkeitstabelle",
    "title": "Tabellen und Grafiken in R",
    "section": "2.1 Einfache Häufigkeitstabelle",
    "text": "2.1 Einfache Häufigkeitstabelle\nWir berechnen eine einfache Häufigkeitstabelle für die Variable ‘edubde1’ (höchster Bildungsabschluss).\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n())\ngeschlechterverteilung\n\n# A tibble: 2 × 2\n  geschlecht anzahl\n  &lt;fct&gt;       &lt;int&gt;\n1 MANN           15\n2 FRAU            5\n\n\nIm nächsten Schritt fügen wir relative Anteile hinzu, um Verhältnisse besser zu verstehen.\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl))\ngeschlechterverteilung\n\n# A tibble: 2 × 3\n  geschlecht anzahl anteil\n  &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;\n1 MANN           15   0.75\n2 FRAU            5   0.25\n\n\nNun fügen wir noch Prozentanteile ein, indem with mit 100 multiplizieren und runden (was in erster Linie kosmetischer Natur ist).\n\ngeschlechterverteilung &lt;- sample_klein %&gt;% \n  group_by(geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\ngeschlechterverteilung\n\n# A tibble: 2 × 4\n  geschlecht anzahl anteil prozent\n  &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 MANN           15   0.75      75\n2 FRAU            5   0.25      25"
  },
  {
    "objectID": "Skript_3.4.html#tabellen-speichern",
    "href": "Skript_3.4.html#tabellen-speichern",
    "title": "Tabellen und Grafiken in R",
    "section": "2.2 Tabellen speichern",
    "text": "2.2 Tabellen speichern\nJetzt schreiben wir die Daten in eine CSV-Datei, die wir bspw. später mit Excel oder einer anderen Tabellenkalkulation öffnen können.\n\nwrite_excel_csv2(geschlechterverteilung, file = \"geschlechterverteilung.csv\")"
  },
  {
    "objectID": "Skript_3.4.html#kreuztabellen",
    "href": "Skript_3.4.html#kreuztabellen",
    "title": "Tabellen und Grafiken in R",
    "section": "2.3 Kreuztabellen",
    "text": "2.3 Kreuztabellen\nWas, wenn wir zwei Variablen mit Blick auf ihre Werte in Beziehung setzen wollen? Das bezeichnet man als Kreuz- oder Kontigenztabelle.\n\nbildung_und_geschlecht &lt;- sample_klein %&gt;% \n  group_by(bildung, geschlecht) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\n\n`summarise()` has grouped output by 'bildung'. You can override using the\n`.groups` argument.\n\nbildung_und_geschlecht\n\n# A tibble: 9 × 5\n# Groups:   bildung [6]\n  bildung            geschlecht anzahl anteil prozent\n  &lt;fct&gt;              &lt;fct&gt;       &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 VOLKS-,HAUPTSCHULE MANN            1  0.5        50\n2 VOLKS-,HAUPTSCHULE FRAU            1  0.5        50\n3 MITTLERE REIFE     MANN            4  0.571      57\n4 MITTLERE REIFE     FRAU            3  0.429      43\n5 FACHHOCHSCHULREIFE MANN            3  1         100\n6 HOCHSCHULREIFE     MANN            5  0.833      83\n7 HOCHSCHULREIFE     FRAU            1  0.167      17\n8 NOCH SCHUELER      MANN            1  1         100\n9 &lt;NA&gt;               MANN            1  1         100\n\n\nMöglicherweise möchte ich den Anteil anders berechnen und nicht die relative Geschlechterverteilung innerhalb eines Bildungsabschlusses in den Blick nehmen, sondern etwa die Verteilung der Bildungsabschlüsse jeweils für männliche und weibliche Studienteilnehmer anschauen. Dies lässt sich mit einer Neugruppierung der Daten durch group_by() erreichen.\n\ngeschlecht_und_bildung &lt;- sample_klein %&gt;% \n  group_by(geschlecht, bildung) %&gt;% \n  summarise(anzahl = n()) %&gt;% \n  mutate(anteil = anzahl/sum(anzahl),\n         prozent = round(anteil * 100))\n\n`summarise()` has grouped output by 'geschlecht'. You can override using the\n`.groups` argument.\n\ngeschlecht_und_bildung\n\n# A tibble: 9 × 5\n# Groups:   geschlecht [2]\n  geschlecht bildung            anzahl anteil prozent\n  &lt;fct&gt;      &lt;fct&gt;               &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 MANN       VOLKS-,HAUPTSCHULE      1 0.0667       7\n2 MANN       MITTLERE REIFE          4 0.267       27\n3 MANN       FACHHOCHSCHULREIFE      3 0.2         20\n4 MANN       HOCHSCHULREIFE          5 0.333       33\n5 MANN       NOCH SCHUELER           1 0.0667       7\n6 MANN       &lt;NA&gt;                    1 0.0667       7\n7 FRAU       VOLKS-,HAUPTSCHULE      1 0.2         20\n8 FRAU       MITTLERE REIFE          3 0.6         60\n9 FRAU       HOCHSCHULREIFE          1 0.2         20"
  },
  {
    "objectID": "Skript_3.4.html#balkendiagramme",
    "href": "Skript_3.4.html#balkendiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.1 Balkendiagramme",
    "text": "3.1 Balkendiagramme\nDas einfachste Diagramm, das man mit dem ggplot2-Paket erstellen kann, ist ein Balkendiagramm (barplot). Das nachstehende Beispiel zeigt die Häufigkeitsverteilung der Variable gndr (gender = Geschlecht) im Datensatz.\n\nggplot(sample_klein, aes(geschlecht)) + \n  geom_bar()\n\n\n\n\nIn einem nächsten Schritt fügen wir eine Überschrift hinzu und formatieren die Balken und Achsen so, dass das Plot lesbarer ist.\n\nggplot(sample_klein, aes(bildung)) + \n  geom_bar() + \n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nFarben lassen sich in ggplot vielseitig einsetzen, um Kategorienunterschiede anzuzeigen. Dies geschieht mit den Argumenten ‘fill’ bzw. ‘color’.\n\nggplot(sample_klein, aes(bildung, fill = bildung)) + \n  geom_bar() +\n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nEs existieren zahlreiche Farbpaletten für ggplot, um unterschiedliche Arten von Beziehungen darzustellen. Das nachstehende Palette unterscheidet verschiedenen Kategorien. Für gradierte Variablen (‘viel’ - ‘wenig’) sind andere Paletten z.T. besser geeignet. Eine gute Auswahl an Palette enthält u.a. das Paket RColorBrewer.\n\nggplot(sample_klein, aes(bildung, fill = bildung)) + \n  geom_bar() + coord_flip() + \n  scale_fill_brewer(palette = \"Set1\") +  \n  ggtitle(\"Verteilung der Bildungsabschlüsse der Befragten\") + \n  xlab(\"\") + ylab(\"\")\n\n\n\n\nSchließlich lassen sich auch die Beschriftung und weitere Aspekte eines Plots anpassen (Achsenorientierung, Legende etc).\n\nggplot(sample_klein, aes(as_factor(fernsehkonsum))) + \n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + \n  ggtitle(\"Fernsehkonsum pro Woche in Tagen\") + \n  xlab(\"Fernsehkonsum pro Woche in Tagen\") + ylab(\"Anzahl der Respondenten\")"
  },
  {
    "objectID": "Skript_3.4.html#histogramme",
    "href": "Skript_3.4.html#histogramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.2 Histogramme",
    "text": "3.2 Histogramme\nFür ein besseres Verständnis einer Verteilung sind oftmals die Häufigkeitsausprägungen einer kontinuierlichen Variable in gleich großen Gruppen interessant (sog. “bins”). Dabei hilft der Visualisierungstyp Histogramm.\n\nggplot(sample_mittel, aes(alter)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nAuch hier lassen sich relevante Aspekte anpassen, etwa die Anzahl und Breite der Balken.\n\nggplot(sample_mittel, aes(alter)) + \n  geom_histogram(bins = 40) + \n  ggtitle(\"Altersverteilung der Respondenten\") + \n  xlab(\"Alter\") + ylab(\"Anzahl der Respondenten\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nHier können wir erstmalig die Farbe eines Elements gezielt einsetzen, um eine zusätzliche (also nach Vetrauen auf der x-Achse und der Anzahl der Respondenten auf der y-Achse eine dritte Variable) darzustellen, nämlich das Geschlecht der Respondenten.\n\nggplot(sample_gross, aes(vertrauen_polizei, fill = geschlecht)) + \n  geom_histogram(binwidth = 1, position = \"dodge\") + \n  ggtitle(\"Vertrauen in die Polizei nach Geschlecht\") + \n  xlab(\"Vetrauen (1-7)\") + ylab(\"Anzahl der Respondenten\") + labs(fill = \"Geschlecht\") \n\nWarning: Removed 177 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nNeben Balken beherrscht ggplot auch zahlreiche weitere Darstellungsformen (sog. geoms). Eine interessante Alternative zum klassischen Histogramm ist etwa das Dichte-Plot (density plot). Nachstehend verwenden wir zwei Flächen und einen Trasparenz-Effekt für die Darstellung."
  },
  {
    "objectID": "Skript_3.4.html#dichte-plots",
    "href": "Skript_3.4.html#dichte-plots",
    "title": "Tabellen und Grafiken in R",
    "section": "3.3 Dichte-Plots",
    "text": "3.3 Dichte-Plots\n\npolizei &lt;- sample_gross %&gt;% select(vertrauen_polizei, geschlecht) %&gt;% filter(!is.na(geschlecht))\nggplot(polizei, aes(vertrauen_polizei, fill = geschlecht)) + \n  geom_density(alpha = 0.5) + \n  ggtitle(\"Vertrauen in die Polizei nach Geschlecht\") + \n  xlab(\"Vetrauen (1-7)\") + ylab(\"Anteil der Respondenten\") + labs(fill = \"Geschlecht\") \n\nWarning: Removed 177 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Groups with fewer than two data points have been dropped.\n\n\nWarning in max(ids, na.rm = TRUE): kein nicht-fehlendes Argument für max; gebe\n-Inf zurück"
  },
  {
    "objectID": "Skript_3.4.html#liniendiagramme",
    "href": "Skript_3.4.html#liniendiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.4 Liniendiagramme",
    "text": "3.4 Liniendiagramme\nZu den klassischen Plot-Typen gehören neben Barplots und Histogrammen auch Linien-, Punkt- und Streudiagramme, sowie Boxplots.\n\nvertrauen_nach_partei &lt;- sample_gross %&gt;% \n  rename(Partei = wahlabsicht_partei) %&gt;% \n  group_by(Partei) %&gt;% \n  summarise(Vertrauenswürdigkeit = mean(vertrauen_zeitungswesen, na.rm = T))\n\nggplot(vertrauen_nach_partei, aes(Partei, Vertrauenswürdigkeit, group = 1)) + \n  geom_line() + geom_point(size = 3) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Vertrauen in die Presse nach Partei\")\n\n\n\n\nEs lassen sich auch problemlos mehrere Geoms kominieren (hier: Linie und Punkte). Im folgenden Beispiel lässt sich durch eine absteigende Sortierung der Ergebnisse ein klareres Resultat erzielen.\n\nvertrauen_nach_partei_sortiert &lt;- vertrauen_nach_partei %&gt;% \n  arrange(desc(Vertrauenswürdigkeit)) %&gt;% \n  mutate(Rang = row_number())\n\nggplot(vertrauen_nach_partei_sortiert, aes(reorder(Partei, Rang), Vertrauenswürdigkeit, group = 1)) + \n  geom_line() + geom_point(size = 3) + \n  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +\n  ggtitle(\"Verrauen in die Presse nach Partei\") + xlab(\"\")"
  },
  {
    "objectID": "Skript_3.4.html#streudiagramme",
    "href": "Skript_3.4.html#streudiagramme",
    "title": "Tabellen und Grafiken in R",
    "section": "3.5 Streudiagramme",
    "text": "3.5 Streudiagramme\nEine weiterer Diagrammtyp, der häufig zum Einsatz kommt, ist das Streudiagramm (scatter plot). Mit diesem Plottypen lassen sich die Beziehung mehrerer Variablen (i.d.R. zwei, jeweils auf der x/y-Achse) darstellen.\n\neinkommen &lt;- daten %&gt;% \n  select(age, sex, educ, di01a) %&gt;% \n  rename(alter = age,\n         geschlecht = sex,\n         bildung = educ,\n         einkommen = di01a) %&gt;% \n  replace_with_na_all(condition = ~.x &lt; 0) %&gt;% \n  mutate(geschlecht = as_factor(geschlecht),\n         bildung = as_factor(bildung)) %&gt;% \n  drop_na() %&gt;% \n  slice_sample(n = 80)\neinkommen\n\n# A tibble: 80 × 4\n   alter     geschlecht bildung            einkommen\n   &lt;dbl+lbl&gt; &lt;fct&gt;      &lt;fct&gt;              &lt;dbl+lbl&gt;\n 1 24        MANN       HOCHSCHULREIFE     1000     \n 2 41        MANN       FACHHOCHSCHULREIFE 3000     \n 3 80        MANN       HOCHSCHULREIFE     3600     \n 4 59        FRAU       HOCHSCHULREIFE     1900     \n 5 69        MANN       HOCHSCHULREIFE     2800     \n 6 60        MANN       HOCHSCHULREIFE     3500     \n 7 49        MANN       MITTLERE REIFE     2100     \n 8 63        FRAU       FACHHOCHSCHULREIFE 2500     \n 9 30        MANN       FACHHOCHSCHULREIFE 3000     \n10 30        MANN       MITTLERE REIFE      900     \n# ℹ 70 more rows\n\n\nWelche Beziehung lässt sich zwischen Alter (x-Achse) und täglicher Internetnutzung in Minuten (y-Achse) feststellen?\n\nggplot(einkommen, aes(alter, einkommen)) +\n  geom_point()\n\n\n\n\nWir wenden uns jetzt einem etwas ausgefeilteren Beispiel zu, nämlich der Beziehung von politischen Interesse und Nachrichtennutzung.\n\nggplot(einkommen, aes(alter, einkommen)) +\n  geom_jitter(width = .5, size = 2) +\n  geom_smooth(method = 'lm', formula = 'y ~ x') + \n  ggtitle(\"Zusammenhang zwischen Alter und Nettoeinkommen\") + \n  xlab(\"Alter\") + ylab(\"Nettoeinkommen in Euro\")\n\n\n\n\nBei der Linie, die wir mit geom_smooth gezeichnet haben, handelt es sich um eine Regressionsgerade. Der graue Bereich um die Gerade zeigt den lokalen Standardfehler an. Auf Regressionmodelle gehen wir zum Abschluss des Moduls noch intensiv ein.\nWas, wenn wir mehr als drei Variablen (bzw. unterschiedliche Ausprägungen einer kategorialen Variable) darstellen wollen? Neben der Positionierung auf der x- und y-Achse und der Farbe können wir hier zusätzlich auch noch mit unterschiedlichen Formen arbeiten.\nDas nachstehende Beispiel ist nicht unbedingt besonders informativ, zeigt aber das Prinzip nachvollziehbar auf.\n\nggplot(einkommen, aes(alter, einkommen, color = bildung, shape = geschlecht)) +\n  geom_jitter(width = .5, height = .5, size = 2) +\n  #geom_smooth(method = 'lm', formula = 'y ~ x', se = FALSE) + \n  ggtitle(\"Zusammenhang zwischen Alter und Nettoeinkommen \") + \n  xlab(\"Alter\") + ylab(\"Nettoeinkommen\") + \n  labs(color = \"Bildungsabschluss\") + labs(shape = \"Geschlecht\")"
  },
  {
    "objectID": "Skript_3.4.html#facettierte-plots",
    "href": "Skript_3.4.html#facettierte-plots",
    "title": "Tabellen und Grafiken in R",
    "section": "3.6 Facettierte Plots",
    "text": "3.6 Facettierte Plots\nNeben dem Einsatz unterschiedlicher Farben und Formen kann auch sog. Facettierung zum Einsatz kommen um zusätzliche Informationen darzustellen. In einem weiteren Beispiel nehmen wir den Zusammenhang zwischen soziodemographischen Variablen und der Internetnutzung in den Blick. Dazu fügen wir zunächst eine Variable Altersgruppe ein, welche die einzelnen Altersangaben in Kohorten zusammenfasst.\n\nsocial_media &lt;- sample_gross %&gt;% \n  mutate(altersgruppe = cut(alter, \n                           breaks = c(0, 24, 34, 44, 54, 64, Inf),\n                           labels = c(\"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\")),\n         zufriedenheit_demokratie_zusammengefasst = as_factor(case_when(\n           zufriedenheit_demokratie %in% c(\"SEHR ZUFRIEDEN\", \n                                           \"ZIEMLICH ZUFRIEDEN\", \n                                           \"ETWAS ZUFRIEDEN\") ~ \"eher zufrieden\",\n           zufriedenheit_demokratie %in% c(\"ETWAS UNZUFRIEDEN\",\n                                           \"ZIEML. UNZUFRIEDEN\",\n                                           \"SEHR UNZUFRIEDEN\") ~ \"eher unzufrieden\"))) %&gt;% \n  select(altersgruppe, geschlecht, zufriedenheit_demokratie_zusammengefasst, social_media_nachrichtenquelle)\n\nDann aggregieren wir nach Altersgruppe, Geschlecht und Land und plotten anschließend die Ergebnisse.\n\nsocial_media_aggregiert &lt;- social_media %&gt;% \n  group_by(altersgruppe, geschlecht, zufriedenheit_demokratie_zusammengefasst) %&gt;% \n  summarise(socmedia = median(social_media_nachrichtenquelle)) %&gt;% \n  drop_na()\n\n`summarise()` has grouped output by 'altersgruppe', 'geschlecht'. You can\noverride using the `.groups` argument.\n\n\n\nggplot(social_media_aggregiert, aes(altersgruppe, socmedia, fill = geschlecht)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(cols = vars(zufriedenheit_demokratie_zusammengefasst)) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Demokratiezufriedenheit und Social Media-Nutzung nach Alter und Geschlecht\") + \n  xlab(\"Alter\") + ylab(\"Nutzung von Social Media als Nachrichtenquelle\")\n\n\n\n\nSie können Ihr einfaches Diagramm mit Hilfe zusätzlicher Komponenten verfeinern, indem Sie sie dem Diagramm hinzufügen (für ein sehr einfaches “Vokabular” von ggplot2 besuchen Sie diesen Link –&gt; (https://ggplot2.tidyverse.org/reference/).\nAls nächstes schauen wir die Zustimmung zu der Aussafe an, dass der globale Klimawandel größtenteils oder vollständig durch den Menschen verursacht wird.\n\nvertrauen_summiert &lt;- sample_gross %&gt;% \n  rowwise() %&gt;% \n  mutate(vertrauen_gesamt = sum(across(starts_with(\"vertrauen_\")))) %&gt;% \n  select(alter, geschlecht, entwicklung_kriminalitaet, vertrauen_gesamt) %&gt;% \n  ungroup() %&gt;% \n  drop_na()\n\n\nggplot(vertrauen_summiert, aes(entwicklung_kriminalitaet, vertrauen_gesamt)) +\n  geom_boxplot() + geom_jitter(alpha = 0.3) + \n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  ggtitle(\"Kriminalitätseinschätzung und Vertrauen in gesellschaftliche Institutionen\") + \n  xlab(\"Kriminalitätseinschätzung\") + ylab(\"Vertrauen in gesellschaftliche Institutionen\")\n\n\n\n#ggsave(\"Kriminalität_und_Vertrauen.pdf\")"
  },
  {
    "objectID": "Skript_6.3.html#datenmanagement-und-laden-der-daten",
    "href": "Skript_6.3.html#datenmanagement-und-laden-der-daten",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "Voraussetzungsprüfung"
  },
  {
    "objectID": "Skript_6.3.html#laden-der-daten-und-benötigten-pakete",
    "href": "Skript_6.3.html#laden-der-daten-und-benötigten-pakete",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "Wir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n1p_load(tidyverse, ggplot2, haven, mosaic, knitr,effectsize, car, broom)\n2theme_set(theme_classic())\n\n\n1\n\nMittels p_load laden wir alle benötigten Pakete gleichzeitig.\n\n2\n\nWir legen allgemein den Hintergrund theme_classic fest.\n\n\n\n\nAnschließend laden wir die Daten aus dem Allbus:\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_6.3.html#datenmanagement",
    "href": "Skript_6.3.html#datenmanagement",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "Um mit dem Datensatz zu arbeiten benötigen wir einige grundlegende Schritte des Datenmanagements für ausführliche Erklärungen siehe hier. Für unseren t-test möchten wir uns anschauen, wie sich das Geschlecht der Befragten auf ihr Vertrauen in XX auswirkt.\n\ndat &lt;- daten %&gt;% \n  filter(., sex == 1 | sex == 2) %&gt;%  #&gt;1\n  rename(., trustpol = pt15) #&gt;2\n\n\nWir filtern die Fälle, sodass alle Befragten entweder 1=männlich oder 2=weiblich sind.\nDie Variable Vertrauen in die Polizei(pt15) benennen wir in trustpol"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests-für-einstichproben",
    "href": "Skript_6.3.html#durchführung-des-t-tests-für-einstichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.1 Durchführung des t-tests für Einstichproben",
    "text": "2.1 Durchführung des t-tests für Einstichproben"
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.2 Nicht-parametrischer Test:",
    "text": "2.2 Nicht-parametrischer Test:"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben",
    "href": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "5.1 Durchführung des t-tests für verbundene Stichproben",
    "text": "5.1 Durchführung des t-tests für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-test",
    "href": "Skript_6.3.html#nicht-parametrischer-test-mann-whitney-u-wilcoxon-test",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Test",
    "text": "4.6 Nicht-parametrischer Test: Mann-Whitney-U & Wilcoxon-Test\nHistorie der Testverfahren"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben-1",
    "href": "Skript_6.3.html#durchführung-des-t-tests-für-verbundene-stichproben-1",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.1 Durchführung des t-tests für verbundene Stichproben",
    "text": "4.1 Durchführung des t-tests für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#nicht-parametrischer-test-wilcoxon-vorzeichen-rangtest",
    "href": "Skript_6.3.html#nicht-parametrischer-test-wilcoxon-vorzeichen-rangtest",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "5.2 Nicht-parametrischer Test: Wilcoxon-Vorzeichen-Rangtest",
    "text": "5.2 Nicht-parametrischer Test: Wilcoxon-Vorzeichen-Rangtest"
  },
  {
    "objectID": "Skript_6.1.html#allgmeines-zu-mittelwertvergleichen",
    "href": "Skript_6.1.html#allgmeines-zu-mittelwertvergleichen",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "Statistische Tests erlauben es uns, auf der Basis von quantitativen Daten eine begründete Entscheidung über die Gültigkeit oder Ungültigkeit einer Hypothese zu treffen.\nBei den hier verwendeten Beispielen geht es darum, Unterschiede zwischen zwei oder mehr Stichproben (oder Teilstichproben) statistisch zu überprüfen.\nEin statistischer Test liefert Aufschluss darüber, wie wahrscheinlich es ist, dass wir die Nullhypothese verwerfen und/oder die alternative Hypothese annehmen können.\nKönnen wir aufgrund des Testausgangs davon ausgehen, dass ein (nicht zufälliger) Unterschied zwischen zwei Stichproben besteht, so sprechen wir von einem signifikanten (überzufälligen) Unterschied."
  },
  {
    "objectID": "Skript_6.3.html#deskriptives",
    "href": "Skript_6.3.html#deskriptives",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "",
    "text": "# Deskription der Abhängigen Variablen\n#des_stat &lt;- (favstats(AV ~ UV, data = daten))\n#kable (des_stat,\n#      col.names = c(label_UV,\"Minimum\", \"1.Quartil\", \n#                    \"Median\", \"3.Quartil\", \n#                    \"Maximum\", \"M\", \"SD\", \"N\",\n#                    \"Fehlend\"),\n#      digits = 2)"
  },
  {
    "objectID": "Skript_6.3.html#skalierung-der-uv-und-av",
    "href": "Skript_6.3.html#skalierung-der-uv-und-av",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.1 Skalierung der UV und AV",
    "text": "4.1 Skalierung der UV und AV\ndaten %&gt;% select(UV, AV) %&gt;% lapply(class)"
  },
  {
    "objectID": "Skript_6.3.html#normalverteilung-der-av",
    "href": "Skript_6.3.html#normalverteilung-der-av",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.2 Normalverteilung der AV",
    "text": "4.2 Normalverteilung der AV\ndaten %&gt;% group_by(UV) %&gt;% do(tidy(shapiro.test(.$AV)))"
  },
  {
    "objectID": "Skript_6.3.html#varianzhomogenität",
    "href": "Skript_6.3.html#varianzhomogenität",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.3 Varianzhomogenität",
    "text": "4.3 Varianzhomogenität\ndaten %&gt;% drop_na(UV, AV) %&gt;% leveneTest(AV~UV, data = .)"
  },
  {
    "objectID": "Skript_6.3.html#durchführung-des-t-tests",
    "href": "Skript_6.3.html#durchführung-des-t-tests",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.4 Durchführung des t-Tests",
    "text": "4.4 Durchführung des t-Tests"
  },
  {
    "objectID": "Skript_6.3.html#interpretation-des-outputs",
    "href": "Skript_6.3.html#interpretation-des-outputs",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "4.5 Interpretation des Outputs",
    "text": "4.5 Interpretation des Outputs\n\nwas sehen wir?\nwas bedeutet das?\n\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse des t-tests werden üblicherweise im Text angegeben. Für diese Angabe werden die folgenden Informationen benötigt:\n✅ die Werte des Kaiser-Meyer-Olkin Kritierums (KMO)\nDas Format ist üblicherweise:\n\nBeispiel: Zunächst wurde Faktorenanalyse der 15 gemessenen Indikatoren durchgeführt. Das Kaiser-Meyer-Olkin (KMO)-Maß für die Stichprobenadäquanz betrug 0,89. Dies deutet darauf hin, dass die Korrelationsmuster relativ kompakt sind und die Faktorenanalyse eindeutige und zuverlässige Faktoren ergeben sollte. Der Bartlett-Test auf Sphärizität war ebenfalls signifikant (χ2(105) = 33164.76, p &lt; .001). Dies bedeutet, dass es einige Beziehungen zwischen den untersuchten Variablen gibt. Sowohl der KMO-Test als auch der Bartlett-Test bestätigen, dass die Faktorenanalyse angemessen ist."
  }
]