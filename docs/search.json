[
  {
    "objectID": "Skript_7.3.html",
    "href": "Skript_7.3.html",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei mehr als zwei Variablen"
  },
  {
    "objectID": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Notebook gehen wir (wie angekündigt) zunächst näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Dann lernen wir die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren.\n\n1.1.1 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ESS8_vier_laender-Datensatzes, den wir entsprechend einlesen:\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\")\n#install.packages(\"lmtest\")\n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n#install.packages(\"sandwich\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\nlibrary(lmtest)\n\nWarning: Paket 'lmtest' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: zoo\n\n\nWarning: Paket 'zoo' wurde unter R Version 4.3.1 erstellt\n\n\n\nAttache Paket: 'zoo'\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\nWarning: Paket 'sandwich' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n1.1.2 Data Management\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die Internetnutzung (netustm); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Rezeptionszeit von politischen Nachrichten (nwspol) sowie das Geschlecht der Befragten (gndr) an. Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl um.\nDann setzen wir den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifizieren wir wieder, auf welche Variablen sich der Befehl beziehen soll). Das modifizierte Datenset weisen wir einem neuen Datenobjekt zu: daten_mod2\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(internetnutzung = netustm,\n         alter = agea,\n         politische_Nachrichtenrezeption = nwspol,\n         gender = gndr) %&gt;% \n  drop_na(c(internetnutzung, alter, politische_Nachrichtenrezeption, gender)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001263 GB    Female    72 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2  10006488 DE    Female    42 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 3  10000260 DE    Male      55 &lt;NA&gt;                     Mittle… Kein H… Abgesc…\n 4      1147 FR    Male      44 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      2089 FR    Male      72 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10000032 DE    Male      30 None of these (NEVER ma… Refusal Kein H… Refusal\n 7      1766 FR    Male      45 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8 100000390 GB    Male      63 Legally divorced/civil … &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 9 100000483 GB    Female    55 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10      2133 FR    Male      58 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n# ℹ 90 more rows\n# ℹ 158 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\n\n1.1.3 Erinnerung: Einfache lineare Regression mit Alter als UV und Internetnutzung als AV mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nsummary(model) # klassischer Output mit relevanten Kennzahlen\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-222.39 -116.02  -56.96   35.95  719.76 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  313.600     54.435   5.761 0.0000000965 ***\nalter         -2.186      1.086  -2.013       0.0469 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 186.3 on 98 degrees of freedom\nMultiple R-squared:  0.0397,    Adjusted R-squared:  0.0299 \nF-statistic: 4.051 on 1 and 98 DF,  p-value: 0.04688\n\n#summary(lm.beta(model)) # klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Internetnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um “den Estimate-Wert” in Messeinheiten (hier: -4.296 Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit dem Teil A dieses Skripts, nämlich der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern Sie sich, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Sie haben Recht: Eigentlich würden wir erst die Voraussetzungen prüfen, dann das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen.\n\n\n1.1.4 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)\n\n\n1.1.5 TEIL A: Prüfung der Voraussetzungen einer Regressionsanalyse\n\n1.1.5.1 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt.\n\n\n1.1.5.2 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß – unabhängig wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde.\n\ncheck_heteroscedasticity(model)\n\nOK: Error variance appears to be homoscedastic (p = 0.154).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). In diesem Fall liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\n\n\n1.1.5.3 Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei höheren Werten erkennbar ist, weil wir einen leicht nach rechts geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\n\n\n1.1.5.4 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 313.60004   54.06636  5.8003 0.00000008113 ***\nalter        -2.18624    0.98757 -2.2137       0.02917 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!\n\n\n1.1.5.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.680).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0,588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!\n\n\n1.1.5.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.\n\n\n1.1.5.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, kann ich wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n1.1.5.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr coole Funktion, die mir eine visuelle Inspektion meiner Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion kann ich mir dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\n\ncheck_model(model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n\n1.1.6 TEIL B: Die multiple lineare Regression\n\n1.1.6.1 Anwendungsbereich der multiplen linearen Regression\nDie multiple lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen metrischen Variablen besteht. Die multiple lineare Regressionsanalyse hat das Ziel, eine abhängige Variable (y) mittels mehrerer unabhängigen Variablen (x1, x2, …) zu erklären. (Zur Erinnerung: Für nur eine x-Variable nutzen wir die einfache lineare Regression)\nMit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen den unabhängigen und der einen abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variablen vorhergesagt werden?\nDie multiple Regression entspricht in ihrer Analyslogik also der einfachen linearen Regression - nur dass sie mehr als eine unabhängige Variable berücksichtigt.\n\n\n1.1.6.2 Ziel der Analyse\nMit Hilfe der multiplen Regression wollen wir die Annahme prüfen, dass die Variablen Alter (agea) sowie die Rezeptionszeit von politischen Nachrichten (nwspol) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) haben bzw. diese erklären und vorhersagen können. Alle Variablen sind metrisch und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n1.1.6.3 Modell zum Zusammenhang von Alter, politischer Nachrichtenrezeption und Internetnutzung spezifizieren und anzeigen lassen\nDie Berechnung der multiplen Regression unterscheidet sich nicht stark von der Berechnung der einfachen linearen Regression. In die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) einfach die zusätzliche unabhängige Variable (politische_Nachrichtenrezeption) ein, indem wir sie mit einem + Zeichen anhängen:\n\nmodel_m &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption, data = daten_mod)\nsummary(lm.beta(model_m))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption, \n    data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-208.19 -111.70  -49.67   39.45  717.79 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     299.4215           NA    54.9302   5.451\nalter                            -2.2233      -0.2026     1.0798  -2.059\npolitische_Nachrichtenrezeption   0.2045       0.1466     0.1373   1.490\n                                   Pr(&gt;|t|)    \n(Intercept)                     0.000000381 ***\nalter                                0.0422 *  \npolitische_Nachrichtenrezeption      0.1395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.1 on 97 degrees of freedom\nMultiple R-squared:  0.06118,   Adjusted R-squared:  0.04182 \nF-statistic:  3.16 on 2 and 97 DF,  p-value: 0.0468\n\n\n\n\n1.1.6.4 Interpretation des Outputs: Was sehen wir in der Regressionstabelle?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängigen Variablen “alter” und “politische_Nachrichtenrezeption” zu erklären.\n\n\n1.1.6.5 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n1.1.6.6 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n1.1.6.7 Standardized\nDiese Estimates sind die standardisierte b-Werte. Weil wir diese über die lm.beta-Funktion standardisiert haben, lassen sich die Koeefizienten auch bei unterschiedlicher Skalierung vergleichen.\n\n\n1.1.6.8 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n1.1.6.9 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n1.1.6.10 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n1.1.6.11 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter und politische Nachrichtenrezeption auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter und die politische Nachrichtenrezeption etwa 20 Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n1.1.6.12 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n1.1.6.13 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n1.1.6.14 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (-4.9169) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &lt; .05 statistisch signifikant. Diese Ergebnisse überraschen uns nicht: Den Einfluss des Alters haben wir ja letzte Woche schon überprüft. Mit der Erweiterung zur multiplen Regression können wir nun zusätzlich sagen, dass die politische Nachrichtenrezeption auch einen Einfluss auf die Internet-Nutzung hat, denn der Wert ist ebenfalls signifikant (p &lt; .05)! Die F-Statistik sagt uns zusätzlich, dass auch unser Gesamtmodell signifikant ist (p-value: 0.00001239, also &lt; .05).\n\n\n1.1.6.15 Erinnerung: Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model_m))\n\n# A tibble: 3 × 6\n  term                         estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;                           &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)                   299.          NA        54.9        5.45 3.81e-7\n2 alter                          -2.22        -0.203     1.08      -2.06 4.22e-2\n3 politische_Nachrichtenrezep…    0.205        0.147     0.137      1.49 1.40e-1\n\n\n\nglance(model_m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0612        0.0418  185.      3.16  0.0468     2  -662. 1333. 1343.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "href": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden",
    "text": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.3 Prüfung der Voraussetzungen",
    "text": "1.3 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "href": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.4 Berechnung und Interpretation einer multiplen Regression",
    "text": "1.4 Berechnung und Interpretation einer multiplen Regression"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Skript_1.1.html",
    "href": "Skript_1.1.html",
    "title": "Was ist R?",
    "section": "",
    "text": "1 Was ist R?\nEinführung in die Logik von R, R Studio Server und das R Environment Einführung in Markdown (Quarto?) Basics der Befehlssyntax in Base R und Tidyverse (im Vergleich), ab dann aber alles in dplyr Laden von Daten und Importieren von anderen Datenformaten Einführung in das Datenmanagement: Projekte und Ordnerstrukturen auf dem PC Öffnen von Datensätzen, Laden von Daten, Importieren von anderen Datensätzen Speichern von Daten aus R in verschiedenen Formaten (Rda, csv etc.) Installieren von Paketen und Laden von Librarys (p_load) Vorschau: Datentypen und deren Charakteristika.\nHier müssen noch die LearnR Inhalte\nLearnR"
  },
  {
    "objectID": "Skript_1.2.html",
    "href": "Skript_1.2.html",
    "title": "R-Studio",
    "section": "",
    "text": "1 Wie funktioniert R-Studio?"
  },
  {
    "objectID": "Skript_1.3.html",
    "href": "Skript_1.3.html",
    "title": "Relevante Begriffe",
    "section": "",
    "text": "1 Was ist die Konsole und was ein Skript?"
  },
  {
    "objectID": "Skript_2.1.html",
    "href": "Skript_2.1.html",
    "title": "Der Aufbau von Datensätzen",
    "section": "",
    "text": "1 Was ist ein Datensatz und wie ist dieser aufgebaut?"
  },
  {
    "objectID": "Skript_2.2.html",
    "href": "Skript_2.2.html",
    "title": "klassische Formen von Datensätzen",
    "section": "",
    "text": "1 Klassische Formen von Datensätzen in der Kommunikationswissenschaft"
  },
  {
    "objectID": "Skript_2.3.html",
    "href": "Skript_2.3.html",
    "title": "Praktische Tipps",
    "section": "",
    "text": "1 Praktische Tipps im Umgang mit Datensätzen"
  },
  {
    "objectID": "Skript_3.2.html",
    "href": "Skript_3.2.html",
    "title": "Datentypen und -strukturen",
    "section": "",
    "text": "1 Datentypen und -strukturen in R\nGrundsätzlicher Aufbau von einem Datensatz (Was ist ein Fall? Was ist eine Spalte? Was eine Zeile?) Überblick über die Daten erhalten Einführung in das Datenmanagement: Objekte festlegen und Variablen definieren Datentypen und deren Charakteristika Datentypen, Vektoren, Matrizen, und Data Frames in R Vorschau: Manipulation und Transformation von Daten jeweils alles mit dplyr"
  },
  {
    "objectID": "Skript_3.3.html",
    "href": "Skript_3.3.html",
    "title": "Selektion, Manipulation und Transformation",
    "section": "",
    "text": "1 Selektion, Manipulation und Transformation von Daten\nDas tidyverse-Universum Manipulation und Transformation von Daten Zufallsstichprobe ziehen Daten filtern/Fälle und Spalten auswählen; Subsets bilden aufgrund von konditionalen Bedingungen Gruppieren von Fällen (group_by)\nUmgang mit realen, nicht-sauberen Datensätzen an konkreten Beispielen (z.B. missing data; -99/-77, invertierte Items; “weiß nicht”; fehlende Faktoren) Variablen recodieren / rename (Neue) Variablen berechnen (z.B. mehrere Variablen in einem Index bzw. einer Skala zusammenfassen)"
  },
  {
    "objectID": "Skript_3.4.html",
    "href": "Skript_3.4.html",
    "title": "Tabellen und Grafiken in R",
    "section": "",
    "text": "1 Eine Einführung in GGPlot"
  },
  {
    "objectID": "Skript_4.1.html",
    "href": "Skript_4.1.html",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "1 Berechnung von einfachen Häufigkeiten\n\n\n2 Visuelle Darstellung von einfachen Häufigkeiten\n\n\n3 Interpretation von einfachen Häufigkeiten"
  },
  {
    "objectID": "Skript_4.2.html",
    "href": "Skript_4.2.html",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "",
    "text": "Berechnung von Lageparametern"
  },
  {
    "objectID": "Skript_4.2.html#berechnung",
    "href": "Skript_4.2.html#berechnung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung",
    "href": "Skript_4.2.html#visualisierung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation",
    "href": "Skript_4.2.html#interpretation",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-1",
    "href": "Skript_4.2.html#berechnung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-1",
    "href": "Skript_4.2.html#visualisierung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-1",
    "href": "Skript_4.2.html#interpretation-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-2",
    "href": "Skript_4.2.html#berechnung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-2",
    "href": "Skript_4.2.html#visualisierung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-2",
    "href": "Skript_4.2.html#interpretation-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html",
    "href": "Skript_4.3.html",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "",
    "text": "Berechnung und Interpretation von Streuungsmaßen"
  },
  {
    "objectID": "Skript_4.3.html#berechnung",
    "href": "Skript_4.3.html#berechnung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung",
    "href": "Skript_4.3.html#visualisierung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation",
    "href": "Skript_4.3.html#interpretation",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-1",
    "href": "Skript_4.3.html#berechnung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-1",
    "href": "Skript_4.3.html#visualisierung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-1",
    "href": "Skript_4.3.html#interpretation-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-2",
    "href": "Skript_4.3.html#berechnung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-2",
    "href": "Skript_4.3.html#visualisierung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-2",
    "href": "Skript_4.3.html#interpretation-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html",
    "href": "Skript_4.4.html",
    "title": "Verteilungen und deren Visualisierung",
    "section": "",
    "text": "Berechnung und Interpretation von Verteilungen"
  },
  {
    "objectID": "Skript_4.4.html#berechnung",
    "href": "Skript_4.4.html#berechnung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung",
    "href": "Skript_4.4.html#visualisierung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation",
    "href": "Skript_4.4.html#interpretation",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html#berechnung-1",
    "href": "Skript_4.4.html#berechnung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-1",
    "href": "Skript_4.4.html#visualisierung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-1",
    "href": "Skript_4.4.html#interpretation-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_5.1.html",
    "href": "Skript_5.1.html",
    "title": "Die Messung von latenten Variablen",
    "section": "",
    "text": "1 Einführung in die Messung von latenten Variablen"
  },
  {
    "objectID": "Skript_5.2.html",
    "href": "Skript_5.2.html",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir möchten einen Index berechnen. Im Fragebogen des ESS sind einige Indikatoren enthalten, welche sich zu einem Index zusammenführen lassen. Dies möchten wir jetzt tun. Allerdings müssen wir sicher gehen, dass die Messung der Indikatoren auch wirklich nach unseren theoretischen Annahmen nach funktioniert hat. Konkret müssen wir überprüfen, ob sich die theoretisch angenommenen Dimensionen unseres Index auch in den Daten zu finden sind. Hierfür können wir die Faktorenanalyse oder Principal Component Analysis verwenden.\n\n\nVideo\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten.\n\n\n\n\nlibrary(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)\n\n\n\n\n\ndaten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")\n\n\n\n\nWir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2\n\n\n\n\n\n\nsummary(ess_wirksamkeit)\n\n    psppsgva        actrolga        psppipla        cptppola    \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.211   Mean   :2.085   Mean   :2.154   Mean   :2.133  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n\n#Alternativ kann describe() verwendet werden. Describe() ist eine Funktion des psych Pakets, welche die wichtigsten Streu- und Lageparameter für Variablen angiebt\n\n\n\n\nDie Faktorenanalyse bringt einige Voraussetzungen an die Daten mit sich, welche zu Beginn geprüft werden müssen. Zum Nachlesen bietet sich hier Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811 an.\nVoraussetzungen sind:\n\nStichprobengröße (in unserem Fall kein Problem)\nAusreichende Anzahl an Variablen (meist 4 oder mehr pro Faktor/Dimension)\nDie Variablen sind intervallskaliert (In der Praxis werden jedoch oft auch ordinalskalierte Variablen verwendet.)\n\nDiese Punkte sind bei uns alle gegeben.\nAnmerkung: Wir können auch eine Faktorenanalyse mit nicht-kontinuierlichen Daten rechnen (bspw. dichotome Variablen). Hier sollten wir sollten wir die Korrelationsmatrix aus polychorischen Korrelationskoeffizienten konstruieren (diese können mit der Funktion polychor() aus dem Paket polycor berechnet werden\nZunächst schauen wir uns einmal die Korrelationskoeffizienten für einige Variablen an.\n\n# Auswählen einiger zusätzlichen Variablen zum zeigen der Korrelationstabelle\n\nbeispiel = daten %&gt;% \n  select(netusoft, \n         trstplt, \n         trstep, \n         psppsgva, \n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(across(netusoft:cptppola, as.numeric)) %&gt;% # mit across kann die gleiche Funktion - in diesem Fall as.numeric() - mehrere Variablen angewandt werden\n  na.omit()\n\n# Bessere Darstellung der Tabelle mit htmlTable und gerundeten Werten für bessere Lesbarkeit\n\nlibrary(htmlTable) # Falls das Paket noch nicht installiert ist, muss es mit install.packages(\"htmlTable\") installiert werden\nhtmlTable(round(cor(beispiel), digits = 3))\n\n\n\n\n\nnetusoft\ntrstplt\ntrstep\npsppsgva\nactrolga\npsppipla\ncptppola\n\n\n\n\nnetusoft\n1\n0.088\n0.128\n0.176\n0.267\n0.224\n0.25\n\n\ntrstplt\n0.088\n1\n0.574\n0.424\n0.187\n0.421\n0.178\n\n\ntrstep\n0.128\n0.574\n1\n0.294\n0.132\n0.291\n0.122\n\n\npsppsgva\n0.176\n0.424\n0.294\n1\n0.329\n0.655\n0.312\n\n\nactrolga\n0.267\n0.187\n0.132\n0.329\n1\n0.422\n0.688\n\n\npsppipla\n0.224\n0.421\n0.291\n0.655\n0.422\n1\n0.408\n\n\ncptppola\n0.25\n0.178\n0.122\n0.312\n0.688\n0.408\n1\n\n\n\n\n\nNoch zu prüfen ist die Korrelation der Items miteinander, hierfür nehmen wir den Bartlett Test.\n\ncortest.bartlett(ess_wirksamkeit)\n\nR was not square, finding R from data\n\n\n$chisq\n[1] 61775.54\n\n$p.value\n[1] 0\n\n$df\n[1] 6\n\n\nWenn der p-Wert signifikant ist (ist hier der Fall) korrelieren die Items miteinandern. Das ist gewünscht.\nAnsonsten prüfen wir das Kaiser-Meyer-Olkin-Kriterium (KMO) und die Measure of Sampling Adequacy (MSA) anschauen.\n\nKMO(ess_wirksamkeit)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = ess_wirksamkeit)\nOverall MSA =  0.65\nMSA for each item = \npsppsgva actrolga psppipla cptppola \n    0.65     0.65     0.67     0.65 \n\n\nDie Werte für die MSA Werte sollten jeweils größer als 0.5 sein. Ist hier der Fall. Falls ein Item unter 0.5 liegen sollte würde dieses ausgeschlossen werden.\nAls nächster Schritt möchten wir Wissen wieviele Faktoren wir aus den Daten “ziehen” möchten. Hierfür nutzen wir die Funktion nfactors()\n\nnfactors(ess_wirksamkeit, rotate = \"varimax\", fm=\"MLE\")\n\n\n\n\n\nNumber of factors\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.78  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.94  with  3  factors\nThe Velicer MAP achieves a minimum of 0.24  with  1  factors \nEmpirical BIC achieves a minimum of  12803.87  with  1  factors\nSample Size adjusted BIC achieves a minimum of  15644.55  with  1  factors\n\nStatistics by number of factors \n  vss1 vss2  map dof chisq prob sqresid  fit RMSEA   BIC SABIC complex  eChisq\n1 0.78 0.00 0.24   2 15659    0    1.54 0.78  0.43 15638 15645     1.0 1.3e+04\n2 0.75 0.94 0.33  -1     0   NA    0.41 0.94    NA    NA    NA     1.2 2.0e-13\n3 0.75 0.94 1.00  -3     0   NA    0.41 0.94    NA    NA    NA     1.2 6.0e-18\n4 0.68 0.89   NA  -4  3079   NA    0.74 0.89    NA    NA    NA     1.3 1.6e+03\n     SRMR eCRMS  eBIC\n1 1.6e-01  0.28 12804\n2 6.3e-10    NA    NA\n3 3.4e-12    NA    NA\n4 5.7e-02    NA    NA\n\n#?nfactors \n\nHier wählen wir zunächst den R Console Output aus und schauen auf den MAP Test, welcher uns hier einen Faktor/Dimension vorschlägt. Die Funktion gibt uns noch eine Reihe anderer Werte aus, welche wir jetzt aber nicht im Detail anschauen. Eine genauere Beschreibung der Werte kann mit ?nfactors aufgerufen werden.\nDieser Schritt ist sehr wichtig. Wir müssen im nächsten Schritt die Anzahl der Faktoren angeben. Hierfür müssen wir uns zunächst auf Grundlage der Statistischen Tests UND der theoretischen Überlegungen auf eine Anzahl von Faktoren bzw. Dimensionen festgelegt haben.\nIn unserem Beispiel gehen wir testweise von einem Faktoren aus.\nWir legen das Modell an mit folgenden Parametern: Maximum-Likelihood (ML) 1 Faktor Varimax-Rotation –&gt; Mit einer Rotation kann man die Interpretation der Faktoren verbessern, hier gibt es verschiedene Möglichkeiten “varimax” ist eine Option.\n\nfit &lt;- factanal(ess_wirksamkeit, 1, rotation = \"varimax\")\n\n# Anzeigen der Ergebnisse mit 2 Nachkommastellen und dem Ausblenden von Faktorladungen die kleiner als 0.3 sind\n\nprint(fit, digits = 2, cutoff = .3)\n\n\nCall:\nfactanal(x = ess_wirksamkeit, factors = 1, rotation = \"varimax\")\n\nUniquenesses:\npsppsgva actrolga psppipla cptppola \n    0.75     0.33     0.65     0.36 \n\nLoadings:\n         Factor1\npsppsgva 0.50   \nactrolga 0.82   \npsppipla 0.59   \ncptppola 0.80   \n\n               Factor1\nSS loadings       1.90\nProportion Var    0.47\n\nTest of the hypothesis that 1 factor is sufficient.\nThe chi square statistic is 15659.49 on 2 degrees of freedom.\nThe p-value is 0 \n\n\nBeim Output ist das wichtigste die Faktorladungen. Im idealfall laden die Variablen nur jeweils auf einen Faktor. Wir haben bereits geringe Faktorladungen bereits aus der Darstellung ausgeschlossen.\nHier laden alle Variablen mit hohen Werten auf den Faktor1. Wir können also davon ausgehen, dass wir diese Variablen zu einem Index verrechnen können.\nJetzt schauen wir uns doch noch kurz die Uniquenes bzw. Kommunalitäten an. Die gibt uns an wieviel Varianz von den jeweiligen Variablen durch die Faktoren erklärt wird.\n\n1 - fit$uniquenesses\n\n psppsgva  actrolga  psppipla  cptppola \n0.2456154 0.6655900 0.3458397 0.6386451 \n\n\nDie Werte geben uns eine Idee davon, wie wichtig die Variablen für die erzielte Lösung sind.\n\n\n\n\nHauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben.\n\n\n\n\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html",
    "href": "Skript_5.3.html",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Berechnung und Interpretation der Reliabilität von Skalen"
  },
  {
    "objectID": "Skript_6.1.html",
    "href": "Skript_6.1.html",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "1 Einführung in die Bivariate Statistik und Mittelwertvergleiche"
  },
  {
    "objectID": "Skript_6.2.html",
    "href": "Skript_6.2.html",
    "title": "Bestimmen von Unterschieden in der Varianz mit Kreuztabellen und dem Chi-Quadrat Test",
    "section": "",
    "text": "Bestimmen von Unterschieden in der Varianz\n\n1 Kreuztabellen und Chi-Quadrat Test"
  },
  {
    "objectID": "Skript_6.3.html",
    "href": "Skript_6.3.html",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "",
    "text": "Bestimmen von Unterschieden in der zentralen Tendenz"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben",
    "text": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben\n\n1.1.1 t-Test für unabhängige Stichproben\n\n\n1.1.2 Mann-Whitney\n\n\n1.1.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n1.1.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n\nCode\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\n\nCode\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n\n1.1.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n1.1.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n\nCode\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n\nCode\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n\nCode\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n1.1.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\nCode\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n1.1.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\nCode\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\n\nCode\nprint(fit)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n1.1.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt.\n\n\n\n1.1.4 Kruskal Wallis\n\n\n1.1.5 mehrfaktorielle Varianzanalyse\n\n1.1.5.1 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\n\nCode\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\n\nCode\nprint(fit2)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n\n1.1.5.2 Post-Hoc Tests\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n1.1.5.3 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n\nCode\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\n\n\nCode\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben",
    "text": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben\n\n1.2.1 t-Test für verbundene Stichproben\n\n\n1.2.2 Wilcoxon"
  },
  {
    "objectID": "Skript_6.4.html",
    "href": "Skript_6.4.html",
    "title": "Proportionen Häufigkeiten mit dem Binomial Test und Pearson Chi-Quadrat",
    "section": "",
    "text": "Bestimmen von Unterschieden in Bezug auf Proportionen Häufigkeiten\n\n1 Binomial Test\n\n\n2 Pearson Chi-Quadarat"
  },
  {
    "objectID": "Skript_7.1.html",
    "href": "Skript_7.1.html",
    "title": "Einführung",
    "section": "",
    "text": "1 Einführung in das Überprüfen von Zusammenhängen\nWillkommen zu unserem Kapitel über die statistische Überprüfung von Zusammenhängen mit Hilfe von R. In der datengetriebenen Welt von heute ist es von großer Bedeutung, statistische Zusammenhänge zu verstehen und zu validieren. In diesem Kapitel werden wir Ihnen zeigen, wie Sie mithilfe von R verschiedene statistische Tests durchführen können, um Zusammenhänge zwischen Variablen zu untersuchen.\nDer erste Schritt besteht darin, die Daten in R zu importieren und zu explorieren. Wir werden Ihnen zeigen, wie Sie die Daten visualisieren und grundlegende statistische Kennzahlen berechnen können, um einen ersten Eindruck von den vorliegenden Zusammenhängen zu bekommen. Anschließend werden wir auf verschiedene statistische Tests eingehen, darunter den Korrelationstest, den Chi-Quadrat-Test und verschiedene Formen der Regression. Sie lernen, wie Sie diese Tests in R implementieren, die Ergebnisse interpretieren und fundierte Schlussfolgerungen ziehen können.\nDarüber hinaus werden wir auf wichtige Konzepte wie Signifikanzniveau, p-Wert und Konfidenzintervalle eingehen, um Ihnen ein solides Verständnis dafür zu vermitteln, wie statistische Zusammenhänge bewertet werden können. Durch die Anwendung dieser Methoden werden Sie in der Lage sein, Ihre Daten genau zu analysieren, potenzielle Zusammenhänge zu identifizieren und deren Bedeutung zu bewerten. Tauchen Sie ein in die spannende Welt der statistischen Überprüfung von Zusammenhängen mit R und erweitern Sie Ihr analytisches Toolkit!\n(Es handelt sich bei dem Text um einen Platzhalter, erstellt von chatGDP)"
  },
  {
    "objectID": "Skript_7.2.html",
    "href": "Skript_7.2.html",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei zwei Variablen"
  },
  {
    "objectID": "Skript_7.2.html#korrelation",
    "href": "Skript_7.2.html#korrelation",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.1 Korrelation",
    "text": "3.1 Korrelation"
  },
  {
    "objectID": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "href": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse",
    "text": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse\nIn diesem Notebook wird die einfache lineare Regression auf Grundlage der ESS-Daten vorgestellt. In der nächsten Sitzung gehen wir näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen die multiple lineare Regression kennen.\n\n\n\nPicture generated by Midjourney\n\n\n\n3.2.1 Anwendungsbereich der linearen Regression\nDie einfache lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen zwei metrischen Variablen besteht. Sie wird daher auch als bivariate Regression bezeichnet.\nZiel ist es, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?\n\n\n3.2.2 Vorbereitung und Laden der Daten\nZunächst laden wir die Pakete des tidyverse. Weiterhin laden wir das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt.\nDen Datensatz finet ihr hier.\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\") \n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n3.2.3 Ziel der Analyse\nMit Hilfe der Regression wollen wir die Annahme prüfen, dass das Alter (agea) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) hat. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n3.2.4 Data Management\nDamit der Output etwas nachvollziehbarer wird, benenne ich die Variablen mit dem rename-Befehl zunächst einmal um. Dann nutze ich den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifiziere ich, auf welche Variablen sich der Befehl beziehen soll). Das alles weise ich einem neuen Datenobjekt zu: daten_mod\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm) %&gt;% \n  drop_na(c(alter, internetnutzung)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gndr   alter marsts   edubde1 eduade2 eduade3 nwspol netusoft\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1      1617 SE    Female    51 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        45 Every d…\n 2  10009636 DE    Male      35 &lt;NA&gt;     Abitur… Master… Meiste…     60 Every d…\n 3      2108 FR    Female    48 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Most da…\n 4      2867 FR    Male      56 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 5 100004293 GB    Female    64 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Most da…\n 6      2925 FR    Female    40 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;         0 Most da…\n 7 100003296 GB    Male      18 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        10 Every d…\n 8  10008298 DE    Male      21 None of… Abitur… Kein H… Kein b…     60 Every d…\n 9      1322 FR    Male      35 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        15 Every d…\n10      3631 SE    Female    17 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        15 Every d…\n# ℹ 90 more rows\n# ℹ 156 more variables: internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;,\n#   pplhlp &lt;dbl&gt;, polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;,\n#   psppipla &lt;fct&gt;, cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;,\n#   trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;, trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;,\n#   vote &lt;fct&gt;, prtvede1 &lt;fct&gt;, prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;,\n#   wrkorg &lt;fct&gt;, badge &lt;fct&gt;, sgnptit &lt;fct&gt;, pbldmn &lt;fct&gt;, bctprd &lt;fct&gt;, …\n\n\n\n\n3.2.5 Prüfung der Voraussetzungen 1: Grafische Darstellung des Zusammenhangs der beiden Variablen, um die Annahme von Linearität zu prüfen\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und Internetnutzung. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Internetznutzung) und x (=Alter) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und Internetnutzung\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")\n\n\n\n\n\n\n3.2.6 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen schwachen negativen (aber linearen!) Zusammenhang zwischen Alter und Internetnutzung nahe: mit zunehmendem Alter sinkt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\nNicht wundern: Weil wir oben ein Zufallssample gezogen haben, sieht die Grafik bei Ihnen allen etwas anders aus. Sie kann dadurch auch so ausfallen, dass der lineare Zusammenhang nicht (gut) sichtbar ist – vor allem dann, wenn Ausreißer das Ergebnis massiv verzerren (z.B. wenn ein oder zwei ältere Nutzer mit [unrealistisch?] hoher Nutzungsdauer in ihrer Zufallstichprobe gelandet sind).\n\n\n3.2.7 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regressionsanalyse prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: Internetznutzung), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (Internetnutzung) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n3.2.8 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nmodel\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nCoefficients:\n(Intercept)        alter  \n    349.542       -2.425  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-283.16 -154.26  -76.16   75.81 1099.64 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  349.542     57.337   6.096 0.0000000215 ***\nalter         -2.425      1.285  -1.888        0.062 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 230.6 on 98 degrees of freedom\nMultiple R-squared:  0.03509,   Adjusted R-squared:  0.02524 \nF-statistic: 3.564 on 1 and 98 DF,  p-value: 0.06201\n\n\n\n\n3.2.9 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängige Variable “alter” zu erklären.\n\n3.2.9.1 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n3.2.9.2 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n3.2.9.3 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n3.2.9.4 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n3.2.9.5 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n3.2.9.6 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (14,7) Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n3.2.9.7 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n3.2.9.8 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\n3.2.10 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (z.B. -4.642) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Die UV beeinflusst die AV, R2 = .24, F(1, 116) = 4.71, p = .003.\n\n\n\n\n\n3.2.11 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (Internetnutzung) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\npredict.lm(model, data.frame(alter = 25))\n\n       1 \n288.9069 \n\npredict.lm(model, data.frame(alter = 75))\n\n       1 \n167.6368 \n\n\n\n\n3.2.12 Inhaltliche Interpretation\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von (306) Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von (74) Minuten auf. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(alter = c(25, 75)))\n\n       1        2 \n288.9069 167.6368 \n\n\n\n\n3.2.13 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable Internetkonsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen Internetkonsum von (X) Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\nfitted(model) \n\n       1        2        3        4        5        6        7        8 \n225.8464 264.6529 233.1226 213.7194 194.3162 252.5258 305.8847 298.6085 \n       9       10       11       12       13       14       15       16 \n264.6529 308.3101 296.1831 288.9069 240.3988 298.6085 237.9734 298.6085 \n      17       18       19       20       21       22       23       24 \n223.4210 220.9956 308.3101 267.0783 254.9512 194.3162 269.5037 284.0561 \n      25       26       27       28       29       30       31       32 \n296.1831 228.2718 279.2053 172.4876 296.1831 279.2053 199.1670 267.0783 \n      33       34       35       36       37       38       39       40 \n276.7799 170.0622 199.1670 160.3606 179.7638 293.7577 235.5480 174.9130 \n      41       42       43       44       45       46       47       48 \n301.0339 174.9130 257.3766 170.0622 291.3323 177.3384 242.8242 305.8847 \n      49       50       51       52       53       54       55       56 \n177.3384 293.7577 279.2053 213.7194 240.3988 279.2053 247.6750 235.5480 \n      57       58       59       60       61       62       63       64 \n303.4593 298.6085 240.3988 235.5480 286.4815 204.0178 240.3988 303.4593 \n      65       66       67       68       69       70       71       72 \n301.0339 233.1226 281.6307 308.3101 196.7416 264.6529 279.2053 220.9956 \n      73       74       75       76       77       78       79       80 \n274.3545 313.1609 225.8464 284.0561 216.1448 279.2053 271.9291 281.6307 \n      81       82       83       84       85       86       87       88 \n267.0783 206.4432 281.6307 233.1226 267.0783 213.7194 271.9291 308.3101 \n      89       90       91       92       93       94       95       96 \n271.9291 303.4593 179.7638 189.4654 245.2496 201.5924 296.1831 279.2053 \n      97       98       99      100 \n160.3606 182.1892 213.7194 286.4815 \n\n\nNun haben wir aber im Rahmen unserer Befragung die Internetnutzung der Befragten aber ja schon erhoben. Wozu dient das dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nresiduals.lm(model)\n\n           1            2            3            4            5            6 \n-105.8464262  -24.6528543 -113.1226315 -153.7194174 -164.3162034  -72.5258455 \n           7            8            9           10           11           12 \n -65.8846842 -208.6084789   95.3471457  171.6899141    3.8169229  251.0931281 \n          13           14           15           16           17           18 \n 359.6011633 -178.6084789 -227.9734350 -148.6084789  -43.4210244  -40.9956227 \n          19           20           21           22           23           24 \n 171.6899141  -27.0782560  -74.9512473 -164.3162034  450.4963422  -44.0560683 \n          25           26           27           28           29           30 \n   3.8169229 -168.2718279   80.7947352  -82.4875875 -176.1830771  439.7947352 \n          31           32           33           34           35           36 \n -79.1670069  -87.0782560   23.2201369   39.9378142 -139.1670069 -115.3605788 \n          37           38           39           40           41           42 \n -89.7637928  -23.7576754 -115.5480332  -84.9129893  178.9661194  -54.9129893 \n          43           44           45           46           47           48 \n -77.3766490  129.9378142  -51.3322736  -87.3383911 -122.8242385 -155.8846842 \n          49           50           51           52           53           54 \n -87.3383911   66.2423246  320.7947352   86.2805826 -180.3988367  -99.2052648 \n          55           56           57           58           59           60 \n-127.6750420 -205.5480332  446.5407176 -118.6084789   -0.3988367 -205.5480332 \n          61           62           63           64           65           66 \n-166.4814701  155.9821896 -180.3988367  716.5407176 -181.0338806  -53.1226315 \n          67           68           69           70           71           72 \n-161.6306666 -158.3100859  -46.7416051 -204.6528543 -159.2052648  -40.9956227 \n          73           74           75           76           77           78 \n 625.6455387 -283.1608894   74.1535738 -104.0560683 -156.1448192  -99.2052648 \n          79           80           81           82           83           84 \n -91.9290596 -161.6306666 -147.0782560  213.5567879 -221.6306666 -173.1226315 \n          85           86           87           88           89           90 \n  92.9217440  326.2805826   88.0709404  291.6899141 -181.9290596 -183.4592824 \n          91           92           93           94           95           96 \n -29.7637928   20.5346002  714.7503598 -141.5924086  183.8169229  -99.2052648 \n          97           98           99          100 \n1099.6394212  -62.1891946  146.2805826   13.5185299 \n\n\n\n\n3.2.14 Inhaltliche Interpretation\nFür unseren Fall Nummer 3 beträgt die Abweichung der Prognose von der Beobachtung (121) Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 15 Prozent nicht besonders groß ist.\n\n\n3.2.15 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten_mod$vorhersage &lt;- predict(model) \ndaten_mod$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n  geom_segment(aes(xend = alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Internetnutzung\") + # Titel\n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\") # Achsen-Beschriftung\n\n\n\n\n\n\n3.2.16 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-283.16 -154.26  -76.16   75.81 1099.64 \n\nCoefficients:\n            Estimate Standardized Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 349.5419           NA    57.3369   6.096 0.0000000215 ***\nalter        -2.4254      -0.1873     1.2848  -1.888        0.062 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 230.6 on 98 degrees of freedom\nMultiple R-squared:  0.03509,   Adjusted R-squared:  0.02524 \nF-statistic: 3.564 on 1 and 98 DF,  p-value: 0.06201\n\n\n\n\n3.2.17 Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   350.         NA         57.3       6.10 0.0000000215\n2 alter          -2.43       -0.187      1.28     -1.89 0.0620      \n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0351        0.0252  231.      3.56  0.0620     1  -685. 1376. 1384.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model)\n\n# A tibble: 100 × 8\n   internetnutzung alter .fitted .resid   .hat .sigma   .cooksd .std.resid\n             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1             120    51    226. -106.  0.0132   232. 0.00143       -0.462\n 2             240    35    265.  -24.7 0.0111   232. 0.0000647     -0.108\n 3             120    48    233. -113.  0.0116   231. 0.00143       -0.493\n 4              60    56    214. -154.  0.0171   231. 0.00394       -0.672\n 5              30    64    194. -164.  0.0266   231. 0.00713       -0.722\n 6             180    40    253.  -72.5 0.0100   232. 0.000506      -0.316\n 7             240    18    306.  -65.9 0.0262   232. 0.00113       -0.290\n 8              90    21    299. -209.  0.0222   231. 0.00952       -0.915\n 9             360    35    265.   95.3 0.0111   232. 0.000967       0.416\n10             480    17    308.  172.  0.0277   231. 0.00811        0.755\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "href": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden",
    "text": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.4 Prüfung der Voraussetzungen",
    "text": "3.4 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "href": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.5 Berechnung und Interpretation einer einfachen linearen Regression",
    "text": "3.5 Berechnung und Interpretation einer einfachen linearen Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "href": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.1 t-Test für unabhängige Stichproben",
    "text": "2.1 t-Test für unabhängige Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#mann-whitney",
    "href": "Skript_6.3.html#mann-whitney",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.2 Mann-Whitney",
    "text": "2.2 Mann-Whitney"
  },
  {
    "objectID": "Skript_6.3.html#die-varianzanalyse",
    "href": "Skript_6.3.html#die-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.3 Die Varianzanalyse",
    "text": "2.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n2.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n2.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n2.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n2.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n2.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n2.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.3.html#kruskal-wallis",
    "href": "Skript_6.3.html#kruskal-wallis",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.4 Kruskal Wallis",
    "text": "2.4 Kruskal Wallis"
  },
  {
    "objectID": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.5 Mehrfaktorielle Varianzanalyse",
    "text": "2.5 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n2.5.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n2.5.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "href": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.1 t-Test für verbundene Stichproben",
    "text": "3.1 t-Test für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#wilcoxon",
    "href": "Skript_6.3.html#wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.2 Wilcoxon",
    "text": "3.2 Wilcoxon"
  },
  {
    "objectID": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "href": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.2 Multiple Regression mit Dummy-Codierung",
    "text": "1.2 Multiple Regression mit Dummy-Codierung\n\n1.2.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Geschlecht und Internetnutzung\nNun wollen wir noch Geschlecht (gndr) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei Gender haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist! Es handelt sich vielmehr um eine kategoriale Variable. Wie Sie schon gelernt haben, können Sie diese mit einem “Trick” ebenfalls in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Wir wollen uns hier mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl erst einmal umcodieren.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n1.2.2 Dummy Codierung der Variable Gender\n\ndaten_mod2 &lt;- daten_mod %&gt;%\nmutate(gender_r  = recode(gender, 'Male'='0', 'Female'='1')) %&gt;% # Recodierung der Var Gender zur Dummy-Variable\n  mutate(gender_r = as.numeric(as.character(gender_r))) # Variable als numerischen Wert behandeln\ndaten_mod2\n\n# A tibble: 100 × 167\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001263 GB    Female    72 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2  10006488 DE    Female    42 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 3  10000260 DE    Male      55 &lt;NA&gt;                     Mittle… Kein H… Abgesc…\n 4      1147 FR    Male      44 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      2089 FR    Male      72 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10000032 DE    Male      30 None of these (NEVER ma… Refusal Kein H… Refusal\n 7      1766 FR    Male      45 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8 100000390 GB    Male      63 Legally divorced/civil … &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 9 100000483 GB    Female    55 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10      2133 FR    Male      58 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n# ℹ 90 more rows\n# ℹ 159 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\ntable(daten_mod2$gender_r)\n\n\n 0  1 \n49 51 \n\nsummary(daten_mod2$gender_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    1.00    0.51    1.00    1.00 \n\nclass(daten_mod2$gender_r)\n\n[1] \"numeric\"\n\n\n\n\n1.2.3 Regressionsmodell zum Zusammenhang von Alter, Nachrichtenrezeptiion, Geschlecht und Internetnutzung spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable gender_r ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel_m2 &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption + gender_r, data = daten_mod2)\nsummary(lm.beta(model_m2))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption + \n    gender_r, data = daten_mod2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-233.88 -120.66  -42.97   23.33  689.97 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     268.1371           NA    58.5986   4.576\nalter                            -2.1276      -0.1939     1.0753  -1.979\npolitische_Nachrichtenrezeption   0.1925       0.1379     0.1367   1.408\ngender_r                         54.3519       0.1444    36.9466   1.471\n                                 Pr(&gt;|t|)    \n(Intercept)                     0.0000142 ***\nalter                              0.0507 .  \npolitische_Nachrichtenrezeption    0.1625    \ngender_r                           0.1445    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 184 on 96 degrees of freedom\nMultiple R-squared:  0.08188,   Adjusted R-squared:  0.05318 \nF-statistic: 2.854 on 3 and 96 DF,  p-value: 0.04125\n\n\n\n\n1.2.4 Inhaltliche Interpretation\nGender hat hier keinen signifikanten Einfluss auf den Internetkonsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy- Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (9,8) Minuten geringeren Internetkonsum als Männer (wobei dieser Befund statistisch ja (nicht) signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.5 Vorhersage des multivariaten Modells für die tägliche Internetnutzung durch Alter, Nachrichtenrezeption und Geschlecht\n\npredict.lm(model_m2, data.frame(alter = c(25, 75), gender_r = c(0,1), politische_Nachrichtenrezeption = c(5, 10)))\n\n       1        2 \n215.9085 164.8410 \n\n\n\n\n1.2.6 Inhaltliche Interpretation\nEine männliche Person, die 25 Jahre alt ist und 5 Minuten pro Tag politische Nachrichten rezipiert hat, einen prognostizierten Internetkonsum von 293 Minuten. Eine weibliche Person, die 75 Jahre alt ist und ebenfalls 5 Minuten pro Tag politische Nachrichten rezipiert, hat einen prognostizierten Internetkonsum von 41 Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.7 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie diese ausführen, haben Sie ja heute zu Anfang der Sitzung gelernt (siehe oben). Zusätzlich müssen Sie bei einer multiplen Regresssion noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model_m2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                            Term  VIF       VIF 95% CI Increased SE Tolerance\n                           alter 1.00 [1.00,      Inf]         1.00      1.00\n politische_Nachrichtenrezeption 1.00 [1.00,      Inf]         1.00      1.00\n                        gender_r 1.01 [1.00, 1.59e+09]         1.00      0.99\n Tolerance 95% CI\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n\n\n\n\n1.2.8 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5; wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "href": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse",
    "text": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse\n\ndaten_mod3 &lt;- daten %&gt;% \n  select(agea, netustm, cntry) %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm,\n         land = cntry) %&gt;% \n  filter(land %in% c(\"DE\", \"FR\", \"IS\", \"PL\")) %&gt;% \n  drop_na() %&gt;% \n  group_by(land) %&gt;% \n  slice_sample(n = 100) %&gt;% \n  ungroup()\ndaten_mod3\n\n# A tibble: 200 × 3\n   alter internetnutzung land \n   &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;\n 1    71              90 DE   \n 2    29             180 DE   \n 3    50             720 DE   \n 4    50             360 DE   \n 5    41             600 DE   \n 6    19             360 DE   \n 7    52             370 DE   \n 8    34              60 DE   \n 9    26              90 DE   \n10    18             120 DE   \n# ℹ 190 more rows\n\n\n\nggplot(daten_mod3, aes(alter, internetnutzung, colour = land)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(method = lm, formula = \"y ~ x\", se = FALSE) + \n  scale_colour_brewer(palette = \"Set1\") + \n  ggtitle(\"Lineare Regression für Alter und Internetnutzung (vier Länder)\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")"
  },
  {
    "objectID": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "href": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.3 Literatur und Beispiele aus der Praxis",
    "text": "3.3 Literatur und Beispiele aus der Praxis\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link\n\n:::"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Autoren.html",
    "href": "Autoren.html",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Wir möchten uns einmal kurz vorstellen…"
  },
  {
    "objectID": "Autoren.html#patrick-zerrer",
    "href": "Autoren.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "Patrick Zerrer",
    "text": "Patrick Zerrer\n\n\n\nPatrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften”. Das Masterstudium „Öffentliche Kommunikation” an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action” mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik” mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Hintergrund.html",
    "href": "Hintergrund.html",
    "title": "Lernen durch Praxis - Der Weg zu fundierten statistischen Kenntnissen",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nUnser Leitbild ist es, Ihnen einen Kurs anzubieten, der auf der Verschränkung von praktischem Lernen, grundlegenden Kenntnissen und eigenständigem Forschen basiert. Unser Ziel ist es, Ihnen die Werkzeuge und das Verständnis zu vermitteln, um quantitative Forschungsdesigns mit R erfolgreich umzusetzen.\nWir beginnen mit einer soliden Grundlage, in der wir Ihnen die wesentlichen Konzepte und Techniken der quantitativen Forschung vermitteln. Wir legen Wert darauf, dass Sie die grundlegenden Prinzipien verstehen, bevor wir Sie in die Praxis entlassen. Sie werden lernen, wie Sie Daten in R importieren, explorieren und visualisieren können, um ein tieferes Verständnis für Ihre Forschungsfragen zu gewinnen.\nNachdem Sie diese Grundlagen erworben haben, gehen wir einen Schritt weiter und bieten Ihnen die Möglichkeit, Ihr eigenes Forschungsprojekt durchzuführen. Unter Anleitung unserer erfahrenen Tutoren werden Sie ein eigenständiges Projekt entwickeln, bei dem Sie Ihre neu erlernten statistischen Kenntnisse anwenden können. Sie werden Schritt für Schritt lernen, wie Sie Hypothesen aufstellen, Daten sammeln, analysieren und interpretieren. Durch dieses praktische Erleben vertiefen Sie nicht nur Ihr Verständnis, sondern gewinnen auch wertvolle Erfahrungen im Bereich der quantitativen Forschung.\nUnser Kurs legt großen Wert darauf, dass Sie nicht nur theoretisches Wissen erlangen, sondern dieses Wissen in die Praxis umsetzen können. Wir sind der festen Überzeugung, dass das eigenständige Durchführen eines Forschungsprojekts Ihnen nicht nur ein tieferes Verständnis für statistische Methoden gibt, sondern auch Ihre analytischen und Problemlösungsfähigkeiten stärkt. Unsere Tutoren stehen Ihnen dabei zur Seite und bieten Ihnen individuelle Unterstützung, um sicherzustellen, dass Sie Ihr volles Potenzial entfalten können.\nWillkommen zu einer spannenden Lernreise, bei der Sie nicht nur statistische Kenntnisse erwerben, sondern auch die Fähigkeit entwickeln, Ihr Wissen auf praktische Weise anzuwenden. Wir sind davon überzeugt, dass Sie durch dieses ganzheitliche Lernkonzept Ihre Ziele erreichen und in der Welt der quantitativen Forschung erfolgreich sein werden.\n(Platzhalter generiert durch ChatGDP)"
  },
  {
    "objectID": "Hintergrund.html#patrick-zerrer",
    "href": "Hintergrund.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Patrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften\". Das Masterstudium „Öffentliche Kommunikation\" an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action\" mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik\" mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_5.2.html#laden-der-nötigen-pakete",
    "href": "Skript_5.2.html#laden-der-nötigen-pakete",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "library(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.2.html#laden-des-datensatzes",
    "href": "Skript_5.2.html#laden-des-datensatzes",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "daten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "href": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "summary(ess_wirksamkeit)\n\n    psppsgva        actrolga        psppipla        cptppola    \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.211   Mean   :2.085   Mean   :2.154   Mean   :2.133  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n\n#Alternativ kann describe() verwendet werden. Describe() ist eine Funktion des psych Pakets, welche die wichtigsten Streu- und Lageparameter für Variablen angiebt"
  },
  {
    "objectID": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "href": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Hauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben."
  },
  {
    "objectID": "Skript_5.2.html#quellen-für-das-script",
    "href": "Skript_5.2.html#quellen-für-das-script",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Stephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html#laden-der-nötigen-pakete",
    "href": "Skript_5.3.html#laden-der-nötigen-pakete",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Laden der nötigen Pakete",
    "text": "1.1 Laden der nötigen Pakete\n\nlibrary(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.3.html#laden-des-datensatzes",
    "href": "Skript_5.3.html#laden-des-datensatzes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Laden des Datensatzes",
    "text": "1.2 Laden des Datensatzes\n\ndaten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Reliabilität von Skalen",
    "section": "1.3 Teildatensatz mit den benötigten Index-Variablen",
    "text": "1.3 Teildatensatz mit den benötigten Index-Variablen\nWir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "href": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "title": "Reliabilität von Skalen",
    "section": "1.4 Berechnen eines Ungewichteten Summenindex",
    "text": "1.4 Berechnen eines Ungewichteten Summenindex\nWir haben statistisch getestet, ob wir einen Index aus den genannten Variablen bilden können.\nWir berechnen nur die einfachste Form eines Index, den ungewichteten Summenindex. Das bedeutet, dass wir die Werte pro befragter Person für die genannten Variablen aufsummieren und KEINE Gewichtungen einbauen. Eine Gewichtung wäre bspw. wenn wir eine Variable doppelt zählen würden.\n\nindex_wirksamkeit &lt;- ess_wirksamkeit %&gt;% \n  mutate(index_trust = psppsgva + actrolga + psppipla + cptppola)\nhead(index_wirksamkeit)\n\n# A tibble: 6 × 5\n  psppsgva actrolga psppipla cptppola index_trust\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;\n1        2        2        2        3           9\n2        1        3        2        3           9\n3        2        2        2        2           8\n4        3        2        3        4          12\n5        3        3        4        1          11\n6        2        2        2        2           8\n\nsummary(index_wirksamkeit)\n\n    psppsgva        actrolga        psppipla        cptppola    \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :2.000   Median :2.000  \n Mean   :2.211   Mean   :2.085   Mean   :2.154   Mean   :2.133  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.000   Max.   :5.000  \n  index_trust    \n Min.   : 4.000  \n 1st Qu.: 6.000  \n Median : 8.000  \n Mean   : 8.583  \n 3rd Qu.:11.000  \n Max.   :20.000  \n\n\nWir können uns noch die deskriptive Statistik für den Index anschauen, diese ist wichtig um den berechneten Index korrekt zu interpretieren. Mit dem Index können wir in den folgenden Sitzungen weiterrechnen."
  },
  {
    "objectID": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "href": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "title": "Reliabilität von Skalen",
    "section": "1.5 Reliabilität des Indizes berechnen",
    "text": "1.5 Reliabilität des Indizes berechnen\nBevor wir diesen Index einsetzen können, müssen wir zunächst noch checken, ob die Variablen auch inhaltlich zusammenpassen. Dazu ermitteln wir Crombach’s Alpha als Maß der Skalenreliabilität:\n\nindex_wirksamkeit %&gt;%\n  select(psppsgva:cptppola) %&gt;%\n  psych::alpha(check.keys=TRUE)\n\n\nReliability analysis   \nCall: psych::alpha(x = ., check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r\n      0.78      0.78    0.79      0.48 3.6 0.0018  2.1 0.78     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.78  0.78  0.79\nDuhachek  0.78  0.78  0.79\n\n Reliability if an item is dropped:\n         raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r\npsppsgva      0.76      0.76    0.71      0.51 3.2   0.0020 0.024  0.43\nactrolga      0.72      0.72    0.67      0.47 2.6   0.0025 0.030  0.42\npsppipla      0.72      0.71    0.67      0.45 2.5   0.0024 0.044  0.34\ncptppola      0.72      0.73    0.68      0.48 2.7   0.0024 0.027  0.43\n\n Item statistics \n             n raw.r std.r r.cor r.drop mean   sd\npsppsgva 42132  0.73  0.74  0.63   0.52  2.2 0.94\nactrolga 42132  0.80  0.79  0.71   0.62  2.1 1.05\npsppipla 42132  0.79  0.80  0.72   0.62  2.2 0.94\ncptppola 42132  0.80  0.78  0.70   0.60  2.1 1.05\n\nNon missing response frequency for each item\n            1    2    3    4    5 miss\npsppsgva 0.25 0.38 0.29 0.07 0.01    0\nactrolga 0.36 0.32 0.22 0.07 0.03    0\npsppipla 0.28 0.38 0.27 0.07 0.01    0\ncptppola 0.34 0.33 0.23 0.08 0.03    0"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "1.6 Interpretation von Crombach’s Alpha",
    "text": "1.6 Interpretation von Crombach’s Alpha\nZur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\nWerte &lt;0,6: nicht akzeptabel 0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert 0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert 0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "href": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.7 Interpretation des Wirksamkeit-Indizes",
    "text": "1.7 Interpretation des Wirksamkeit-Indizes\nDie Werte sind ein gutes Ergebnis. Die Items zeigen eine gute Inter-Item-Korrelation.\nWir können noch nachschauen, ob wir die Skalen-Reliabilität verbessern können, indem wir einzelne Items herauswerfen. Denn der Output von Cronbachs Alpha gibt uns auch hilfreiche Aufschlüsse darüber, welche Items man evtl. ausschließen kann, um Cronbachs Alpha bei ungenügender Höhe noch auf ein mindestens akzeptables Maß zu heben. Diese Information findet sich im Bereich “Reliability if an item is dropped”:. In unserem Fall wird die reliabitlitä aber noch schlechter - wir können nichts mehr verbessern."
  },
  {
    "objectID": "Skript_5.3.html#quellen-für-das-script",
    "href": "Skript_5.3.html#quellen-für-das-script",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Quellen für das Script",
    "text": "1.8 Quellen für das Script\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "href": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Video\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten."
  },
  {
    "objectID": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "href": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Die Faktorenanalyse bringt einige Voraussetzungen an die Daten mit sich, welche zu Beginn geprüft werden müssen. Zum Nachlesen bietet sich hier Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811 an.\nVoraussetzungen sind:\n\nStichprobengröße (in unserem Fall kein Problem)\nAusreichende Anzahl an Variablen (meist 4 oder mehr pro Faktor/Dimension)\nDie Variablen sind intervallskaliert (In der Praxis werden jedoch oft auch ordinalskalierte Variablen verwendet.)\n\nDiese Punkte sind bei uns alle gegeben.\nAnmerkung: Wir können auch eine Faktorenanalyse mit nicht-kontinuierlichen Daten rechnen (bspw. dichotome Variablen). Hier sollten wir sollten wir die Korrelationsmatrix aus polychorischen Korrelationskoeffizienten konstruieren (diese können mit der Funktion polychor() aus dem Paket polycor berechnet werden\nZunächst schauen wir uns einmal die Korrelationskoeffizienten für einige Variablen an.\n\n# Auswählen einiger zusätzlichen Variablen zum zeigen der Korrelationstabelle\n\nbeispiel = daten %&gt;% \n  select(netusoft, \n         trstplt, \n         trstep, \n         psppsgva, \n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(across(netusoft:cptppola, as.numeric)) %&gt;% # mit across kann die gleiche Funktion - in diesem Fall as.numeric() - mehrere Variablen angewandt werden\n  na.omit()\n\n# Bessere Darstellung der Tabelle mit htmlTable und gerundeten Werten für bessere Lesbarkeit\n\nlibrary(htmlTable) # Falls das Paket noch nicht installiert ist, muss es mit install.packages(\"htmlTable\") installiert werden\nhtmlTable(round(cor(beispiel), digits = 3))\n\n\n\n\n\nnetusoft\ntrstplt\ntrstep\npsppsgva\nactrolga\npsppipla\ncptppola\n\n\n\n\nnetusoft\n1\n0.088\n0.128\n0.176\n0.267\n0.224\n0.25\n\n\ntrstplt\n0.088\n1\n0.574\n0.424\n0.187\n0.421\n0.178\n\n\ntrstep\n0.128\n0.574\n1\n0.294\n0.132\n0.291\n0.122\n\n\npsppsgva\n0.176\n0.424\n0.294\n1\n0.329\n0.655\n0.312\n\n\nactrolga\n0.267\n0.187\n0.132\n0.329\n1\n0.422\n0.688\n\n\npsppipla\n0.224\n0.421\n0.291\n0.655\n0.422\n1\n0.408\n\n\ncptppola\n0.25\n0.178\n0.122\n0.312\n0.688\n0.408\n1\n\n\n\n\n\nNoch zu prüfen ist die Korrelation der Items miteinander, hierfür nehmen wir den Bartlett Test.\n\ncortest.bartlett(ess_wirksamkeit)\n\nR was not square, finding R from data\n\n\n$chisq\n[1] 61775.54\n\n$p.value\n[1] 0\n\n$df\n[1] 6\n\n\nWenn der p-Wert signifikant ist (ist hier der Fall) korrelieren die Items miteinandern. Das ist gewünscht.\nAnsonsten prüfen wir das Kaiser-Meyer-Olkin-Kriterium (KMO) und die Measure of Sampling Adequacy (MSA) anschauen.\n\nKMO(ess_wirksamkeit)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = ess_wirksamkeit)\nOverall MSA =  0.65\nMSA for each item = \npsppsgva actrolga psppipla cptppola \n    0.65     0.65     0.67     0.65 \n\n\nDie Werte für die MSA Werte sollten jeweils größer als 0.5 sein. Ist hier der Fall. Falls ein Item unter 0.5 liegen sollte würde dieses ausgeschlossen werden.\nAls nächster Schritt möchten wir Wissen wieviele Faktoren wir aus den Daten “ziehen” möchten. Hierfür nutzen wir die Funktion nfactors()\n\nnfactors(ess_wirksamkeit, rotate = \"varimax\", fm=\"MLE\")\n\n\n\n\n\nNumber of factors\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.78  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.94  with  3  factors\nThe Velicer MAP achieves a minimum of 0.24  with  1  factors \nEmpirical BIC achieves a minimum of  12803.87  with  1  factors\nSample Size adjusted BIC achieves a minimum of  15644.55  with  1  factors\n\nStatistics by number of factors \n  vss1 vss2  map dof chisq prob sqresid  fit RMSEA   BIC SABIC complex  eChisq\n1 0.78 0.00 0.24   2 15659    0    1.54 0.78  0.43 15638 15645     1.0 1.3e+04\n2 0.75 0.94 0.33  -1     0   NA    0.41 0.94    NA    NA    NA     1.2 2.0e-13\n3 0.75 0.94 1.00  -3     0   NA    0.41 0.94    NA    NA    NA     1.2 6.0e-18\n4 0.68 0.89   NA  -4  3079   NA    0.74 0.89    NA    NA    NA     1.3 1.6e+03\n     SRMR eCRMS  eBIC\n1 1.6e-01  0.28 12804\n2 6.3e-10    NA    NA\n3 3.4e-12    NA    NA\n4 5.7e-02    NA    NA\n\n#?nfactors \n\nHier wählen wir zunächst den R Console Output aus und schauen auf den MAP Test, welcher uns hier einen Faktor/Dimension vorschlägt. Die Funktion gibt uns noch eine Reihe anderer Werte aus, welche wir jetzt aber nicht im Detail anschauen. Eine genauere Beschreibung der Werte kann mit ?nfactors aufgerufen werden.\nDieser Schritt ist sehr wichtig. Wir müssen im nächsten Schritt die Anzahl der Faktoren angeben. Hierfür müssen wir uns zunächst auf Grundlage der Statistischen Tests UND der theoretischen Überlegungen auf eine Anzahl von Faktoren bzw. Dimensionen festgelegt haben.\nIn unserem Beispiel gehen wir testweise von einem Faktoren aus.\nWir legen das Modell an mit folgenden Parametern: Maximum-Likelihood (ML) 1 Faktor Varimax-Rotation –&gt; Mit einer Rotation kann man die Interpretation der Faktoren verbessern, hier gibt es verschiedene Möglichkeiten “varimax” ist eine Option.\n\nfit &lt;- factanal(ess_wirksamkeit, 1, rotation = \"varimax\")\n\n# Anzeigen der Ergebnisse mit 2 Nachkommastellen und dem Ausblenden von Faktorladungen die kleiner als 0.3 sind\n\nprint(fit, digits = 2, cutoff = .3)\n\n\nCall:\nfactanal(x = ess_wirksamkeit, factors = 1, rotation = \"varimax\")\n\nUniquenesses:\npsppsgva actrolga psppipla cptppola \n    0.75     0.33     0.65     0.36 \n\nLoadings:\n         Factor1\npsppsgva 0.50   \nactrolga 0.82   \npsppipla 0.59   \ncptppola 0.80   \n\n               Factor1\nSS loadings       1.90\nProportion Var    0.47\n\nTest of the hypothesis that 1 factor is sufficient.\nThe chi square statistic is 15659.49 on 2 degrees of freedom.\nThe p-value is 0 \n\n\nBeim Output ist das wichtigste die Faktorladungen. Im idealfall laden die Variablen nur jeweils auf einen Faktor. Wir haben bereits geringe Faktorladungen bereits aus der Darstellung ausgeschlossen.\nHier laden alle Variablen mit hohen Werten auf den Faktor1. Wir können also davon ausgehen, dass wir diese Variablen zu einem Index verrechnen können.\nJetzt schauen wir uns doch noch kurz die Uniquenes bzw. Kommunalitäten an. Die gibt uns an wieviel Varianz von den jeweiligen Variablen durch die Faktoren erklärt wird.\n\n1 - fit$uniquenesses\n\n psppsgva  actrolga  psppipla  cptppola \n0.2456154 0.6655900 0.3458397 0.6386451 \n\n\nDie Werte geben uns eine Idee davon, wie wichtig die Variablen für die erzielte Lösung sind."
  }
]