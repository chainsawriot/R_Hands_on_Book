[
  {
    "objectID": "Skript_7.3.html",
    "href": "Skript_7.3.html",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei mehr als zwei Variablen"
  },
  {
    "objectID": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Notebook gehen wir (wie angekündigt) zunächst näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Dann lernen wir die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren.\n\n1.1.1 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ESS8_vier_laender-Datensatzes, den wir entsprechend einlesen:\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\")\n#install.packages(\"lmtest\")\n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n#install.packages(\"sandwich\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\nlibrary(lmtest)\n\nWarning: Paket 'lmtest' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: zoo\n\n\nWarning: Paket 'zoo' wurde unter R Version 4.3.1 erstellt\n\n\n\nAttache Paket: 'zoo'\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\nWarning: Paket 'sandwich' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n1.1.2 Data Management\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die Internetnutzung (netustm); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Rezeptionszeit von politischen Nachrichten (nwspol) sowie das Geschlecht der Befragten (gndr) an. Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl um.\nDann setzen wir den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifizieren wir wieder, auf welche Variablen sich der Befehl beziehen soll). Das modifizierte Datenset weisen wir einem neuen Datenobjekt zu: daten_mod2\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(internetnutzung = netustm,\n         alter = agea,\n         politische_Nachrichtenrezeption = nwspol,\n         gender = gndr) %&gt;% \n  drop_na(c(internetnutzung, alter, politische_Nachrichtenrezeption, gender)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001263 GB    Female    72 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2  10006488 DE    Female    42 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 3  10000260 DE    Male      55 &lt;NA&gt;                     Mittle… Kein H… Abgesc…\n 4      1147 FR    Male      44 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      2089 FR    Male      72 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10000032 DE    Male      30 None of these (NEVER ma… Refusal Kein H… Refusal\n 7      1766 FR    Male      45 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8 100000390 GB    Male      63 Legally divorced/civil … &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 9 100000483 GB    Female    55 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10      2133 FR    Male      58 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n# ℹ 90 more rows\n# ℹ 158 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\n\n1.1.3 Erinnerung: Einfache lineare Regression mit Alter als UV und Internetnutzung als AV mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nsummary(model) # klassischer Output mit relevanten Kennzahlen\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-222.39 -116.02  -56.96   35.95  719.76 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  313.600     54.435   5.761 0.0000000965 ***\nalter         -2.186      1.086  -2.013       0.0469 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 186.3 on 98 degrees of freedom\nMultiple R-squared:  0.0397,    Adjusted R-squared:  0.0299 \nF-statistic: 4.051 on 1 and 98 DF,  p-value: 0.04688\n\n#summary(lm.beta(model)) # klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Internetnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um “den Estimate-Wert” in Messeinheiten (hier: -4.296 Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit dem Teil A dieses Skripts, nämlich der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern Sie sich, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Sie haben Recht: Eigentlich würden wir erst die Voraussetzungen prüfen, dann das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen.\n\n\n1.1.4 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)\n\n\n1.1.5 TEIL A: Prüfung der Voraussetzungen einer Regressionsanalyse\n\n1.1.5.1 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt.\n\n\n1.1.5.2 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß – unabhängig wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde.\n\ncheck_heteroscedasticity(model)\n\nOK: Error variance appears to be homoscedastic (p = 0.154).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). In diesem Fall liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\n\n\n1.1.5.3 Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei höheren Werten erkennbar ist, weil wir einen leicht nach rechts geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\n\n\n1.1.5.4 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 313.60004   54.06636  5.8003 0.00000008113 ***\nalter        -2.18624    0.98757 -2.2137       0.02917 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!\n\n\n1.1.5.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.680).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0,588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!\n\n\n1.1.5.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.\n\n\n1.1.5.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, kann ich wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n1.1.5.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr coole Funktion, die mir eine visuelle Inspektion meiner Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion kann ich mir dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\n\ncheck_model(model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n\n1.1.6 TEIL B: Die multiple lineare Regression\n\n1.1.6.1 Anwendungsbereich der multiplen linearen Regression\nDie multiple lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen metrischen Variablen besteht. Die multiple lineare Regressionsanalyse hat das Ziel, eine abhängige Variable (y) mittels mehrerer unabhängigen Variablen (x1, x2, …) zu erklären. (Zur Erinnerung: Für nur eine x-Variable nutzen wir die einfache lineare Regression)\nMit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen den unabhängigen und der einen abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variablen vorhergesagt werden?\nDie multiple Regression entspricht in ihrer Analyslogik also der einfachen linearen Regression - nur dass sie mehr als eine unabhängige Variable berücksichtigt.\n\n\n1.1.6.2 Ziel der Analyse\nMit Hilfe der multiplen Regression wollen wir die Annahme prüfen, dass die Variablen Alter (agea) sowie die Rezeptionszeit von politischen Nachrichten (nwspol) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) haben bzw. diese erklären und vorhersagen können. Alle Variablen sind metrisch und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n1.1.6.3 Modell zum Zusammenhang von Alter, politischer Nachrichtenrezeption und Internetnutzung spezifizieren und anzeigen lassen\nDie Berechnung der multiplen Regression unterscheidet sich nicht stark von der Berechnung der einfachen linearen Regression. In die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) einfach die zusätzliche unabhängige Variable (politische_Nachrichtenrezeption) ein, indem wir sie mit einem + Zeichen anhängen:\n\nmodel_m &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption, data = daten_mod)\nsummary(lm.beta(model_m))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption, \n    data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-208.19 -111.70  -49.67   39.45  717.79 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     299.4215           NA    54.9302   5.451\nalter                            -2.2233      -0.2026     1.0798  -2.059\npolitische_Nachrichtenrezeption   0.2045       0.1466     0.1373   1.490\n                                   Pr(&gt;|t|)    \n(Intercept)                     0.000000381 ***\nalter                                0.0422 *  \npolitische_Nachrichtenrezeption      0.1395    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 185.1 on 97 degrees of freedom\nMultiple R-squared:  0.06118,   Adjusted R-squared:  0.04182 \nF-statistic:  3.16 on 2 and 97 DF,  p-value: 0.0468\n\n\n\n\n1.1.6.4 Interpretation des Outputs: Was sehen wir in der Regressionstabelle?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängigen Variablen “alter” und “politische_Nachrichtenrezeption” zu erklären.\n\n\n1.1.6.5 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n1.1.6.6 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n1.1.6.7 Standardized\nDiese Estimates sind die standardisierte b-Werte. Weil wir diese über die lm.beta-Funktion standardisiert haben, lassen sich die Koeefizienten auch bei unterschiedlicher Skalierung vergleichen.\n\n\n1.1.6.8 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n1.1.6.9 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n1.1.6.10 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n1.1.6.11 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter und politische Nachrichtenrezeption auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter und die politische Nachrichtenrezeption etwa 20 Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n1.1.6.12 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n1.1.6.13 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n1.1.6.14 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (-4.9169) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &lt; .05 statistisch signifikant. Diese Ergebnisse überraschen uns nicht: Den Einfluss des Alters haben wir ja letzte Woche schon überprüft. Mit der Erweiterung zur multiplen Regression können wir nun zusätzlich sagen, dass die politische Nachrichtenrezeption auch einen Einfluss auf die Internet-Nutzung hat, denn der Wert ist ebenfalls signifikant (p &lt; .05)! Die F-Statistik sagt uns zusätzlich, dass auch unser Gesamtmodell signifikant ist (p-value: 0.00001239, also &lt; .05).\n\n\n1.1.6.15 Erinnerung: Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model_m))\n\n# A tibble: 3 × 6\n  term                         estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;                           &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)                   299.          NA        54.9        5.45 3.81e-7\n2 alter                          -2.22        -0.203     1.08      -2.06 4.22e-2\n3 politische_Nachrichtenrezep…    0.205        0.147     0.137      1.49 1.40e-1\n\n\n\nglance(model_m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0612        0.0418  185.      3.16  0.0468     2  -662. 1333. 1343.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "href": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden",
    "text": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.3 Prüfung der Voraussetzungen",
    "text": "1.3 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "href": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.4 Berechnung und Interpretation einer multiplen Regression",
    "text": "1.4 Berechnung und Interpretation einer multiplen Regression"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Skript_1.1.html",
    "href": "Skript_1.1.html",
    "title": "Was ist R?",
    "section": "",
    "text": "1 Was ist R?\nEinführung in die Logik von R, R Studio Server und das R Environment Einführung in Markdown (Quarto?) Basics der Befehlssyntax in Base R und Tidyverse (im Vergleich), ab dann aber alles in dplyr Laden von Daten und Importieren von anderen Datenformaten Einführung in das Datenmanagement: Projekte und Ordnerstrukturen auf dem PC Öffnen von Datensätzen, Laden von Daten, Importieren von anderen Datensätzen Speichern von Daten aus R in verschiedenen Formaten (Rda, csv etc.) Installieren von Paketen und Laden von Librarys (p_load) Vorschau: Datentypen und deren Charakteristika."
  },
  {
    "objectID": "Skript_1.2.html",
    "href": "Skript_1.2.html",
    "title": "R-Studio",
    "section": "",
    "text": "1 Wie funktioniert R-Studio?"
  },
  {
    "objectID": "Skript_1.3.html",
    "href": "Skript_1.3.html",
    "title": "Relevante Begriffe",
    "section": "",
    "text": "1 Die Logik von Markdown und Quarto"
  },
  {
    "objectID": "Skript_2.1.html",
    "href": "Skript_2.1.html",
    "title": "Der Aufbau von Datensätzen",
    "section": "",
    "text": "1 Was ist ein Datensatz und wie ist dieser aufgebaut?\n\n\n2 Klassische Formen von Datensätzen in der Kommunikationswissenschaft"
  },
  {
    "objectID": "Skript_2.2.html",
    "href": "Skript_2.2.html",
    "title": "klassische Formen von Datensätzen",
    "section": "",
    "text": "1 Einlesen von Datensätzen und Praktische Tipps"
  },
  {
    "objectID": "Skript_2.3.html",
    "href": "Skript_2.3.html",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "1 Der ALLBUS"
  },
  {
    "objectID": "Skript_3.2.html",
    "href": "Skript_3.2.html",
    "title": "Datentypen und -strukturen",
    "section": "",
    "text": "1 Datentypen und -strukturen in R\nGrundsätzlicher Aufbau von einem Datensatz (Was ist ein Fall? Was ist eine Spalte? Was eine Zeile?) Überblick über die Daten erhalten Einführung in das Datenmanagement: Objekte festlegen und Variablen definieren Datentypen und deren Charakteristika Datentypen, Vektoren, Matrizen, und Data Frames in R Vorschau: Manipulation und Transformation von Daten jeweils alles mit dplyr"
  },
  {
    "objectID": "Skript_3.3.html",
    "href": "Skript_3.3.html",
    "title": "Selektion, Manipulation und Transformation",
    "section": "",
    "text": "1 Selektion, Manipulation und Transformation von Daten\nDas tidyverse-Universum Manipulation und Transformation von Daten Zufallsstichprobe ziehen Daten filtern/Fälle und Spalten auswählen; Subsets bilden aufgrund von konditionalen Bedingungen Gruppieren von Fällen (group_by)\nUmgang mit realen, nicht-sauberen Datensätzen an konkreten Beispielen (z.B. missing data; -99/-77, invertierte Items; “weiß nicht”; fehlende Faktoren) Variablen recodieren / rename (Neue) Variablen berechnen (z.B. mehrere Variablen in einem Index bzw. einer Skala zusammenfassen)"
  },
  {
    "objectID": "Skript_3.4.html",
    "href": "Skript_3.4.html",
    "title": "Tabellen und Grafiken in R",
    "section": "",
    "text": "1 Eine Einführung in GGPlot"
  },
  {
    "objectID": "Skript_4.1.html",
    "href": "Skript_4.1.html",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "1 Berechnung von einfachen Häufigkeiten\n\n\n2 Visuelle Darstellung von einfachen Häufigkeiten\n\n\n3 Interpretation von einfachen Häufigkeiten"
  },
  {
    "objectID": "Skript_4.2.html",
    "href": "Skript_4.2.html",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "",
    "text": "Berechnung von Lageparametern"
  },
  {
    "objectID": "Skript_4.2.html#berechnung",
    "href": "Skript_4.2.html#berechnung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung",
    "href": "Skript_4.2.html#visualisierung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation",
    "href": "Skript_4.2.html#interpretation",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-1",
    "href": "Skript_4.2.html#berechnung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-1",
    "href": "Skript_4.2.html#visualisierung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-1",
    "href": "Skript_4.2.html#interpretation-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-2",
    "href": "Skript_4.2.html#berechnung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-2",
    "href": "Skript_4.2.html#visualisierung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-2",
    "href": "Skript_4.2.html#interpretation-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html",
    "href": "Skript_4.3.html",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "",
    "text": "Berechnung und Interpretation von Streuungsmaßen"
  },
  {
    "objectID": "Skript_4.3.html#berechnung",
    "href": "Skript_4.3.html#berechnung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung",
    "href": "Skript_4.3.html#visualisierung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation",
    "href": "Skript_4.3.html#interpretation",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-1",
    "href": "Skript_4.3.html#berechnung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-1",
    "href": "Skript_4.3.html#visualisierung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-1",
    "href": "Skript_4.3.html#interpretation-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-2",
    "href": "Skript_4.3.html#berechnung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-2",
    "href": "Skript_4.3.html#visualisierung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-2",
    "href": "Skript_4.3.html#interpretation-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html",
    "href": "Skript_4.4.html",
    "title": "Verteilungen und deren Visualisierung",
    "section": "",
    "text": "Berechnung und Interpretation von Verteilungen"
  },
  {
    "objectID": "Skript_4.4.html#berechnung",
    "href": "Skript_4.4.html#berechnung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung",
    "href": "Skript_4.4.html#visualisierung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation",
    "href": "Skript_4.4.html#interpretation",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html#berechnung-1",
    "href": "Skript_4.4.html#berechnung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-1",
    "href": "Skript_4.4.html#visualisierung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-1",
    "href": "Skript_4.4.html#interpretation-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_5.1.html",
    "href": "Skript_5.1.html",
    "title": "Die Messung von latenten Variablen",
    "section": "",
    "text": "Eine überfüllte Stadt an der Oberfläche, Bild generiert von Midjourney\n\n\n\n1 Einführung in die Messung von latenten Variablen\nIn der Kommunikationswissenschaft, beziehungsweise in den Sozialwissenschaften allgemein, bezieht sich der Begriff latente Variable auf eine nicht direkt beobachtbare Eigenschaft oder einen nicht direkt messbaren Faktor, der sich jedoch durch mehrere beobachtbare Variablen (teilweise auch als Indikatoren bezeichnet) manifestiert. Latente Variablen sind theoretische Konstrukte, die nicht direkt gemessen werden können. Als Wissenschaftler:innen gehen wir allerdings davon aus, dass diese Konstrukte existieren und wir somit komplexe gesellschaftliche Phänomene mit der Hilfe von latenten Variablen besser verstehen können.\nWenn wir uns jetzt ganz konkret mit dem Vertrauen in gesellschaftliche Institutionen beschäftigen, ist Vertrauen eine latente Variable. Wir können Vertrauen in gesellschaftliche Institutionen nicht direkt beobachten, wir sehen beispielsweise einem Menschen nicht an, ob er oder sie dem Bundesverfassungsgericht als eine von mehreren gesellschaftlichen Institutionen stark oder nicht so stark vertraut. Wenn wir uns als Forscher:innen fragen, inwieweit die Menschen in Deutschland den gesellschaftlichen Institutionen vertrauen, müssen wir zunächst theoretisch klären was wir unter dem Vertrauen in gesellschaftliche Institutionen verstehen.\nDas Vertrauen in gesellschaftliche Institutionen ist ein abstraktes und komplexes theoretisches Konstrukt, das sich aus verschiedenen Bestandteilen zusammensetzt. Diese einzelnen Bestandteile (Vertrauen in das Bundesverfassungsgericht, Vertrauen in den Bundestag, etc.) können wir durch sogenannte Indikatoren beobachten und damit messbar machen (in einer Befragung wäre das bspw. die Frage nach dem Vertrauen in das Bundesverfassungsgericht). Auf der Grundlage dieser Indikatoren versuchen wir dann auf das Vertrauen in gesellschaftliche Institutionen zu schließen.\nLatente Variablen spielen eine wichtige Rolle in der statistischen Modellierung und Analyse, insbesondere in der Faktorenanalyse. Durch das Einbeziehen von latenten Variablen in die Analyse, können komplexe Beziehungen und Zusammenhänge zwischen verschiedenen Variablen besser verstanden und erklärt werden. Zusätzlich ist eine Reduktion von Komplexität möglich, indem mehrere beobachtbare Variablen (bzw. Indikatoren) in einer latenten Variable zusammengefasst werden, was zu einem besseren Verständnis der zugrunde liegenden Phänomene führen kann. Der Prozess des Zusammenfassens wird häufig auch als Indexbildung bezeichnet.\nDie Indexbildung umfasst drei relevante Schritte: (1) Zunächst muss ein theoretisches Konstrukt entwickelt werden. In unserem Fall würde das bedeuten, dass wir ein theoretisches Verständnis von Vertrauen in gesellschaftliche Institutionen entwickeln und uns klar werden was wir darunter verstehen. (2) Im nächsten Schritt müssten wir klären, ob sich dieses theoretisches Konstrukt in Form einer latenten Variable auch in unseren Daten finden lässt, hierfür werden wir eine explorative Faktorenanalyse durchführen. (3) Als letzten Schritt führen wir die eigentliche Indexbidlung durch. Hier berechnen wir eine neue Variable, den Index für Vertrauen in gesellschaftliche Institutionen und Überprüfen dessen Güte.\nVor diesem Hintergrund können wir einige Ziele für das Kapitel 5 aufstellen:\n\nWir möchten zunächst ein besseres Verständnis der latenten Variable Vertrauen in gesellschaftliche Institutionen entwickeln.\nDes Weiteren möchten wir unser Verständnis von Vertrauen in gesellschaftliche Institutionen empirisch anhand der Daten überprüfen, um zu klären, ob eine latente Variable messbar ist.\nWir möchten einen Index für das Vertrauen in gesellschaftliche Institutionen berechnen und dessen Qualität prüfen."
  },
  {
    "objectID": "Skript_5.2.html",
    "href": "Skript_5.2.html",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Betreten der zugrunde liegenden Struktur der Stadt, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_5.3.html",
    "href": "Skript_5.3.html",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Validieren, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_6.1.html",
    "href": "Skript_6.1.html",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "1 Einführung in die Bivariate Statistik und Mittelwertvergleiche"
  },
  {
    "objectID": "Skript_6.2.html",
    "href": "Skript_6.2.html",
    "title": "Bestimmen von Unterschieden in der Varianz mit Kreuztabellen und dem Chi-Quadrat Test",
    "section": "",
    "text": "Bestimmen von Unterschieden in der Varianz\n\n1 Kreuztabellen und Chi-Quadrat Test"
  },
  {
    "objectID": "Skript_6.3.html",
    "href": "Skript_6.3.html",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "",
    "text": "Bestimmen von Unterschieden in der zentralen Tendenz"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben",
    "text": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben\n\n1.1.1 t-Test für unabhängige Stichproben\n\n\n1.1.2 Mann-Whitney\n\n\n1.1.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n1.1.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n\nCode\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\n\nCode\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n\n1.1.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n1.1.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n\nCode\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n\nCode\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n\nCode\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n1.1.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\nCode\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n1.1.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\nCode\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\n\nCode\nprint(fit)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n1.1.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt.\n\n\n\n1.1.4 Kruskal Wallis\n\n\n1.1.5 mehrfaktorielle Varianzanalyse\n\n1.1.5.1 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\n\nCode\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\n\nCode\nprint(fit2)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n\n1.1.5.2 Post-Hoc Tests\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n1.1.5.3 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n\nCode\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\n\n\nCode\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben",
    "text": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben\n\n1.2.1 t-Test für verbundene Stichproben\n\n\n1.2.2 Wilcoxon"
  },
  {
    "objectID": "Skript_6.4.html",
    "href": "Skript_6.4.html",
    "title": "Proportionen Häufigkeiten mit dem Binomial Test und Pearson Chi-Quadrat",
    "section": "",
    "text": "Bestimmen von Unterschieden in Bezug auf Proportionen Häufigkeiten\n\n1 Binomial Test\n\n\n2 Pearson Chi-Quadarat"
  },
  {
    "objectID": "Skript_7.1.html",
    "href": "Skript_7.1.html",
    "title": "Einführung",
    "section": "",
    "text": "1 Einführung in das Überprüfen von Zusammenhängen\nWillkommen zu unserem Kapitel über die statistische Überprüfung von Zusammenhängen mit Hilfe von R. In der datengetriebenen Welt von heute ist es von großer Bedeutung, statistische Zusammenhänge zu verstehen und zu validieren. In diesem Kapitel werden wir Ihnen zeigen, wie Sie mithilfe von R verschiedene statistische Tests durchführen können, um Zusammenhänge zwischen Variablen zu untersuchen.\nDer erste Schritt besteht darin, die Daten in R zu importieren und zu explorieren. Wir werden Ihnen zeigen, wie Sie die Daten visualisieren und grundlegende statistische Kennzahlen berechnen können, um einen ersten Eindruck von den vorliegenden Zusammenhängen zu bekommen. Anschließend werden wir auf verschiedene statistische Tests eingehen, darunter den Korrelationstest, den Chi-Quadrat-Test und verschiedene Formen der Regression. Sie lernen, wie Sie diese Tests in R implementieren, die Ergebnisse interpretieren und fundierte Schlussfolgerungen ziehen können.\nDarüber hinaus werden wir auf wichtige Konzepte wie Signifikanzniveau, p-Wert und Konfidenzintervalle eingehen, um Ihnen ein solides Verständnis dafür zu vermitteln, wie statistische Zusammenhänge bewertet werden können. Durch die Anwendung dieser Methoden werden Sie in der Lage sein, Ihre Daten genau zu analysieren, potenzielle Zusammenhänge zu identifizieren und deren Bedeutung zu bewerten. Tauchen Sie ein in die spannende Welt der statistischen Überprüfung von Zusammenhängen mit R und erweitern Sie Ihr analytisches Toolkit!\n(Es handelt sich bei dem Text um einen Platzhalter, erstellt von chatGDP)"
  },
  {
    "objectID": "Skript_7.2.html",
    "href": "Skript_7.2.html",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei zwei Variablen"
  },
  {
    "objectID": "Skript_7.2.html#korrelation",
    "href": "Skript_7.2.html#korrelation",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.1 Korrelation",
    "text": "3.1 Korrelation"
  },
  {
    "objectID": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "href": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse",
    "text": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse\nIn diesem Notebook wird die einfache lineare Regression auf Grundlage der ESS-Daten vorgestellt. In der nächsten Sitzung gehen wir näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen die multiple lineare Regression kennen.\n\n\n\nPicture generated by Midjourney\n\n\n\n3.2.1 Anwendungsbereich der linearen Regression\nDie einfache lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen zwei metrischen Variablen besteht. Sie wird daher auch als bivariate Regression bezeichnet.\nZiel ist es, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?\n\n\n3.2.2 Vorbereitung und Laden der Daten\nZunächst laden wir die Pakete des tidyverse. Weiterhin laden wir das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt.\nDen Datensatz finet ihr hier.\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\") \n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n3.2.3 Ziel der Analyse\nMit Hilfe der Regression wollen wir die Annahme prüfen, dass das Alter (agea) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) hat. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n3.2.4 Data Management\nDamit der Output etwas nachvollziehbarer wird, benenne ich die Variablen mit dem rename-Befehl zunächst einmal um. Dann nutze ich den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifiziere ich, auf welche Variablen sich der Befehl beziehen soll). Das alles weise ich einem neuen Datenobjekt zu: daten_mod\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm) %&gt;% \n  drop_na(c(alter, internetnutzung)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gndr   alter marsts   edubde1 eduade2 eduade3 nwspol netusoft\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1       835 FR    Male      16 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 2  10006306 DE    Female    39 None of… Abitur… Diplom… Kein b…     25 Every d…\n 3  10008723 DE    Male      16 None of… (Noch)… Kein H… Kein b…     30 Most da…\n 4       231 SE    Female    80 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       270 Most da…\n 5 100003876 GB    Male      33 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 6       408 FR    Female    30 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 7  10009322 DE    Male      72 &lt;NA&gt;     Abitur… Diplom… Kein b…     90 Every d…\n 8 100003453 GB    Male      45 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 9      1126 SE    Female    60 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n10 100001390 GB    Female    77 Widowed… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        10 Every d…\n# ℹ 90 more rows\n# ℹ 156 more variables: internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;,\n#   pplhlp &lt;dbl&gt;, polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;,\n#   psppipla &lt;fct&gt;, cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;,\n#   trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;, trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;,\n#   vote &lt;fct&gt;, prtvede1 &lt;fct&gt;, prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;,\n#   wrkorg &lt;fct&gt;, badge &lt;fct&gt;, sgnptit &lt;fct&gt;, pbldmn &lt;fct&gt;, bctprd &lt;fct&gt;, …\n\n\n\n\n3.2.5 Prüfung der Voraussetzungen 1: Grafische Darstellung des Zusammenhangs der beiden Variablen, um die Annahme von Linearität zu prüfen\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und Internetnutzung. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Internetznutzung) und x (=Alter) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und Internetnutzung\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")\n\n\n\n\n\n\n3.2.6 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen schwachen negativen (aber linearen!) Zusammenhang zwischen Alter und Internetnutzung nahe: mit zunehmendem Alter sinkt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\nNicht wundern: Weil wir oben ein Zufallssample gezogen haben, sieht die Grafik bei Ihnen allen etwas anders aus. Sie kann dadurch auch so ausfallen, dass der lineare Zusammenhang nicht (gut) sichtbar ist – vor allem dann, wenn Ausreißer das Ergebnis massiv verzerren (z.B. wenn ein oder zwei ältere Nutzer mit [unrealistisch?] hoher Nutzungsdauer in ihrer Zufallstichprobe gelandet sind).\n\n\n3.2.7 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regressionsanalyse prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: Internetznutzung), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (Internetnutzung) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n3.2.8 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nmodel\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nCoefficients:\n(Intercept)        alter  \n    378.194       -3.982  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  378.194     59.747   6.330 0.0000000074 ***\nalter         -3.982      1.198  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.9 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängige Variable “alter” zu erklären.\n\n3.2.9.1 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n3.2.9.2 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n3.2.9.3 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n3.2.9.4 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n3.2.9.5 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n3.2.9.6 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (14,7) Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n3.2.9.7 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n3.2.9.8 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\n3.2.10 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (z.B. -4.642) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Die UV beeinflusst die AV, R2 = .24, F(1, 116) = 4.71, p = .003.\n\n\n\n\n\n3.2.11 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (Internetnutzung) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\npredict.lm(model, data.frame(alter = 25))\n\n       1 \n278.6328 \n\npredict.lm(model, data.frame(alter = 75))\n\n       1 \n79.51086 \n\n\n\n\n3.2.12 Inhaltliche Interpretation\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von (306) Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von (74) Minuten auf. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(alter = c(25, 75)))\n\n        1         2 \n278.63275  79.51086 \n\n\n\n\n3.2.13 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable Internetkonsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen Internetkonsum von (X) Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\nfitted(model) \n\n        1         2         3         4         5         6         7         8 \n314.47470 222.87862 314.47470  59.59867 246.77325 258.72057  91.45818 198.98400 \n        9        10        11        12        13        14        15        16 \n139.24743  71.54599 163.14206 195.00156 254.73813 242.79081 306.50982 175.08937 \n       17        18        19        20        21        22        23        24 \n131.28255 131.28255 155.17718 218.89619 111.37036 202.96644 119.33524  99.42305 \n       25        26        27        28        29        30        31        32 \n242.79081 274.65032 103.40549 103.40549 155.17718  99.42305 210.93131 167.12449 \n       33        34        35        36        37        38        39        40 \n310.49226 179.07181 107.38793 131.28255 222.87862  67.56355 198.98400 302.52738 \n       41        42        43        44        45        46        47        48 \n270.66788 278.63275 187.03668  99.42305 234.82594 226.86106 246.77325 262.70300 \n       49        50        51        52        53        54        55        56 \n115.35280  71.54599 163.14206 274.65032 155.17718 163.14206 187.03668 163.14206 \n       57        58        59        60        61        62        63        64 \n234.82594 226.86106  99.42305 250.75569 115.35280 175.08937 183.05425 202.96644 \n       65        66        67        68        69        70        71        72 \n159.15962 123.31768 123.31768 143.22987 155.17718 254.73813 242.79081 278.63275 \n       73        74        75        76        77        78        79        80 \n210.93131 151.19474 270.66788 234.82594 191.01912 119.33524 310.49226 226.86106 \n       81        82        83        84        85        86        87        88 \n159.15962 206.94887 270.66788 250.75569 147.21231 238.80838 230.84350 103.40549 \n       89        90        91        92        93        94        95        96 \n147.21231 159.15962 167.12449 151.19474 302.52738 210.93131 187.03668 119.33524 \n       97        98        99       100 \n175.08937 266.68544 302.52738 198.98400 \n\n\nNun haben wir aber im Rahmen unserer Befragung die Internetnutzung der Befragten aber ja schon erhoben. Wozu dient das dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nresiduals.lm(model)\n\n          1           2           3           4           5           6 \n-194.474695 -162.878625 -269.474695  -29.598673 -126.773252 -228.720565 \n          7           8           9          10          11          12 \n -31.458176  -78.983998 -109.247430  -11.545986  -43.142057  404.998440 \n         13          14          15          16          17          18 \n-194.738127  237.209186  -96.509820  304.910630  -41.282554  -71.282554 \n         19          20          21          22          23          24 \n -35.177181  -38.896187  -51.370365 -142.966435   90.664759  -49.423051 \n         25          26          27          28          29          30 \n-240.790814   25.349683  -13.405489   16.594511 -125.177181  -84.423051 \n         31          32          33          34          35          36 \n -30.931311  132.875505 -130.492257  -89.071808   72.612073  -11.282554 \n         37          38          39          40          41          42 \n -42.878625  112.436451  401.016002   -2.527382  629.332121  921.367245 \n         43          44          45          46          47          48 \n 112.963316  -84.423051 -114.825938 -136.861063  353.226748   37.296997 \n         49          50          51          52          53          54 \n 184.647197  108.454014   16.857943  385.349683 -125.177181 -133.142057 \n         55          56          57          58          59          60 \n-172.036684  -73.142057  365.174062 -166.861063  -39.423051 -205.755690 \n         61          62          63          64          65          66 \n 184.647197 -115.089370  -63.054246  -62.966435  -99.159619  176.682322 \n         67          68          69          70          71          72 \n-108.317678 -118.229868  -35.177181  -74.738127  -32.790814   81.367245 \n         73          74          75          76          77          78 \n-180.931311  -31.194743  449.332121 -114.825938  348.980878  -29.335241 \n         79          80          81          82          83          84 \n-280.492257 -166.861063  -99.159619  -86.948873  -30.667879  -10.755690 \n         85          86          87          88          89          90 \n -27.212306 -118.808376 -200.843500  -73.405489   92.787694 -114.159619 \n         91          92          93          94          95          96 \n  12.875505  328.805257 -212.527382   29.068689 -127.036684  120.664759 \n         97          98          99         100 \n-165.089370  273.314559  -92.527382 -108.983998 \n\n\n\n\n3.2.14 Inhaltliche Interpretation\nFür unseren Fall Nummer 3 beträgt die Abweichung der Prognose von der Beobachtung (121) Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 15 Prozent nicht besonders groß ist.\n\n\n3.2.15 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten_mod$vorhersage &lt;- predict(model) \ndaten_mod$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n  geom_segment(aes(xend = alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Internetnutzung\") + # Titel\n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\") # Achsen-Beschriftung\n\n\n\n\n\n\n3.2.16 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Standardized Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 378.1937           NA    59.7466   6.330 0.0000000074 ***\nalter        -3.9824      -0.3183     1.1979  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.17 Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   378.         NA         59.7       6.33 0.00000000740\n2 alter          -3.98       -0.318      1.20     -3.32 0.00125      \n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.101        0.0922  198.      11.1 0.00125     1  -670. 1346. 1353.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model)\n\n# A tibble: 100 × 8\n   internetnutzung alter .fitted .resid   .hat .sigma   .cooksd .std.resid\n             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1             120    16   314.  -194.  0.0452   198. 0.0239       -1.00  \n 2              60    39   223.  -163.  0.0124   199. 0.00428      -0.827 \n 3              45    16   314.  -269.  0.0452   197. 0.0458       -1.39  \n 4              30    80    59.6  -29.6 0.0496   199. 0.000613     -0.153 \n 5             120    33   247.  -127.  0.0172   199. 0.00364      -0.645 \n 6              30    30   259.  -229.  0.0206   198. 0.0143       -1.17  \n 7              60    72    91.5  -31.5 0.0327   199. 0.000441     -0.161 \n 8             120    45   199.   -79.0 0.0102   199. 0.000823     -0.400 \n 9              30    60   139.  -109.  0.0161   199. 0.00253      -0.556 \n10              60    77    71.5  -11.5 0.0428   199. 0.0000792    -0.0595\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "href": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden",
    "text": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.4 Prüfung der Voraussetzungen",
    "text": "3.4 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "href": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.5 Berechnung und Interpretation einer einfachen linearen Regression",
    "text": "3.5 Berechnung und Interpretation einer einfachen linearen Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n(Platzhalter generiert durch ChatGDP)\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "href": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.1 t-Test für unabhängige Stichproben",
    "text": "2.1 t-Test für unabhängige Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#mann-whitney",
    "href": "Skript_6.3.html#mann-whitney",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.2 Mann-Whitney",
    "text": "2.2 Mann-Whitney"
  },
  {
    "objectID": "Skript_6.3.html#die-varianzanalyse",
    "href": "Skript_6.3.html#die-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.3 Die Varianzanalyse",
    "text": "2.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n2.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n2.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n2.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n2.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n2.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n2.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.3.html#kruskal-wallis",
    "href": "Skript_6.3.html#kruskal-wallis",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.4 Kruskal Wallis",
    "text": "2.4 Kruskal Wallis"
  },
  {
    "objectID": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.5 Mehrfaktorielle Varianzanalyse",
    "text": "2.5 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n2.5.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n2.5.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "href": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.1 t-Test für verbundene Stichproben",
    "text": "3.1 t-Test für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#wilcoxon",
    "href": "Skript_6.3.html#wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.2 Wilcoxon",
    "text": "3.2 Wilcoxon"
  },
  {
    "objectID": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "href": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.2 Multiple Regression mit Dummy-Codierung",
    "text": "1.2 Multiple Regression mit Dummy-Codierung\n\n1.2.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Geschlecht und Internetnutzung\nNun wollen wir noch Geschlecht (gndr) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei Gender haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist! Es handelt sich vielmehr um eine kategoriale Variable. Wie Sie schon gelernt haben, können Sie diese mit einem “Trick” ebenfalls in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Wir wollen uns hier mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl erst einmal umcodieren.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n1.2.2 Dummy Codierung der Variable Gender\n\ndaten_mod2 &lt;- daten_mod %&gt;%\nmutate(gender_r  = recode(gender, 'Male'='0', 'Female'='1')) %&gt;% # Recodierung der Var Gender zur Dummy-Variable\n  mutate(gender_r = as.numeric(as.character(gender_r))) # Variable als numerischen Wert behandeln\ndaten_mod2\n\n# A tibble: 100 × 167\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001263 GB    Female    72 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2  10006488 DE    Female    42 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 3  10000260 DE    Male      55 &lt;NA&gt;                     Mittle… Kein H… Abgesc…\n 4      1147 FR    Male      44 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      2089 FR    Male      72 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10000032 DE    Male      30 None of these (NEVER ma… Refusal Kein H… Refusal\n 7      1766 FR    Male      45 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8 100000390 GB    Male      63 Legally divorced/civil … &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 9 100000483 GB    Female    55 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10      2133 FR    Male      58 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n# ℹ 90 more rows\n# ℹ 159 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\ntable(daten_mod2$gender_r)\n\n\n 0  1 \n49 51 \n\nsummary(daten_mod2$gender_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    1.00    0.51    1.00    1.00 \n\nclass(daten_mod2$gender_r)\n\n[1] \"numeric\"\n\n\n\n\n1.2.3 Regressionsmodell zum Zusammenhang von Alter, Nachrichtenrezeptiion, Geschlecht und Internetnutzung spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable gender_r ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel_m2 &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption + gender_r, data = daten_mod2)\nsummary(lm.beta(model_m2))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption + \n    gender_r, data = daten_mod2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-233.88 -120.66  -42.97   23.33  689.97 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     268.1371           NA    58.5986   4.576\nalter                            -2.1276      -0.1939     1.0753  -1.979\npolitische_Nachrichtenrezeption   0.1925       0.1379     0.1367   1.408\ngender_r                         54.3519       0.1444    36.9466   1.471\n                                 Pr(&gt;|t|)    \n(Intercept)                     0.0000142 ***\nalter                              0.0507 .  \npolitische_Nachrichtenrezeption    0.1625    \ngender_r                           0.1445    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 184 on 96 degrees of freedom\nMultiple R-squared:  0.08188,   Adjusted R-squared:  0.05318 \nF-statistic: 2.854 on 3 and 96 DF,  p-value: 0.04125\n\n\n\n\n1.2.4 Inhaltliche Interpretation\nGender hat hier keinen signifikanten Einfluss auf den Internetkonsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy- Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (9,8) Minuten geringeren Internetkonsum als Männer (wobei dieser Befund statistisch ja (nicht) signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.5 Vorhersage des multivariaten Modells für die tägliche Internetnutzung durch Alter, Nachrichtenrezeption und Geschlecht\n\npredict.lm(model_m2, data.frame(alter = c(25, 75), gender_r = c(0,1), politische_Nachrichtenrezeption = c(5, 10)))\n\n       1        2 \n215.9085 164.8410 \n\n\n\n\n1.2.6 Inhaltliche Interpretation\nEine männliche Person, die 25 Jahre alt ist und 5 Minuten pro Tag politische Nachrichten rezipiert hat, einen prognostizierten Internetkonsum von 293 Minuten. Eine weibliche Person, die 75 Jahre alt ist und ebenfalls 5 Minuten pro Tag politische Nachrichten rezipiert, hat einen prognostizierten Internetkonsum von 41 Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.7 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie diese ausführen, haben Sie ja heute zu Anfang der Sitzung gelernt (siehe oben). Zusätzlich müssen Sie bei einer multiplen Regresssion noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model_m2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                            Term  VIF       VIF 95% CI Increased SE Tolerance\n                           alter 1.00 [1.00,      Inf]         1.00      1.00\n politische_Nachrichtenrezeption 1.00 [1.00,      Inf]         1.00      1.00\n                        gender_r 1.01 [1.00, 1.59e+09]         1.00      0.99\n Tolerance 95% CI\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n\n\n\n\n1.2.8 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5; wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "href": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse",
    "text": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse\n\ndaten_mod3 &lt;- daten %&gt;% \n  select(agea, netustm, cntry) %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm,\n         land = cntry) %&gt;% \n  filter(land %in% c(\"DE\", \"FR\", \"IS\", \"PL\")) %&gt;% \n  drop_na() %&gt;% \n  group_by(land) %&gt;% \n  slice_sample(n = 100) %&gt;% \n  ungroup()\ndaten_mod3\n\n# A tibble: 200 × 3\n   alter internetnutzung land \n   &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;\n 1    71              90 DE   \n 2    29             180 DE   \n 3    50             720 DE   \n 4    50             360 DE   \n 5    41             600 DE   \n 6    19             360 DE   \n 7    52             370 DE   \n 8    34              60 DE   \n 9    26              90 DE   \n10    18             120 DE   \n# ℹ 190 more rows\n\n\n\nggplot(daten_mod3, aes(alter, internetnutzung, colour = land)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(method = lm, formula = \"y ~ x\", se = FALSE) + \n  scale_colour_brewer(palette = \"Set1\") + \n  ggtitle(\"Lineare Regression für Alter und Internetnutzung (vier Länder)\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")"
  },
  {
    "objectID": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "href": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.3 Literatur und Beispiele aus der Praxis",
    "text": "3.3 Literatur und Beispiele aus der Praxis\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link\n\n:::"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n(Platzhalter generiert durch ChatGDP)\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Autoren.html",
    "href": "Autoren.html",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Wir möchten uns einmal kurz vorstellen…"
  },
  {
    "objectID": "Autoren.html#patrick-zerrer",
    "href": "Autoren.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "Patrick Zerrer",
    "text": "Patrick Zerrer\n\n\n\nPatrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften”. Das Masterstudium „Öffentliche Kommunikation” an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action” mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik” mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Hintergrund.html",
    "href": "Hintergrund.html",
    "title": "Lernen durch Praxis - Der Weg zu fundierten statistischen Kenntnissen",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nUnser Leitbild ist es, Ihnen einen Kurs anzubieten, der auf der Verschränkung von praktischem Lernen, grundlegenden Kenntnissen und eigenständigem Forschen basiert. Unser Ziel ist es, Ihnen die Werkzeuge und das Verständnis zu vermitteln, um quantitative Forschungsdesigns mit R erfolgreich umzusetzen.\nWir beginnen mit einer soliden Grundlage, in der wir Ihnen die wesentlichen Konzepte und Techniken der quantitativen Forschung vermitteln. Wir legen Wert darauf, dass Sie die grundlegenden Prinzipien verstehen, bevor wir Sie in die Praxis entlassen. Sie werden lernen, wie Sie Daten in R importieren, explorieren und visualisieren können, um ein tieferes Verständnis für Ihre Forschungsfragen zu gewinnen.\nNachdem Sie diese Grundlagen erworben haben, gehen wir einen Schritt weiter und bieten Ihnen die Möglichkeit, Ihr eigenes Forschungsprojekt durchzuführen. Unter Anleitung unserer erfahrenen Tutoren werden Sie ein eigenständiges Projekt entwickeln, bei dem Sie Ihre neu erlernten statistischen Kenntnisse anwenden können. Sie werden Schritt für Schritt lernen, wie Sie Hypothesen aufstellen, Daten sammeln, analysieren und interpretieren. Durch dieses praktische Erleben vertiefen Sie nicht nur Ihr Verständnis, sondern gewinnen auch wertvolle Erfahrungen im Bereich der quantitativen Forschung.\nUnser Kurs legt großen Wert darauf, dass Sie nicht nur theoretisches Wissen erlangen, sondern dieses Wissen in die Praxis umsetzen können. Wir sind der festen Überzeugung, dass das eigenständige Durchführen eines Forschungsprojekts Ihnen nicht nur ein tieferes Verständnis für statistische Methoden gibt, sondern auch Ihre analytischen und Problemlösungsfähigkeiten stärkt. Unsere Tutoren stehen Ihnen dabei zur Seite und bieten Ihnen individuelle Unterstützung, um sicherzustellen, dass Sie Ihr volles Potenzial entfalten können.\nWillkommen zu einer spannenden Lernreise, bei der Sie nicht nur statistische Kenntnisse erwerben, sondern auch die Fähigkeit entwickeln, Ihr Wissen auf praktische Weise anzuwenden. Wir sind davon überzeugt, dass Sie durch dieses ganzheitliche Lernkonzept Ihre Ziele erreichen und in der Welt der quantitativen Forschung erfolgreich sein werden.\n(Platzhalter generiert durch ChatGDP)"
  },
  {
    "objectID": "Hintergrund.html#patrick-zerrer",
    "href": "Hintergrund.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Patrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften\". Das Masterstudium „Öffentliche Kommunikation\" an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action\" mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik\" mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_5.2.html#laden-der-nötigen-pakete",
    "href": "Skript_5.2.html#laden-der-nötigen-pakete",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "library(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.2.html#laden-des-datensatzes",
    "href": "Skript_5.2.html#laden-des-datensatzes",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "daten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "href": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "title": "Die Faktorenanalyse",
    "section": "1.2 Deskriptive Statistik für den Teildatensatz",
    "text": "1.2 Deskriptive Statistik für den Teildatensatz\nWir werfen einen kurzen Blick in die deskriptive Statistik für unseren Teildatensatz, um ein besseres Verständnis für die Daten zu erhalten.\n\nsummary(allbus_vertrauen)\n\n Ver_Gesundheitswesen   Ver_BVerfG    Ver_Bundestag   Ver_Verwaltung \n Min.   :1.000        Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000        1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000  \n Median :5.000        Median :6.000   Median :4.000   Median :5.000  \n Mean   :4.939        Mean   :5.255   Mean   :4.058   Mean   :4.482  \n 3rd Qu.:6.000        3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000        Max.   :7.000   Max.   :7.000   Max.   :7.000  \n Ver_kath_Kirche Ver_evan_Kirche   Ver_Justiz        Ver_TV     \n Min.   :1.000   Min.   :1.00    Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.00    1st Qu.:4.000   1st Qu.:3.000  \n Median :2.000   Median :3.00    Median :5.000   Median :4.000  \n Mean   :2.331   Mean   :3.04    Mean   :4.581   Mean   :3.577  \n 3rd Qu.:3.000   3rd Qu.:4.00    3rd Qu.:6.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.00    Max.   :7.000   Max.   :7.000  \n  Ver_Zeitung       Ver_Uni      Ver_Regierung    Ver_Polizei     Ver_Parteien \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000   1st Qu.:2.00  \n Median :4.000   Median :5.000   Median :4.000   Median :5.000   Median :3.00  \n Mean   :4.012   Mean   :5.203   Mean   :4.054   Mean   :4.948   Mean   :3.19  \n 3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:4.00  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00  \n   Ver_Kom_EU      Ver_EU_Par   \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000  \n Median :4.000   Median :4.000  \n Mean   :3.515   Mean   :3.556  \n 3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000"
  },
  {
    "objectID": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "href": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Hauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben."
  },
  {
    "objectID": "Skript_5.2.html#quellen-für-das-script",
    "href": "Skript_5.2.html#quellen-für-das-script",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Stephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html#laden-der-nötigen-pakete",
    "href": "Skript_5.3.html#laden-der-nötigen-pakete",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Laden der nötigen Pakete",
    "text": "1.1 Laden der nötigen Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, psych, dplyr, htmlTable)\n\nUnd laden im Anschluss den notwendigen Datensatz."
  },
  {
    "objectID": "Skript_5.3.html#laden-des-datensatzes",
    "href": "Skript_5.3.html#laden-des-datensatzes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Laden des Datensatzes",
    "text": "1.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Reliabilität von Skalen",
    "section": "1.3 Teildatensatz mit den benötigten Index-Variablen",
    "text": "1.3 Teildatensatz mit den benötigten Index-Variablen\nWir greifen natürlich auf die gleiche Datengrundlage zurück, welche wir auch für die Faktorenanalyse verwendet haben. Was in unserem Fall bedeutet, dass wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nWir bereiten die Daten entsprechend vor, indem wir die fehlenden Werte entfernen und die Variablen in numerische umwandeln.\n\nallbus_vertrauen = daten %&gt;%\n  \n  # Wir wählen mit select() die gewünschten Variablen aus\n  select(pt01:pt20) %&gt;% \n  \n  # die Kombination aus mutate() und across() ermöglicht es uns die Funktion as.numeric() in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden\n  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;% \n\n  # Wir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen)\n  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;% \n  \n  # Wir schmeißen fehlende Werte raus\n  na.omit()\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n  # mit dem rename() Befehl können wir die Variablen umbennen\n  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\nhtmlTable(head(allbus_vertrauen))\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nWir haben nun alle Daten geladen und die Variablen entsprechend vorbereitet. Wir können eigentlich mit der Indexbidlung beginnen, müssen uns allerdings davor noch entscheiden, welche Art von Index wir bilden möchten."
  },
  {
    "objectID": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "href": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "title": "Reliabilität von Skalen",
    "section": "1.5 Berechnen eines Ungewichteten Summenindex",
    "text": "1.5 Berechnen eines Ungewichteten Summenindex\nWir haben bereits in Kapitel 5.2 mittels der explorativen Faktorenanalyse statistisch getestet, ob wir einen Index aus den genannten Variablen bilden können. Dies ist der Fall. Wir berechnen nur die einfachste Form eines Index, den ungewichteten Summenindex. Das bedeutet, dass wir die Werte pro befragter Person für die genannten Variablen aufsummieren und KEINE Gewichtungen einbauen. Eine Gewichtung wäre bspw. wenn wir eine Variable doppelt zählen würden.\nWir erstellen eine neue Variable vertrauen_ges_inst und summieren die Werte aller Indikatoren pro Fall (befragte Person) auf, bevor wir diese durch die Anzahl der Indikatoren teilen. Auf diese Art und Weise erhalten wir die selben Werteausprägungen, wie in den Indikatoren was uns die Interpretation erleichtert.\n\nindex_vertrauen = allbus_vertrauen %&gt;% \n  mutate(vertrauen_ges_inst = (Ver_Gesundheitswesen + Ver_BVerfG + Ver_Bundestag + Ver_Verwaltung + Ver_kath_Kirche + Ver_evan_Kirche + Ver_Justiz+ Ver_TV +  Ver_Zeitung + Ver_Uni + Ver_Regierung + Ver_Polizei + Ver_Parteien + Ver_Kom_EU + Ver_EU_Par) / 15)\nhtmlTable(head(index_vertrauen))\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\nvertrauen_ges_inst\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n4.6\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n4.8\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n4.2\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n4.66666666666667\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n4\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n2.8\n\n\n\n\n\nWir können uns noch die deskriptive Statistik für den Index anschauen, diese ist wichtig um den berechneten Index korrekt zu interpretieren.\n\nsummary(index_vertrauen$vertrauen_ges_inst)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.400   4.133   4.049   4.733   7.000"
  },
  {
    "objectID": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "href": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "title": "Reliabilität von Skalen",
    "section": "1.6 Reliabilität des Indizes berechnen",
    "text": "1.6 Reliabilität des Indizes berechnen\nBevor wir diesen Index einsetzen können, müssen wir zunächst noch checken, ob die Variablen auch inhaltlich zusammenpassen. Dazu ermitteln wir Crombach’s Alpha als Maß der Skalenreliabilität:\n\nindex_vertrauen %&gt;%\n  select(Ver_Gesundheitswesen:Ver_EU_Par) %&gt;%\n  psych::alpha(check.keys=TRUE)\n\n\nReliability analysis   \nCall: psych::alpha(x = ., check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.92      0.92    0.94      0.43  11 0.0021    4  1     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.92  0.92\nDuhachek  0.91  0.92  0.92\n\n Reliability if an item is dropped:\n                     raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r\nVer_Gesundheitswesen      0.92      0.92    0.94      0.44 11.0   0.0021 0.024\nVer_BVerfG                0.91      0.91    0.94      0.43 10.5   0.0022 0.023\nVer_Bundestag             0.91      0.91    0.93      0.41  9.9   0.0024 0.020\nVer_Verwaltung            0.91      0.91    0.94      0.43 10.6   0.0022 0.025\nVer_kath_Kirche           0.92      0.92    0.94      0.45 11.7   0.0020 0.018\nVer_evan_Kirche           0.92      0.92    0.94      0.45 11.3   0.0020 0.021\nVer_Justiz                0.91      0.91    0.93      0.42 10.3   0.0023 0.023\nVer_TV                    0.91      0.91    0.93      0.43 10.7   0.0022 0.023\nVer_Zeitung               0.91      0.91    0.93      0.43 10.5   0.0022 0.023\nVer_Uni                   0.91      0.92    0.94      0.44 10.8   0.0022 0.024\nVer_Regierung             0.91      0.91    0.93      0.41  9.8   0.0024 0.020\nVer_Polizei               0.91      0.91    0.94      0.43 10.7   0.0022 0.024\nVer_Parteien              0.91      0.91    0.93      0.42 10.1   0.0023 0.022\nVer_Kom_EU                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\nVer_EU_Par                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\n                     med.r\nVer_Gesundheitswesen  0.43\nVer_BVerfG            0.41\nVer_Bundestag         0.41\nVer_Verwaltung        0.42\nVer_kath_Kirche       0.43\nVer_evan_Kirche       0.43\nVer_Justiz            0.41\nVer_TV                0.43\nVer_Zeitung           0.41\nVer_Uni               0.42\nVer_Regierung         0.41\nVer_Polizei           0.43\nVer_Parteien          0.41\nVer_Kom_EU            0.41\nVer_EU_Par            0.41\n\n Item statistics \n                        n raw.r std.r r.cor r.drop mean  sd\nVer_Gesundheitswesen 3238  0.58  0.58  0.53   0.51  4.9 1.4\nVer_BVerfG           3238  0.70  0.69  0.67   0.64  5.3 1.5\nVer_Bundestag        3238  0.83  0.82  0.82   0.79  4.1 1.6\nVer_Verwaltung       3238  0.66  0.66  0.62   0.60  4.5 1.3\nVer_kath_Kirche      3238  0.47  0.46  0.42   0.38  2.3 1.5\nVer_evan_Kirche      3238  0.54  0.52  0.49   0.45  3.0 1.7\nVer_Justiz           3238  0.73  0.73  0.70   0.67  4.6 1.5\nVer_TV               3238  0.64  0.65  0.62   0.58  3.6 1.3\nVer_Zeitung          3238  0.67  0.68  0.66   0.62  4.0 1.3\nVer_Uni              3238  0.61  0.63  0.58   0.56  5.2 1.2\nVer_Regierung        3238  0.83  0.83  0.83   0.80  4.1 1.6\nVer_Polizei          3238  0.63  0.64  0.60   0.57  4.9 1.4\nVer_Parteien         3238  0.78  0.77  0.76   0.74  3.2 1.3\nVer_Kom_EU           3238  0.80  0.79  0.81   0.75  3.5 1.5\nVer_EU_Par           3238  0.80  0.79  0.81   0.76  3.6 1.6\n\nNon missing response frequency for each item\n                        1    2    3    4    5    6    7 miss\nVer_Gesundheitswesen 0.02 0.04 0.10 0.17 0.28 0.28 0.11    0\nVer_BVerfG           0.02 0.04 0.07 0.15 0.19 0.28 0.24    0\nVer_Bundestag        0.08 0.10 0.16 0.25 0.24 0.14 0.04    0\nVer_Verwaltung       0.03 0.05 0.13 0.26 0.31 0.18 0.04    0\nVer_kath_Kirche      0.42 0.20 0.16 0.13 0.05 0.03 0.02    0\nVer_evan_Kirche      0.26 0.16 0.19 0.19 0.11 0.07 0.02    0\nVer_Justiz           0.04 0.06 0.13 0.21 0.25 0.24 0.07    0\nVer_TV               0.08 0.13 0.21 0.33 0.18 0.05 0.01    0\nVer_Zeitung          0.05 0.09 0.18 0.30 0.26 0.11 0.01    0\nVer_Uni              0.01 0.02 0.05 0.17 0.29 0.34 0.12    0\nVer_Regierung        0.10 0.10 0.14 0.22 0.24 0.16 0.04    0\nVer_Polizei          0.02 0.04 0.08 0.18 0.28 0.30 0.10    0\nVer_Parteien         0.13 0.18 0.25 0.28 0.13 0.03 0.00    0\nVer_Kom_EU           0.13 0.14 0.19 0.26 0.19 0.08 0.02    0\nVer_EU_Par           0.13 0.14 0.18 0.25 0.19 0.09 0.02    0"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Zur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "href": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Interpretation des Wirksamkeit-Indizes",
    "text": "1.8 Interpretation des Wirksamkeit-Indizes\nDie Werte sind ein gutes Ergebnis. Die Items zeigen eine gute Inter-Item-Korrelation.\nWir können noch nachschauen, ob wir die Skalen-Reliabilität verbessern können, indem wir einzelne Items herauswerfen. Denn der Output von Cronbachs Alpha gibt uns auch hilfreiche Aufschlüsse darüber, welche Items man evtl. ausschließen kann, um Cronbachs Alpha bei ungenügender Höhe noch auf ein mindestens akzeptables Maß zu heben. Diese Information findet sich im Bereich “Reliability if an item is dropped”:. In unserem Fall wird die reliabitlitä aber noch schlechter - wir können nichts mehr verbessern.\nEntsprechend haben wir erfolgreich einen Index für die latente Variable Vertrauen in gesellschaftliche Institutionen gebildet. Wir haben eine theoretische Grundlage gefunden, diese empirisch anhand der Daten des Allbus mittels explorativer Faktorenanalyse geprüft und einen Summenindex berechnet, dessen Qualität wir mittels Cronbachs Alpha zeigen konnten.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der Indexbildung werden meistens direkt im Text angegeben:\n✅ die Art des gebildeten Index (Summenindex, etc.)\n✅ Cronbachs Alpha\n✅ Enthaltene Indikatoren\nDas Format ist normalerweise:\n\nBeispiel: Der Summenindex individuelle Identität umfasst fünf Indikatoren (Ziele und Befriedigung, Regeln und Verantwortung, Gefühle oder Emotionen, Verständnis der Welt, individuelle Identität im Allgemeinen; α = 0,84)."
  },
  {
    "objectID": "Skript_5.3.html#quellen-für-das-script",
    "href": "Skript_5.3.html#quellen-für-das-script",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Quellen für das Script",
    "text": "1.8 Quellen für das Script\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "href": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Video\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten."
  },
  {
    "objectID": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "href": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse",
    "text": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse\nDie Faktorenanalyse bringt, wie jedes statistische Verfahren, eine Reihe von Vorraussetzungen mit. Diese Vorraussetzungen sollten wir kennen und bei der Anwendung der Faktorenanalyse beachten. Viele der Vorraussetzungen beziehen sich auf Pearson-Korrelationskoeffizienten, welcher die statistsiche Grundlage für die Berechnung der Faktoren bildet.\n\nVarianz: Wir sollten sichergehen, dass die Daten aus unserer Stichprobe ausreichend varrieren. Wir werfen hierfür ein Blick in die Daten.\n\n\n# Visuelle Überprüfung mit einem Histogram für die erste Variable \"Ver_Gesundheitswesen\". Die restlichen Variablen sollten auch überprüft werden.\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors) \n\n\n\n\n\nLinearität: Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Variablen. Wenn die tatsächliche Beziehung nicht linear ist, dann verringert sich der Wert von r. Wir können auf Linearität u.a. visuell durch das Betrachten der Daten mittel Streudiagramm prüfen.\n\n\n# Visuelle Überprüfung mit einem Streudiagramm für die ersten beiden Variable (Ver_Gesundheitswesen und Ver_BVerfG)\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen, y = Ver_BVerfG)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"darkgreen\", size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\nNormalverteilung: Der Pearson-Korrelationskoeffizient setzt eine Normalverteilung voraus. Allerdings finden sich in der Realität fast nie perfekt normalverteilte Daten. Schiefe und Kurtosis sind besonders einflussreich die Ergebnisse der Faktorenanalyse und können im Extremfall artefaktische Ergenbnisse erzeugen.\n\n\n# Visuelle Überprüfung mit einem Histogram für die erste Variable (Ver_Gesundheitswesen)\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors) \n\n\n\n# Statistische Überprüfung mittels Shapiro Wilk Test für die erste Variable (pt01)\nshapiro.test(allbus_vertrauen$Ver_Gesundheitswesen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_vertrauen$Ver_Gesundheitswesen\nW = 0.92081, p-value &lt; 2.2e-16\n\n# Ein p-Wert unter 0.05 = keine Normalverteilung \n# Ein p-Wert über 0.05 = Normalverteilung\n\n\nLevel der Messung: Bei Pearson-Korrelationen wird davon ausgegangen, dass normalverteilte Variablen auf Intervall- oder Verhältnisskalen gemessen werden, d. h. es handelt sich um kontinuierliche Daten mit gleichen Intervallen. Diese Eigenschaften treffen nicht auf ordinale (bspw. Kategorien) oder dochotome (bspw. Wahr-Falsch-Items) Variablen zu, was sich negativ auf Pearson-Korrelationskoeffizieten auswirkt und zu verzerrten Ergebnissen führen kann. Allerdings ist ein beträchtlicher Teil der Daten, mit denen wir zu tun haben, ordinal oder dichotom skaliert, um auch mit diesen Daten arbeiten zu können nutzen wir die polychorische Korrelation, welche robuster Nicht-Normalverteilung ist.\nFehlende Werte: In jeder Studie sollten wir die Anzahl und die Art der fehlenden Werte sowie die Gründe und die Methoden für den Umgang mit diesen Daten angegeben werden.\n\n\n# Wir haben den Code bereits am Anfang ausgefürht\nallbus_vertrauen = allbus_vertrauen %&gt;%  \n  \n  # Wir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen)\n  mutate(across(Ver_Gesundheitswesen:Ver_EU_Par, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;% \n  \n  # Wir schmeißen fehlende Werte raus\n  na.omit()\n\n\nAusreißer / Outliers: Wir sollten Ausreißer identifizieren und im Zweifel von der Analyse ausschließen, da diese zu einer Verzerrung der Ergebnisse führen können. Zu den Methoden zur Erkennung von Ausreißern gehören Boxplots und Streudiagramme für einzelne Variablen sowie der Mahalanobis-Abstand für mehrere Variablen.\n\n\nboxplot(allbus_vertrauen)\n\n\n\n\n\nPassung der Daten für die Faktorenanalyse: Wir sollten trotz unserer guten Datengrundlage nochmals prüfen, ob die gemessenen Variablen ausreichend miteinander korreliert sind, um eine Faktorenanalyse zu rechtfertigen. Eine Korrelation zwischen zwei Variablen gibt an, ob und wie stark ein Zusammenhang zwischen den beiden Variablen besteht. An dieser Stelle ist es wichtig, sich zu merken, dass eine Korrelation die Stärke eines Zusammenhangs angibt. Eine genauere Erklärung findet ihr im Kapitel 7. Zunächst können wir einen Blick in die Korrelationsmatrix werfen - eine beträchtliche Anzahl von Korrelationen sollte ±.30 überschreiten. Alternativ können wir ein objektiveren Test der Faktorfähigkeit der Korrelationsmatrix durchführen. Hierfür greifen wir auf den Sphärizitätstest nach Bartlett (1954) zurück.\n\n\nhtmlTable(round(cor(allbus_vertrauen), digits = 3))\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\nVer_Gesundheitswesen\n1\n0.435\n0.446\n0.375\n0.184\n0.21\n0.384\n0.337\n0.334\n0.37\n0.43\n0.413\n0.349\n0.359\n0.357\n\n\nVer_BVerfG\n0.435\n1\n0.629\n0.397\n0.148\n0.223\n0.635\n0.385\n0.424\n0.474\n0.561\n0.45\n0.429\n0.456\n0.476\n\n\nVer_Bundestag\n0.446\n0.629\n1\n0.553\n0.267\n0.323\n0.571\n0.483\n0.487\n0.42\n0.816\n0.445\n0.677\n0.656\n0.661\n\n\nVer_Verwaltung\n0.375\n0.397\n0.553\n1\n0.274\n0.306\n0.471\n0.361\n0.377\n0.384\n0.485\n0.422\n0.472\n0.456\n0.455\n\n\nVer_kath_Kirche\n0.184\n0.148\n0.267\n0.274\n1\n0.699\n0.23\n0.2\n0.202\n0.138\n0.261\n0.219\n0.301\n0.288\n0.284\n\n\nVer_evan_Kirche\n0.21\n0.223\n0.323\n0.306\n0.699\n1\n0.271\n0.258\n0.279\n0.2\n0.327\n0.27\n0.35\n0.328\n0.325\n\n\nVer_Justiz\n0.384\n0.635\n0.571\n0.471\n0.23\n0.271\n1\n0.356\n0.418\n0.453\n0.56\n0.53\n0.48\n0.534\n0.541\n\n\nVer_TV\n0.337\n0.385\n0.483\n0.361\n0.2\n0.258\n0.356\n1\n0.714\n0.378\n0.49\n0.351\n0.492\n0.411\n0.415\n\n\nVer_Zeitung\n0.334\n0.424\n0.487\n0.377\n0.202\n0.279\n0.418\n0.714\n1\n0.488\n0.502\n0.365\n0.489\n0.456\n0.461\n\n\nVer_Uni\n0.37\n0.474\n0.42\n0.384\n0.138\n0.2\n0.453\n0.378\n0.488\n1\n0.467\n0.409\n0.368\n0.43\n0.435\n\n\nVer_Regierung\n0.43\n0.561\n0.816\n0.485\n0.261\n0.327\n0.56\n0.49\n0.502\n0.467\n1\n0.494\n0.719\n0.699\n0.694\n\n\nVer_Polizei\n0.413\n0.45\n0.445\n0.422\n0.219\n0.27\n0.53\n0.351\n0.365\n0.409\n0.494\n1\n0.414\n0.374\n0.37\n\n\nVer_Parteien\n0.349\n0.429\n0.677\n0.472\n0.301\n0.35\n0.48\n0.492\n0.489\n0.368\n0.719\n0.414\n1\n0.707\n0.696\n\n\nVer_Kom_EU\n0.359\n0.456\n0.656\n0.456\n0.288\n0.328\n0.534\n0.411\n0.456\n0.43\n0.699\n0.374\n0.707\n1\n0.957\n\n\nVer_EU_Par\n0.357\n0.476\n0.661\n0.455\n0.284\n0.325\n0.541\n0.415\n0.461\n0.435\n0.694\n0.37\n0.696\n0.957\n1\n\n\n\n\n\nNoch zu prüfen ist die Korrelation der Items miteinander, hierfür nehmen wir den Bartlett Test.\n\ncortest.bartlett(allbus_vertrauen)\n\nR was not square, finding R from data\n\n\n$chisq\n[1] 33164.76\n\n$p.value\n[1] 0\n\n$df\n[1] 105\n\n\nBei großen Stichprobenumfängen, wie in unserem Fall mit dem Allbus, reagiert der Bartlett-Test selbst auf geringfügige Abweichungen vom Zufallsprinzip empfindlich, so dass seine Ergebnisse durch ein Maß für die Stichprobenadäquanz ergänzt werden sollten. Das Kaiser-Meyer-Olkin (KMO; Kaiser, 1974) Maß für die Stichprobenadäquanz ist das Verhältnis von Korrelationen und partiellen Korrelationen, das das Ausmaß widerspiegelt, in dem Korrelationen eine Folge der über alle Variablen geteilten Varianz sind und nicht der von bestimmten Variablenpaaren geteilten Varianz.\n\nKMO(allbus_vertrauen)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = allbus_vertrauen)\nOverall MSA =  0.89\nMSA for each item = \nVer_Gesundheitswesen           Ver_BVerfG        Ver_Bundestag \n                0.96                 0.91                 0.91 \n      Ver_Verwaltung      Ver_kath_Kirche      Ver_evan_Kirche \n                0.95                 0.73                 0.78 \n          Ver_Justiz               Ver_TV          Ver_Zeitung \n                0.94                 0.87                 0.88 \n             Ver_Uni        Ver_Regierung          Ver_Polizei \n                0.94                 0.92                 0.94 \n        Ver_Parteien           Ver_Kom_EU           Ver_EU_Par \n                0.97                 0.83                 0.83 \n\n\nKMO-Werte reichen von 0,00 bis 1,00 und können sowohl für die gesamte Korrelationsmatrix als auch für jede gemessene Variable berechnet werden. Insgesamt sind KMO-Werte ≥.70 erwünscht und Werte unter .50 werden im Allgemeinen als inakzeptabel angesehen. In diesem Fall ist die Korrelationsmatrix nicht faktoriell.\n\n1.3.1 Modell der Faktorenanalyse\nWenn wir den Begriff Faktorenanalyse verwenden, meinen wir meistens zwei eigentlich unterschiedliche Verfahren, die sich in Zweck und Berechnung unterscheiden: die Hauptkomponentenanalyse (PCA) und die explorative Faktorenanalyse. Die Hauptkomponentenanalyse analysiert die gesamte Korrelationsmatrix und zielt darauf ab, Daten zu reduzieren und dabei so viele Informationen aus dem ursprünglichen Datensatz wie möglich zu erhalten. Zu diesem Zweck berechnet die Hauptkomponentenanalyse sogenannte Linearkombinationen der ursprünglichen Messvariablen, die so viele Informationen wie möglich erklären. Diese neuen Messvariablen werden als sogenannte Komponenten bezeichnet und sind im engeren Sinn keine latenten Konstrukte.\nDie Faktorenanalyse versucht im Unterschied dazu, die Gesamtvarianz der gemessenen Variablen in gemeinsame Varianz (Kommmunialität oder h2) und die einzigartige Varianzen (u2) zu trennen. Dies geschieht indem eine reduzierte Korrelationsmatrix analysiert wird, bei der eine Schätzung der gemeinsamen Varianz jeder Messvariablen auf der Diagonalen der Korrelationsmatrix platziert wird.\nZusammenfassend liefern beide Methoden, also sowohl die PCA als auch die Faktorenanalyse eine Schätzungen der Gemeinsamkeit, aber nur die Faktorenanalyse kann die Einzigartigkeit (u2) jeder gemessenen Variable schätzen.\nDie meisten Methodiker:innen empfehlen, dass die explorative Faktorenanalyse verwendet wird, um latente Konstrukte zu identifizieren. Fabrigar und Wegener (2012) empfahlen zum Beispiel Folgendes:\n\nWhen the goal of research is to identify latent constructs for theory building or to create measurement instruments in which the researcher wishes to make the case that the resulting measurement instrument reflects a meaningful underlying construct, we argue that common factor analysis (EFA) procedures are usually preferable. (Fabrigar und Wegener, 2012: 32)\n\nIn unserem Fall wenden wir eine explorative Faktorenanalyse an, da wir uns für die latente Variable Vertrauen in gesellschaftliche Institutionen interessieren. Hierfür greifen wir auf das psych-Paket zurück, welches die Funktion fa für eine Faktorenanalyse enthält.\n\n\n1.3.2 Methode der Schätzung\nNachdem wir die Faktorenanalyse (EFA) als bevorzugtes Modell festgelegt haben, müssen wir noch die Methode zur Schätzung (Extraktion) des Faktorenmodells auswählen. Konkret suchen wir ein mathematischen Verfahren, dass die Beziehungen zwischen den gemessenen Variablen und den Faktoren (d. h. die Regression der gemessenen Variablen auf die gemeinsamen Faktoren) möglichst genau schätzt.\nWir möchten kurz anmerken, dass der mathematische Hintergrund an dieser Stelle des Kurses noch nicht so wichtig ist, da hier einige Grundlagen erst imd Kapitel 7 erklärt werden. Trotzdem macht es unserer Einschätzung nach Sinn die Begriffe bereits zu kennen und deren Vor- und Nachteile bennen zu können.\nEs existieren eine ganze Reihe von unterschiedlichen Schätzmethoden, von denen zwei Methoden am häufigsten angewendet werden. (1) Die ML-Schätzung (Maximum Liklelihood) beruht auf der Normalverteilung und ist entsprechend empfindlicher multivariater Normalität und erfordert meistens einen größere Stichprobe (mehr Fälle). (2) Die PA (wird auch als Hauptfaktoren, MINRES oder OLS bezeichnet) ist im Gegenzug dazu eine Methode der kleinsten Quadrate, welche keine Annahmen über Verteilungen trifft. PA nutzt hier eine wiederholte Zwischenschätzung, welche eine bessere Schätzung der Gemeinsamkeit ermöglicht und wiederholt diese bis eine zufriedenstellende Lösung erreicht ist.\nDie PA eignet sich als Methode der Schätzung insbesondere dann, wenn der Zusammenhang zwischen den gemessenen Variablen und den Faktoren relativ schwach sind (≤.40), der Stichprobenumfang relativ klein ist (≤300), die multivariate Normalität verletzt ist oder wenn die Anzahl der den gemessenen Variablen zugrunde liegenden Faktoren falsch spezifiziert ist. Im Gegensatz dazu ist eine ML-Schätzung besser geeignet, wenn die Beziehungen zwischen Faktoren und Variablen stark sind (&gt;.40), der Stichprobenumfang groß ist, multivariate Normalität erreicht wird und die Anzahl der Faktoren korrekt angegeben ist.\nWir können für unser Beispiel weiterhin die Maximum-likelihood Faktorenanalyse aus dem psych-Paket mit der Funktion fa verwenden, da wir in unserem Fall die entsprechenden Voraussetzungen in Bezug auf den Stichprobenumfang, die Stärke der Beziehung, sowie der Anzahl der Faktoren erfüllen. Für dieses Beispiel nutzen wir trotz der nicht perfekten Normalverteilung die ML Methode, alternativ könnten die PA Methode als robustere Variante nehmen.\n\n\n1.3.3 Anzahl der Faktoren\nWie bereits bei der ML-Schätzung angedeutet, müssen wir die Anzahl der Faktoren festlegen. Hierfür müssen wir die Anzahl der Faktoren für die weitere Analyse festlegen. Wir erreichen das indem wir mehrere Modelle schätzen und somit Rückschlüsse auf ein optimales Modell mit der für uns passenden Anzahl an Faktoren ziehen. Vereinfacht können wir uns das Auswringen eines nassen Handtuchs vorstellen, bei der der erste Faktor die meiste Varianz extrahiert - ergo die größte Mene an Wasser - und die nachfolgenden Faktoren sukzessive kleinere Anteile der Varianz extrahieren. Auf diese Art und Weise können wir eine Schätzung des optimalen Modells vornehmen.\nWir verwenden hierfür die nfactors-Funktion, welche uns mehere Schätzungen ausgibt.\n\nnfactors(allbus_vertrauen, rotate = \"varimax\", fm=\"mle\")\n\n\n\n\n\nNumber of factors\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = FALSE, title = title, use = use, cor = cor)\nVSS complexity 1 achieves a maximimum of 0.88  with  1  factors\nVSS complexity 2 achieves a maximimum of 0.9  with  3  factors\nThe Velicer MAP achieves a minimum of 0.04  with  1  factors \nEmpirical BIC achieves a minimum of  -125.8  with  6  factors\nSample Size adjusted BIC achieves a minimum of  -26  with  8  factors\n\nStatistics by number of factors \n   vss1 vss2   map dof   chisq     prob sqresid  fit RMSEA  BIC SABIC complex\n1  0.88 0.00 0.035  90 1.0e+04  0.0e+00     7.0 0.88 0.187 9520  9806     1.0\n2  0.57 0.90 0.040  76 5.0e+03  0.0e+00     5.8 0.90 0.141 4348  4590     1.5\n3  0.62 0.90 0.044  63 3.0e+03  0.0e+00     3.8 0.93 0.120 2514  2714     1.5\n4  0.51 0.85 0.048  51 1.6e+03 1.8e-308     3.0 0.95 0.098 1221  1383     1.7\n5  0.38 0.71 0.061  40 6.0e+02 1.2e-100     2.4 0.96 0.066  275   402     2.0\n6  0.38 0.68 0.076  30 3.0e+02  6.2e-46     2.2 0.96 0.053   56   151     2.1\n7  0.34 0.58 0.101  21 1.5e+02  1.1e-21     2.0 0.97 0.044  -19    48     2.4\n8  0.32 0.57 0.138  13 3.8e+01  3.1e-04     1.3 0.98 0.024  -67   -26     2.5\n9  0.32 0.55 0.164   6 9.6e+00  1.4e-01     1.4 0.98 0.014  -39   -20     2.6\n10 0.31 0.52 0.164   0 6.0e+00       NA     1.6 0.97    NA   NA    NA     2.7\n11 0.29 0.53 0.240  -5 7.4e-02       NA     1.4 0.98    NA   NA    NA     2.7\n12 0.29 0.48 0.412  -9 2.6e-06       NA     1.4 0.98    NA   NA    NA     2.6\n13 0.30 0.52 0.719 -12 1.4e-08       NA     1.5 0.98    NA   NA    NA     2.7\n14 0.29 0.52 1.000 -14 0.0e+00       NA     1.4 0.98    NA   NA    NA     2.7\n15 0.36 0.68    NA -15 9.2e+02       NA     2.5 0.96    NA   NA    NA     2.2\n    eChisq    SRMR  eCRMS eBIC\n1  5.5e+03 9.0e-02 0.0969 4749\n2  3.8e+03 7.4e-02 0.0875 3150\n3  1.5e+03 4.8e-02 0.0614 1029\n4  7.6e+02 3.3e-02 0.0480  347\n5  2.0e+02 1.7e-02 0.0279 -122\n6  1.2e+02 1.3e-02 0.0245 -126\n7  6.4e+01 9.7e-03 0.0217 -106\n8  1.0e+01 3.9e-03 0.0111  -95\n9  2.8e+00 2.0e-03 0.0085  -46\n10 2.5e+00 1.9e-03     NA   NA\n11 1.1e-02 1.2e-04     NA   NA\n12 1.3e-06 1.4e-06     NA   NA\n13 5.4e-09 8.9e-08     NA   NA\n14 1.4e-12 1.4e-09     NA   NA\n15 2.3e+02 1.9e-02     NA   NA\n\n\nDie minimalen durchschnittlichen Teilwerte (MAP), wird als die genauesten empirischen Schätzungen für die Anzahl der beizubehaltenden Faktoren betrachtet. Der MAP Wert schlägt uns einen Faktor vor, entsprechend gehen wir im folgenden von einem Faktor bzw. einer lateten Variable aus.\n\n\n1.3.4 Rotation der Faktoren\nBei der Durchführung der Faktorenanalyse werden sogenannte Faktorladungen ermittelt, die anzeigen, wie stark jede Variable mit den extrahierten Faktoren zusammenhängt. Während des Analyseprozesses kann es vorkommen, dass die Faktorladungen rotiert werden, um eine eindeutigere und interpretierbarere Struktur der Faktoren zu erzielen. Die Rotation der Faktorladungen ermöglicht es, die Ausprägung der Faktoren auf weniger, aber stärker ausgeprägte Variablen zu konzentrieren, was die Interpretation und Verständlichkeit der Analyseergebnisse erleichtert. Es gibt verschiedene Rotationsmethoden, wie beispielsweise die Varimax- oder Quartimax-Rotation, die je nach Ziel der Faktorenanalyse angewendet werden können. Es existieren Dutzende von analytischen Rotationsmethoden, wobei Varimax die beliebteste orthogonale Rotationsmethode ist, während Promax und Oblimin die beliebtesten schrägen Rotationsmethoden sind. Sowohl bei Promax als auch bei Oblimin können wir den Grad der Korrelation zwischen den Faktoren kontrollieren (über die Parameter Kappa bzw. Delta).\nWichtig ist, dass sich die Interpretation der Faktorladungen zwischen orthogonalen und schrägen Rotationen unterscheiden. Bei orthogonalen Lösungen können die Faktorladungen als Korrelationen zwischen gemeinsamen Faktoren und gemessenen Variablen interpretiert werden. Diese Korrelationen reichen von -1,00 bis +1,00, und der Anteil der Varianz in einer gemessenen Variablen, der durch einen gemeinsamen Faktor beigetragen wurde, kann durch Quadrieren der Faktorladung berechnet werden. Im Gegensatz dazu ergeben sich bei schrägen Lösungen zwei verschiedene Arten von Faktorladungen: Struktur- und Musterkoeffizienten. Strukturkoeffizienten können auch als Korrelationen zwischen gemeinsamen Faktoren und den gemessenen Variablen interpretiert werden. Im Gegensatz dazu sind die Musterkoeffizienten keine einfachen Faktor-Variablen-Korrelationen mehr, sondern sie ähneln standardisierten partiellen Regressionskoeffizienten. Das heißt, sie sind Korrelationen zwischen gemeinsamen Faktoren und gemessenen Variablen, nachdem der Einfluss aller anderen gemeinsamen Faktoren kontrolliert (herausgerechnet) wurde. Dementsprechend können Musterkoeffizienten den Wert von 1,00 überschreiten und können nicht quadriert werden, um den Anteil der Varianz zu ermitteln, der eindeutig auf einen gemeinsamen Faktor zurückzuführen ist.\nIn unserem Fall greifen wir auf die etablierte Rotationsmethode varimax zurück, welches wir entsprechend im R-Code spezifizieren. Zusätzlich geben wir unsere erwartete Anzahl an Faktoren an, welche wir zuvor bestimmt haben (in unserem Fall: 1).\n\n# Wir führen die Faktorenanalyse aus und speichern die Ergebnisse in dem Objekt fit ab\nfit =  fa(allbus_vertrauen, factors = 1, fm = \"ml\", rotation = \"varimax\")\n\n# Anzeigen der Ergebnisse mit 2 Nachkommastellen und dem Ausblenden von Faktorladungen die kleiner als 0.3 sind\n\nprint(fit, digits = 2, cutoff = .3)\n\nFactor Analysis using method =  ml\nCall: fa(r = allbus_vertrauen, fm = \"ml\", factors = 1, rotation = \"varimax\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                      ML1   h2   u2 com\nVer_Gesundheitswesen 0.50 0.25 0.75   1\nVer_BVerfG           0.65 0.42 0.58   1\nVer_Bundestag        0.84 0.71 0.29   1\nVer_Verwaltung       0.60 0.36 0.64   1\nVer_kath_Kirche      0.35 0.12 0.88   1\nVer_evan_Kirche      0.41 0.17 0.83   1\nVer_Justiz           0.68 0.46 0.54   1\nVer_TV               0.58 0.33 0.67   1\nVer_Zeitung          0.61 0.37 0.63   1\nVer_Uni              0.55 0.31 0.69   1\nVer_Regierung        0.86 0.74 0.26   1\nVer_Polizei          0.55 0.30 0.70   1\nVer_Parteien         0.80 0.64 0.36   1\nVer_Kom_EU           0.85 0.72 0.28   1\nVer_EU_Par           0.85 0.72 0.28   1\n\n                ML1\nSS loadings    6.64\nProportion Var 0.44\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  105  with the objective function =  10.26 0.3 with Chi Square =  33164.76\ndf of  the model are 90  and the objective function was  3.17 \n 0.3\nThe root mean square of the residuals (RMSR) is  0.09 \nThe df corrected root mean square of the residuals is  0.1 \n 0.3\nThe harmonic n.obs is  3238 with the empirical chi square  5475.99  with prob &lt;  0 \n 0.3The total n.obs was  3238  with Likelihood Chi Square =  10247.57  with prob &lt;  0 \n 0.3\nTucker Lewis Index of factoring reliability =  0.641\nRMSEA index =  0.187  and the 90 % confidence intervals are  0.184 0.19 0.3\nBIC =  9520.13\nFit based upon off diagonal values = 0.96\nMeasures of factor score adequacy             \n                                                   ML1\nCorrelation of (regression) scores with factors   0.97\nMultiple R square of scores with factors          0.94\nMinimum correlation of possible factor scores     0.89\n\n\n\n\n1.3.5 Interpretation der Ergebnisse\nBei der Betrachtung des Outputs der explorativen Faktorenanalyse beginnen wir mit den Faktorladungen. Faktorladungen geben an, wie stark jede Variable mit den extrahierten Faktoren korreliert. Hohe positive Ladungen zeigen eine starke Beziehung zwischen der Variable und dem Faktor an, während hohe negative Ladungen darauf hindeuten, dass die Variable invers mit dem Faktor zusammenhängt. Variablen mit Ladungen nahe null haben wenig oder keine Beziehung zum jeweiligen Faktor. Durch die Betrachtung dieser Ladungen können wir die die Faktoren interpretieren und auch benennen. Auf diese Art und Weise können wir Rückschlüsse auf die zugrunde liegende latente Variable ziehen. In unserem Fall übersteigen für alle Indikatoren die Faktorladungen über den Wert 0.3, was auf einen relativ starken und damit für uns guten Zusammenhang mit dem Faktor bzw. der latenten Variable spricht.\nZusätzlich zur Interpretation der Faktorladungen ist es auch wichtig, die Uniquenes der Faktoren zu berücksichtigen. Diese zeigen an, wie viel Varianz in den Daten von jedem extrahierten Faktor erklärt wird. In anderen Worten sagt sie also aus, wie gut die Information der Variablen in den Faktoren insgesamt erhalten geblieben ist.\n\nprint(fit$uniquenesses, digits = 2)\n\nVer_Gesundheitswesen           Ver_BVerfG        Ver_Bundestag \n                0.75                 0.58                 0.29 \n      Ver_Verwaltung      Ver_kath_Kirche      Ver_evan_Kirche \n                0.64                 0.88                 0.83 \n          Ver_Justiz               Ver_TV          Ver_Zeitung \n                0.54                 0.67                 0.63 \n             Ver_Uni        Ver_Regierung          Ver_Polizei \n                0.69                 0.26                 0.70 \n        Ver_Parteien           Ver_Kom_EU           Ver_EU_Par \n                0.36                 0.28                 0.28 \n\n\nWenn wir auf die Uniqueness der einzelnen Indikatoren blicken, wird relativ schnell klar, dass wir eine Reihe von Indikatoren mit hohen Uniqueness-Werten haben, deren Varianz zu großen Teilen von der latenten Variable erklären wird und somit stärker als andere Indikatoren zu der latenten Variable “beitragen”.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ die Werte des Kaiser-Meyer-Olkin Kritierums (KMO)\n✅ das Ergebnis des Barlett-Tests\n✅ die Faktorladungen\n✅ die Uniqueness\n✅ die gewählte Rotations-Methode\nDas Format ist normalerweise:\n\nBeispiel: Zunächst wurde Faktorenanalyse der 15 gemessenen Indikatoren durchgeführt. Das Kaiser-Meyer-Olkin (KMO)-Maß für die Stichprobenadäquanz betrug 0,89. Dies deutet darauf hin, dass die Korrelationsmuster relativ kompakt sind und die Faktorenanalyse eindeutige und zuverlässige Faktoren ergeben sollte. Der Bartlett-Test auf Sphärizität war ebenfalls signifikant (χ2(105) = 33164.76, p &lt; .001). Dies bedeutet, dass es einige Beziehungen zwischen den untersuchten Variablen gibt. Sowohl der KMO-Test als auch der Bartlett-Test bestätigen, dass die Faktorenanalyse angemessen ist.\n\n\nDie Faktoren werden rotiert, um eine einfache Struktur zu erhalten. In diesem Fall wurde die varimax Rotationsmethode verwendet. Nach sorgfältiger Betrachtung der zusammenhängenden Variablen in der Analyse wurden dann die Faktorbezeichnung vorgeschlagen und in Tabelle 1 dargestellt. Dabei handelt es sich um Vertrauen in gesellschaftliche Institutionen. Für den Faktor wurden Faktorladungen erstellt (siehe Tabelle 1)."
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "title": "Die Faktorenanalyse",
    "section": "1.1 Teildatensatz mit den benötigten Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Variablen\nDie Variablen werden aufgrund ihrer Nützlichkeit als Indikatoren für die zu untersuchende latente Variable ausgewählt. Entsprechend ist es wichtig, dass die Variablen inhaltliche, diskriminante und konvergente Validität aufweisen. Etwas vereinfacht ausgedrückt sollten die Indikatoren über eine inhaltliche Passung zur latenten Variable verfügen, möglichst gut von anderen latenten Variablen abgrenzbar und mit mehreren unterschiedlichen Arten der Messung nachweisbar sein.\nIn unserem Fall möchten wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nFür die statistische Identifizierung einer latenten Variablen bzw. eines Faktors werden mindestens drei gemessene Variablen benötigt, obwohl mehr Indikatoren vorzuziehen sind. Es werden beispielsweise auch vier bis sechs Indikatoren pro Faktor empfohlen. Im Allgemeinen funktioniert die EFA besser, wenn jeder Faktor überdeterminiert ist (d. h. es werden mehrere gemessene Variablen von der zu entdeckenden latenten Variable bzw. Faktor beeinflusst). Unabhängig von der Anzahl sollten Variablen, die voneinander abhängig sind, nicht in eine EFA einbezogen werden.\n\nallbus_vertrauen = daten %&gt;%\n  \n  # Wir wählen mit select() die gewünschten Variablen aus\n  select(pt01:pt20) %&gt;% \n  \n  # die Kombination aus mutate() und across() ermöglicht es uns die Funktion as.numeric() in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden\n  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;% \n\n  # Wir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen)\n  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;% \n  \n  # Wir schmeißen fehlende Werte raus\n  na.omit()\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n  # mit dem rename() Befehl können wir die Variablen umbennen\n  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n# Wir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen\nhtmlTable(head(allbus_vertrauen))\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nNeben der Auswahl der Variablen bzw. Indikatoren müssen auch die Fälle (in unserem Fall die Anzahl der befragten Personen) festgelegt werden. Hier sollten wir uns zunächst fragen, ob die Stichprobe der Teilnehmer:innen in Bezug auf die gemessenen Indikatoren sinnvoll ist? Handelt es sich um eine repräsentative Stichprobe? Bei dem Allbus ist das der Fall und entsprechend können wir davon ausgehen, dass wir eine passende Stichprobe für die Durchführung eine EFA vorliegen haben."
  },
  {
    "objectID": "Skript_5.2.html#referenzen",
    "href": "Skript_5.2.html#referenzen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. https://doi.org/10.1177/0095798418771807"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-würdigung",
    "href": "Skript_5.2.html#danksagung-und-würdigung",
    "title": "Die Faktorenanalyse",
    "section": "1.4 Danksagung und Würdigung",
    "text": "1.4 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-würdigung",
    "href": "Skript_5.3.html#danksagung-und-würdigung",
    "title": "Reliabilität von Skalen",
    "section": "1.9 Danksagung und Würdigung",
    "text": "1.9 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "href": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.4 Verschiedene Arten von Indizes",
    "text": "1.4 Verschiedene Arten von Indizes\nEs gibt eine ganze Reihe von möglichen Arten von Indizes, welche wir theoretisch berechnen könnten.\n\n\n\n\n\n\n\n\nArt des Index\nBildung (Beispiel)\nBeschreibung\n\n\n\n\nUngewichteter additivier Index\nIndex = Indikator_1 + Indikator_2 + Indikator_3\nDie Ausprägungen der Indikatorvariablen werden addiert bzw. zu gemittelt.\n\n\nUngewichteter multiplikativer Index\nIndex = V1 * V2 * V3\nWenn ein Index Mindestausprägungen auf allen Indikatorvariablen voraussetzt sollte multiplikativ zu einem Gesamtindex verknüpft werden.\n\n\nGewichteter additivier Index\nIndex = (2*V1) + V2 + V3\nGewichtete additive Indizes ermöglichen eine differenzierte Behandlung der einzelnen Indikatoren.\n\n\n\nDie Entscheidung, welche Art der Indexbildung gewählt wird sollte vor dem Hintergrund der Daten, sowie der latenten Variable und deren Eigenschaften erfolgen. Beispielsweise würde es für ein Index, welcher die Zufriedenheit mit einer Bahnreise widerspiegelt und aus den Inidkatoren Reisedauer, Service während der Reise, Komfort während der Fahrt gebildet wird, Sinn ergeben einen Ungewichteten multiplikativen Index zu bilden, da bei einer Reisedauer von Null keine Reise stattgefunden hat und somit auch die anderen beiden Indikatoren nicht von Bedeutung sind."
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "1.7 Interpretation von Cronbach’s Alpha",
    "text": "1.7 Interpretation von Cronbach’s Alpha\nZur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  }
]