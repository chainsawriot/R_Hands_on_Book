[
  {
    "objectID": "Skript_7.3.html",
    "href": "Skript_7.3.html",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei mehr als zwei Variablen"
  },
  {
    "objectID": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "href": "Skript_7.3.html#analyselogik-ziel-und-einsatzgebiete-einer-multiplen-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse",
    "text": "1.1 Analyselogik, Ziel und Einsatzgebiete einer multiplen Regressionsanalyse\nIn diesem Notebook gehen wir (wie angekündigt) zunächst näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein. Dann lernen wir die multiple lineare Regression kennen, die es erlaubt, Zusammenhänge zwischen mehreren x-Variablen und einer y-Variablen zu analysieren.\n\n1.1.1 Vorbereitung und Laden der Daten\nZunächst laden wir wieder die Pakete des tidyverse und das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass wir für die Voraussetzungsprüfung brauchen, sowie die Pakete lmtest und sandwich, mit der wir fehlende Voraussetzungen korrigieren können (siehe unten). Die Regression rechnen wir wieder auf Basis des ESS8_vier_laender-Datensatzes, den wir entsprechend einlesen:\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\")\n#install.packages(\"lmtest\")\n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n#install.packages(\"sandwich\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\nlibrary(lmtest)\n\nWarning: Paket 'lmtest' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: zoo\n\n\nWarning: Paket 'zoo' wurde unter R Version 4.3.1 erstellt\n\n\n\nAttache Paket: 'zoo'\n\nDie folgenden Objekte sind maskiert von 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(sandwich)\n\nWarning: Paket 'sandwich' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n1.1.2 Data Management\nAls abhängige Variable nutzen wir für unser Regressionsmodell wieder die Internetnutzung (netustm); als unabhängige Variablen schauen wir uns wie beim letzten Mal das Alter, sowie heute zusätzlich die Rezeptionszeit von politischen Nachrichten (nwspol) sowie das Geschlecht der Befragten (gndr) an. Damit der Output etwas nachvollziehbarer wird, benennen wir diese Variablen mit dem rename-Befehl um.\nDann setzen wir den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifizieren wir wieder, auf welche Variablen sich der Befehl beziehen soll). Das modifizierte Datenset weisen wir einem neuen Datenobjekt zu: daten_mod2\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(internetnutzung = netustm,\n         alter = agea,\n         politische_Nachrichtenrezeption = nwspol,\n         gender = gndr) %&gt;% \n  drop_na(c(internetnutzung, alter, politische_Nachrichtenrezeption, gender)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 158 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\n\n1.1.3 Erinnerung: Einfache lineare Regression mit Alter als UV und Internetnutzung als AV mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nsummary(model) # klassischer Output mit relevanten Kennzahlen\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-211.39  -97.38  -44.70   53.71  487.73 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 279.2690    45.2122   6.177 0.0000000149 ***\nalter        -1.9585     0.9218  -2.125       0.0361 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 150 on 98 degrees of freedom\nMultiple R-squared:  0.04404,   Adjusted R-squared:  0.03428 \nF-statistic: 4.514 on 1 and 98 DF,  p-value: 0.03613\n\n#summary(lm.beta(model)) # klassischer Output mit relevanten Kennzahlen erweitert um standardisierte beta-Koeffizienten\n\nMit diesem Regressionsmodell haben wir übeprüft, ob das Alter die Internetnutzung erklären kann. Im Output sehen wir, dass das Alter einen signifikanten negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um “den Estimate-Wert” in Messeinheiten (hier: -4.296 Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\nSoweit die Wiederholung. Beginnen wir nun mit dem Teil A dieses Skripts, nämlich der Prüfung der Voraussetzungen einer Regressionsanalyse. Vielleicht wundern Sie sich, warum wir die Voraussetzungen erst im zweiten Schritt prüfen? Sie haben Recht: Eigentlich würden wir erst die Voraussetzungen prüfen, dann das Modell schätzen. Wenn wir unser Modell aber schon geschätzt haben, können wir Funktionen zur Prüfung der Voraussetzungen auf unser gesamtes Modell anwenden (bzw. auf das entsprechende Datenobjekt “model”) - und das erspart uns eine Menge “Handarbeit” mit vielen kleinen Zwischenschritten. Zum Beispiel müssten wir für die Prüfung der Voraussetzungen, die die Residuen betreffen, diese erst einmal berechnen und in einer neuen Variable abspeichern. Es ist also weniger Aufwand, die Voraussetzungen ex post zu prüfen.\n\n\n1.1.4 Erinnerung: Voraussetzungen der einfachen linearen Regression:\nBevor wir zum statistischen Teil kommen, lassen Sie uns noch einmal Revue passieren, was die wichtigsten Voraussetzungen der einfachen linearen Regression sind: 1) (quasi-)metrisches Skalenniveau 2) Linearität des Zusammenhangs zwischen x und y 3) Homoskedastizität der Residuen: Varianzen der Residuen der prognostizierten abhängigen Variablen sind gleich 4) Unabhängigkeit der Residuen: ansonsten Autokorrelation, die Aussagekraft reduziert 5) Normalverteilung der Residuen 6) Keine Ausreißer in den Daten, da schon einzelne Ausreißer einen sonst signifikanten Trend zunichte machen können (ggf. also eliminieren)\n\n\n1.1.5 TEIL A: Prüfung der Voraussetzungen einer Regressionsanalyse\n\n1.1.5.1 Prüfung der Voraussetzungen 1 und 2: metrisches Skalenniveau & Linearität des Zusammenhangs\nOb 1.) die Variablen, die wir in das Regressionsmodell einbeziehen wollen (Alter, Internet-Nutzung) metrisch sind, und ob 2.) ein linearer Zusammenhang besteht, haben wir in der letzten Woche bereits überprüft. Für die Prüfung nach der Linearität des Zusammenhangs zwischen x und y hatten wir ein Streudiagramm mit der geschätzten Regressionsgeraden erzeugt.\n\n\n1.1.5.2 Prüfung der Voraussetzungen 3: Homoskedastizität der Residuen\nLineare Modelle setzen eine konstante Fehlervarianz (Homoskedastizität) voraus. Eine weitere Bedingung der Regressionanalyse ist also, dass die Varianzen der Residuen der prognostizierten abhängigen Variablen für alle Werte des Prädiktors gleich sind, so dass das Modell gleich gute Vorhersagen über alle Werte machen kann. Liegt Homoskedastizität vor, sind die Abweichungen der vorhergesagten Werte von den gemessenen Werten konstant gleich groß – unabhängig wie hoch oder niedrig der Wert des Prädiktors ist. Das ist eine wichtige Voraussetzung, denn das Gegenteil - Heterokedastizität der Residuen - würde zur Ineffizienz unserer Schätzung führen! Denn die Standardfehler der Regressionskoeffizienten werden bei vorhandener Heteroskedastizität nach oben verzerrt geschätzt. Das Ergebnis wäre, dass unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben liegt, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen.\nDas klingt kompliziert? Kann sein, aber keine Panik: Mit der Funktionen check_heteroscedasticity() aus dem performance-package können wir sehr einfach prüfen, ob diese Annahme verletzt wurde.\n\ncheck_heteroscedasticity(model)\n\nOK: Error variance appears to be homoscedastic (p = 0.329).\n\n\nDie Interpretation des Outputs ist einfach, weil R hier uns eine sehr konkrete Aussage zur Überpüfung der Annahme macht: Bei grüner Schrift ist das Ergebnis in Ordnung, d.h. die Fehlervarianz scheint homoskedastisch, denn p wäre dann nicht signifikant. Bei roter Schrift ist die Fehlervarianz heteroskedastisch und p ist signifikant (p &lt; 0.05). In diesem Fall liegt unser Regressionsmodell mit seiner Vorhersage systematisch umso weiter daneben, je größer der Prädiktorwert ist, für den wir die abhängige Variable schätzen wollen. Das müssen wir dann bei der Interpretation der Daten berücksichtigen.\nWie das ganze aussieht, können wir uns auch grafisch über die plot-Funktion anschauen. Dazu erzeugen wir ein Streudiagramm, das die vorhergesagten Werte und die Residuen enthält:\n\nplot(model, 1)\n\n\n\nplot(fitted.values(model), rstandard(model))\n\n\n\n\n\n\n1.1.5.3 Was sehen wir im Plot?\nZunächst betrachten wir die Streuung der Punkte im Streudiagramm ohne Linie. Hier können wir bereits sehen, dass eine Zunahme der Streuung bei höheren Werten erkennbar ist, weil wir einen leicht nach rechts geöffneten Trichter haben. Das zweite Diagramm hilft zusätzlich mit einer roten Linie, die bei Homoskedastizität möglichst gerade wird. Ist sie wellig oder hat sie eine positive oder negative Steigung, können wir von Heteroskedastizität ausgehen. Genauere Auskunft gibt aber der oben gerechnete Test!\n\n\n1.1.5.4 Was tun bei Heteroskedastizität der Residuen? Berechnung von HC-Standard Errors!\nLiegt Heteroskedastizität vor, müssen Sie nicht verzweifeln: Erstens ist die Regressionsanalyse sehr robuts gegen die Verletzung ihrer Voraussetzungen. Zweitens können wir diesen Konflikt einigermaßen elegant auflösen, indem wir pauschal robuste Standardfehler schätzen lassen, so dass die Verletzung nicht mehr zu Schätzfehlern führt. In R gibt es (wie immer) verschiedene Wege Heteroskedastizität zu kontern. Eine einfache Lösung bietet das lmtest-Paket mit der coeftest-Funktion in Kombination mit dem Befehl vcov(), der zur Berechnung von heteroscedasticity consistent (HC) standard errors führt. So ermöglichen wir die Berechnung von heteroskedastizitätskonsistenten bzw. heteroskedastizitätsrobusten Schätzern. Nutzen wir diese Lösung, werden die Standardfehler nicht mehr verzerrt und damit auch nicht die t-Werte und p-Werte unserer Schätzung.\nPS: Zur Berechnung von heteroscedasticity consistent (HC) standard errors gibt es verschiedene HC-Funktionen. Hier nutzen wir zunächst Typ 3, die auch Hayes & Cai empfehlen (Hayes, A. F., & Cai, L. (2007): Using heteroskedasticity-consistent standard error estimators in OLS regression: An introduction and software implementation. Behavior research methods, 39(4), 709-722). HC4 (die zweite Variante) ist dann sinnvoll, wenn die Residuen nicht normalverteilt sind. (Wie wir später sehen werden, ist das bei uns leider auch der Fall)\n\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value      Pr(&gt;|t|)    \n(Intercept) 279.2690    44.7423  6.2417 0.00000001109 ***\nalter        -1.9585     0.8609 -2.2749       0.02509 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#coeftest(model, vcov = vcovHC(model, type = \"HC4\")) # diese Variante wählen, wenn Residuen nicht normalverteilt sind \n\nNach der Ausführung erhalten wir eine neue Regressionstabelle. Wenn Sie diese Tabelle mit dem obigen Output unseres Regressionsmodells abgleichen, sehen Sie, dass sich die eigentlichen Koeffizienten (“Estimates”) nicht verändert haben - aber alle Werte, die rechts davon stehen, also Standardfehler (Std. Error), t-Werte und p-Werte. Diese sind nun um unsere Schätzfehler durch Heteroskedastizität korrigiert.\nAlso weiter geht’s!\n\n\n1.1.5.5 Prüfung der Voraussetzungen 3: Unabhängigkeit der Residuen\nAuch die Annahme, dass die Residuen unabhängig voneinander sind, ist eine wichtige Voraussetzung der Regressionsanalyse. Unabhängigkeit der Residuen bedeutet inhaltlich: Wenn ich den Fehlerterm für eine bestimmte Beobachtung kenne, darf mir das keine Information über den Fehlerterm für die nächste Beobachtung liefern. Es darf also nichts systematisch zu einer Verzerrung meiner Beobachtungen (bzw. meiner Fehlerterme) führen. Ansonsten läge eine Autokorrelation der Fehlerterme vor, die die Aussagekraft des Modells reduzieren würde.\nDas performance-package ist einfach soooo cool! Es beinhaltet auch die check_autocorrelation-Funktion, mit der wir diese Annahme sehr einfach prüfen können:\n\ncheck_autocorrelation(model)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.294).\n\n\nAuch hier ist der Output wieder sehr klar: Die Prüfung ergibt, dass die Residuen unabhängig und nicht autokorreliert sind (p = 0,588) - sonst hätten wir auch hier einen signifikanten p-Wert erhalten. Prima!\n\n\n1.1.5.6 Prüfung der Voraussetzungen 4: Normalverteilung der Residuen\nWenn die Residuuen nicht der Normalverteilungskurve folgen, sondern stattdessen eigene “Muster” in ihrer Verteilung aufweisen, kann dies darauf hindeuten, dass wir nicht alle Prädiktoren im Modell berücksichtigt haben und somit ein Teil der erklärenden Information in die Residuen übergeht, wo sie das erkennbare Muster “verursacht”.\nAuch die Voraussetzung, dass die Residuen normalverteilt sein sollen, lässt sich mit einer Funktion aus dem performance-Package sehr einfach überprüfen:\n\ncheck_normality(model)\n\nWarning: Non-normality of residuals detected (p &lt; .001).\n\n\nAuch hier ist das Ergebnis ohne Probleme zu interpretieren, weil R hier eine “direkte Ansage” macht. In unserem Fall ist die Voraussetzung der Normalverteilung verletzt, weil p signifikant wird. Das müssen wir bei der Interpretation der Daten berücksichtigen. Grundsätzlich können wir hier wieder darauf verweisen, dass die Regressionsanalyse robust gegen die Verletzung ihrer Voraussetzungen ist. Eine Alternative ist, dass wir ein Bootstrapping-Verfahren auf unsere Daten anwenden. Das aber nur zur Info, wenn Sie hier selbstständig weitermachen wollen - das würde jetzt etwas zu weit führen :) Außerdem werden wir unten bei der zusätzlichen visuellen Inspektion mit der Funktion check_models auch noch sehen, dass unsere Annahme nicht allzu schlimm verletzt ist.\n\n\n1.1.5.7 Prüfung der Voraussetzungen 5: Ausreißer im Modell\nAusreißer sind ein Problem für viele parametrische Verfahren, denn einzelne Ausreißer können einen sonst signifikanten Trend zunichte machen (ggf. also eliminieren). Ob es in unserem Modell Ausreißer gibt, kann ich wieder mit einer sehr einfachen Funktion aus dem performance-Package prüfen, die auf das sogenannte “cooks distance” zurückgreift. Der Wert gibt mir Auskunft darüber, welchen Einfluss mögliche Ausreißer auf das Modell haben.\n\ncheck_outliers(model)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.7).\n- For variable: (Whole model)\n\n\nIn unserem Fall gibt es keine Ausreißer, die das Modell beinträchtigen - vielleicht hätten wir sonst auch keinen signifikanten Zusammenhang beobachten können.\n\n\n1.1.5.8 Add-on: Visuelle Inspektion der Modellgüte bzw. der Modellannahmen\nEs gibt im performance-Package auch eine sehr coole Funktion, die mir eine visuelle Inspektion meiner Modellgüte bzw. verschiedenen Modellannahmen erlaubt (Normalität der Residuen, Normalität der zufälligen Effekte, lineare Beziehung, Homogenität der Varianz, Multikollinearität). Mit der check_model-Funktion kann ich mir dazu mehrere Grafiken im Überblick ausgeben lassen. YEAH! :)\n\ncheck_model(model)\n\nNot enough model terms in the conditional part of the model to check for\n  multicollinearity.\n\n\n\n\n\n\n\n\n1.1.6 TEIL B: Die multiple lineare Regression\n\n1.1.6.1 Anwendungsbereich der multiplen linearen Regression\nDie multiple lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen metrischen Variablen besteht. Die multiple lineare Regressionsanalyse hat das Ziel, eine abhängige Variable (y) mittels mehrerer unabhängigen Variablen (x1, x2, …) zu erklären. (Zur Erinnerung: Für nur eine x-Variable nutzen wir die einfache lineare Regression)\nMit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen den unabhängigen und der einen abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variablen vorhergesagt werden?\nDie multiple Regression entspricht in ihrer Analyslogik also der einfachen linearen Regression - nur dass sie mehr als eine unabhängige Variable berücksichtigt.\n\n\n1.1.6.2 Ziel der Analyse\nMit Hilfe der multiplen Regression wollen wir die Annahme prüfen, dass die Variablen Alter (agea) sowie die Rezeptionszeit von politischen Nachrichten (nwspol) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) haben bzw. diese erklären und vorhersagen können. Alle Variablen sind metrisch und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n1.1.6.3 Modell zum Zusammenhang von Alter, politischer Nachrichtenrezeption und Internetnutzung spezifizieren und anzeigen lassen\nDie Berechnung der multiplen Regression unterscheidet sich nicht stark von der Berechnung der einfachen linearen Regression. In die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) einfach die zusätzliche unabhängige Variable (politische_Nachrichtenrezeption) ein, indem wir sie mit einem + Zeichen anhängen:\n\nmodel_m &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption, data = daten_mod)\nsummary(lm.beta(model_m))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption, \n    data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-203.71  -92.77  -47.73   38.81  457.27 \n\nCoefficients:\n                                Estimate Standardized Std. Error t value\n(Intercept)                     261.8715           NA    44.9628   5.824\nalter                            -2.0528      -0.2200     0.9041  -2.271\npolitische_Nachrichtenrezeption   0.2785       0.2186     0.1234   2.256\n                                    Pr(&gt;|t|)    \n(Intercept)                     0.0000000745 ***\nalter                                 0.0254 *  \npolitische_Nachrichtenrezeption       0.0263 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147 on 97 degrees of freedom\nMultiple R-squared:  0.09171,   Adjusted R-squared:  0.07299 \nF-statistic: 4.897 on 2 and 97 DF,  p-value: 0.009415\n\n\n\n\n1.1.6.4 Interpretation des Outputs: Was sehen wir in der Regressionstabelle?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängigen Variablen “alter” und “politische_Nachrichtenrezeption” zu erklären.\n\n\n1.1.6.5 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n1.1.6.6 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n1.1.6.7 Standardized\nDiese Estimates sind die standardisierte b-Werte. Weil wir diese über die lm.beta-Funktion standardisiert haben, lassen sich die Koeefizienten auch bei unterschiedlicher Skalierung vergleichen.\n\n\n1.1.6.8 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n1.1.6.9 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n1.1.6.10 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n1.1.6.11 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter und politische Nachrichtenrezeption auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter und die politische Nachrichtenrezeption etwa 20 Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n1.1.6.12 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n1.1.6.13 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n1.1.6.14 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (-4.9169) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &lt; .05 statistisch signifikant. Diese Ergebnisse überraschen uns nicht: Den Einfluss des Alters haben wir ja letzte Woche schon überprüft. Mit der Erweiterung zur multiplen Regression können wir nun zusätzlich sagen, dass die politische Nachrichtenrezeption auch einen Einfluss auf die Internet-Nutzung hat, denn der Wert ist ebenfalls signifikant (p &lt; .05)! Die F-Statistik sagt uns zusätzlich, dass auch unser Gesamtmodell signifikant ist (p-value: 0.00001239, also &lt; .05).\n\n\n1.1.6.15 Erinnerung: Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model_m))\n\n# A tibble: 3 × 6\n  term                         estimate std_estimate std.error statistic p.value\n  &lt;chr&gt;                           &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)                   262.          NA        45.0        5.82 7.45e-8\n2 alter                          -2.05        -0.220     0.904     -2.27 2.54e-2\n3 politische_Nachrichtenrezep…    0.278        0.219     0.123      2.26 2.63e-2\n\n\n\nglance(model_m)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0917        0.0730  147.      4.90 0.00941     2  -639. 1287. 1297.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "href": "Skript_7.3.html#grafische-darstellung-und-interpretation-einer-multiplen-regressionsgeraden",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden",
    "text": "1.2 Grafische Darstellung und Interpretation einer multiplen Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.3.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.3 Prüfung der Voraussetzungen",
    "text": "1.3 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "href": "Skript_7.3.html#berechnung-und-interpretation-einer-multiplen-regression",
    "title": "Zusammenhänge bei mehr als zwei Variablen",
    "section": "1.4 Berechnung und Interpretation einer multiplen Regression",
    "text": "1.4 Berechnung und Interpretation einer multiplen Regression"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Skript_1.1.html",
    "href": "Skript_1.1.html",
    "title": "Einführung in R und RStudio",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "Skript_1.2.html",
    "href": "Skript_1.2.html",
    "title": "R-Studio",
    "section": "",
    "text": "1 Wie funktioniert R-Studio?"
  },
  {
    "objectID": "Skript_1.3.html",
    "href": "Skript_1.3.html",
    "title": "Die Logik von Markdown und Quarto",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "Skript_2.1.html",
    "href": "Skript_2.1.html",
    "title": "Der Aufbau von Datensätzen",
    "section": "",
    "text": "1 Was ist ein Datensatz und wie ist dieser aufgebaut?\n\n\n2 Klassische Formen von Datensätzen in der Kommunikationswissenschaft"
  },
  {
    "objectID": "Skript_2.2.html",
    "href": "Skript_2.2.html",
    "title": "klassische Formen von Datensätzen",
    "section": "",
    "text": "1 Einlesen von Datensätzen und Praktische Tipps"
  },
  {
    "objectID": "Skript_2.3.html",
    "href": "Skript_2.3.html",
    "title": "Der ALLBUS Datensatz",
    "section": "",
    "text": "1 Der ALLBUS"
  },
  {
    "objectID": "Skript_3.2.html",
    "href": "Skript_3.2.html",
    "title": "Datentypen und -strukturen",
    "section": "",
    "text": "1 Datentypen und -strukturen in R\nGrundsätzlicher Aufbau von einem Datensatz (Was ist ein Fall? Was ist eine Spalte? Was eine Zeile?) Überblick über die Daten erhalten Einführung in das Datenmanagement: Objekte festlegen und Variablen definieren Datentypen und deren Charakteristika Datentypen, Vektoren, Matrizen, und Data Frames in R Vorschau: Manipulation und Transformation von Daten jeweils alles mit dplyr"
  },
  {
    "objectID": "Skript_3.3.html",
    "href": "Skript_3.3.html",
    "title": "Selektion, Manipulation und Transformation",
    "section": "",
    "text": "1 Selektion, Manipulation und Transformation von Daten\nDas tidyverse-Universum Manipulation und Transformation von Daten Zufallsstichprobe ziehen Daten filtern/Fälle und Spalten auswählen; Subsets bilden aufgrund von konditionalen Bedingungen Gruppieren von Fällen (group_by)\nUmgang mit realen, nicht-sauberen Datensätzen an konkreten Beispielen (z.B. missing data; -99/-77, invertierte Items; “weiß nicht”; fehlende Faktoren) Variablen recodieren / rename (Neue) Variablen berechnen (z.B. mehrere Variablen in einem Index bzw. einer Skala zusammenfassen)"
  },
  {
    "objectID": "Skript_3.4.html",
    "href": "Skript_3.4.html",
    "title": "Tabellen und Grafiken in R",
    "section": "",
    "text": "1 Eine Einführung in GGPlot"
  },
  {
    "objectID": "Skript_4.1.html",
    "href": "Skript_4.1.html",
    "title": "Häufigkeiten und deren Visualisierung",
    "section": "",
    "text": "1 Berechnung von einfachen Häufigkeiten\n\n\n2 Visuelle Darstellung von einfachen Häufigkeiten\n\n\n3 Interpretation von einfachen Häufigkeiten"
  },
  {
    "objectID": "Skript_4.2.html",
    "href": "Skript_4.2.html",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "",
    "text": "Berechnung von Lageparametern"
  },
  {
    "objectID": "Skript_4.2.html#berechnung",
    "href": "Skript_4.2.html#berechnung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung",
    "href": "Skript_4.2.html#visualisierung",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation",
    "href": "Skript_4.2.html#interpretation",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-1",
    "href": "Skript_4.2.html#berechnung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-1",
    "href": "Skript_4.2.html#visualisierung-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-1",
    "href": "Skript_4.2.html#interpretation-1",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.2.html#berechnung-2",
    "href": "Skript_4.2.html#berechnung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.2.html#visualisierung-2",
    "href": "Skript_4.2.html#visualisierung-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.2.html#interpretation-2",
    "href": "Skript_4.2.html#interpretation-2",
    "title": "Lageparameter: Modus, Median, Mittelwert",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html",
    "href": "Skript_4.3.html",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "",
    "text": "Berechnung und Interpretation von Streuungsmaßen"
  },
  {
    "objectID": "Skript_4.3.html#berechnung",
    "href": "Skript_4.3.html#berechnung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung",
    "href": "Skript_4.3.html#visualisierung",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation",
    "href": "Skript_4.3.html#interpretation",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-1",
    "href": "Skript_4.3.html#berechnung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-1",
    "href": "Skript_4.3.html#visualisierung-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-1",
    "href": "Skript_4.3.html#interpretation-1",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_4.3.html#berechnung-2",
    "href": "Skript_4.3.html#berechnung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.1 Berechnung",
    "text": "3.1 Berechnung"
  },
  {
    "objectID": "Skript_4.3.html#visualisierung-2",
    "href": "Skript_4.3.html#visualisierung-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.2 Visualisierung",
    "text": "3.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.3.html#interpretation-2",
    "href": "Skript_4.3.html#interpretation-2",
    "title": "Streuungsmaße: Spannweite, Standartabweichung, Varianz",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html",
    "href": "Skript_4.4.html",
    "title": "Verteilungen und deren Visualisierung",
    "section": "",
    "text": "Berechnung und Interpretation von Verteilungen"
  },
  {
    "objectID": "Skript_4.4.html#berechnung",
    "href": "Skript_4.4.html#berechnung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.1 Berechnung",
    "text": "1.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung",
    "href": "Skript_4.4.html#visualisierung",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.2 Visualisierung",
    "text": "1.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation",
    "href": "Skript_4.4.html#interpretation",
    "title": "Verteilungen und deren Visualisierung",
    "section": "1.3 Interpretation",
    "text": "1.3 Interpretation"
  },
  {
    "objectID": "Skript_4.4.html#berechnung-1",
    "href": "Skript_4.4.html#berechnung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.1 Berechnung",
    "text": "2.1 Berechnung"
  },
  {
    "objectID": "Skript_4.4.html#visualisierung-1",
    "href": "Skript_4.4.html#visualisierung-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.2 Visualisierung",
    "text": "2.2 Visualisierung"
  },
  {
    "objectID": "Skript_4.4.html#interpretation-1",
    "href": "Skript_4.4.html#interpretation-1",
    "title": "Verteilungen und deren Visualisierung",
    "section": "2.3 Interpretation",
    "text": "2.3 Interpretation"
  },
  {
    "objectID": "Skript_5.1.html",
    "href": "Skript_5.1.html",
    "title": "Die Messung von latenten Variablen",
    "section": "",
    "text": "Eine überfüllte & scheinbar unstrukturierte Stadt an der Oberfläche, Bild generiert von Midjourney\n\n\n\n1 Einführung in die Messung von latenten Variablen\nIn der Kommunikationswissenschaft, beziehungsweise in den Sozialwissenschaften allgemein, bezieht sich der Begriff latente Variable auf eine nicht direkt beobachtbare Eigenschaft oder einen nicht direkt messbaren Faktor, der sich jedoch durch mehrere beobachtbare Variablen (teilweise auch als Indikatoren bezeichnet) manifestiert. Latente Variablen sind theoretische Konstrukte, die nicht direkt gemessen werden können. Als Wissenschaftler:innen gehen wir allerdings davon aus, dass diese Konstrukte existieren und wir somit komplexe gesellschaftliche Phänomene mit der Hilfe von latenten Variablen besser verstehen können.\nWenn wir uns jetzt ganz konkret mit dem Vertrauen in gesellschaftliche Institutionen beschäftigen, ist Vertrauen eine latente Variable. Wir können Vertrauen in gesellschaftliche Institutionen nicht direkt beobachten, wir sehen beispielsweise einem Menschen nicht an, ob er oder sie dem Bundesverfassungsgericht als eine von mehreren gesellschaftlichen Institutionen stark oder nicht so stark vertraut. Wenn wir uns als Forscher:innen fragen, inwieweit die Menschen in Deutschland den gesellschaftlichen Institutionen vertrauen, müssen wir zunächst theoretisch klären was wir unter dem Vertrauen in gesellschaftliche Institutionen verstehen.\nDas Vertrauen in gesellschaftliche Institutionen ist ein abstraktes und komplexes theoretisches Konstrukt, das sich aus verschiedenen Bestandteilen zusammensetzt. Diese einzelnen Bestandteile (Vertrauen in das Bundesverfassungsgericht, Vertrauen in den Bundestag, etc.) können wir durch sogenannte Indikatoren beobachten und damit messbar machen (in einer Befragung wäre das bspw. die Frage nach dem Vertrauen in das Bundesverfassungsgericht). Auf der Grundlage dieser Indikatoren versuchen wir dann auf das Vertrauen in gesellschaftliche Institutionen zu schließen.\nLatente Variablen spielen eine wichtige Rolle in der statistischen Modellierung und Analyse, insbesondere in der Faktorenanalyse. Durch das Einbeziehen von latenten Variablen in die Analyse, können komplexe Beziehungen und Zusammenhänge zwischen verschiedenen Variablen besser verstanden und erklärt werden. Zusätzlich ist eine Reduktion von Komplexität möglich, indem mehrere beobachtbare Variablen (bzw. Indikatoren) in einer latenten Variable zusammengefasst werden, was zu einem besseren Verständnis der zugrunde liegenden Phänomene führen kann. Der Prozess des Zusammenfassens wird häufig auch als Indexbildung bezeichnet.\nDie Indexbildung umfasst drei relevante Schritte: (1) Zunächst muss ein theoretisches Konstrukt entwickelt werden. In unserem Fall würde das bedeuten, dass wir ein theoretisches Verständnis von Vertrauen in gesellschaftliche Institutionen entwickeln und uns klar werden was wir darunter verstehen. (2) Im nächsten Schritt müssten wir klären, ob sich dieses theoretisches Konstrukt in Form einer latenten Variable auch in unseren Daten finden lässt, hierfür werden wir eine explorative Faktorenanalyse durchführen. (3) Als letzten Schritt führen wir die eigentliche Indexbidlung durch. Hier berechnen wir eine neue Variable, den Index für Vertrauen in gesellschaftliche Institutionen und Überprüfen dessen Güte."
  },
  {
    "objectID": "Skript_5.2.html",
    "href": "Skript_5.2.html",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Das Entdecken der zugrunde liegenden Struktur der Stadt, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_5.3.html",
    "href": "Skript_5.3.html",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Validieren, Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_6.1.html",
    "href": "Skript_6.1.html",
    "title": "Bivariate Statistik & Mittelwertvergleiche",
    "section": "",
    "text": "1 Einführung in die Bivariate Statistik und Mittelwertvergleiche\nStatistische Tests erlauben es uns, auf der Basis von quantitativen Daten eine begründete Entscheidung über die Gültigkeit oder Ungültigkeit einer Hypothese zu treffen.\nBei den hier verwendeten Beispielen geht es darum, Unterschiede zwischen zwei oder mehr Stichproben (oder Teilstichproben) statistisch zu überprüfen.\nEin statistischer Test liefert Aufschluss darüber, wie wahrscheinlich es ist, dass wir die Nullhypothese verwerfen und/oder die alternative Hypothese annehmen können.\nKönnen wir aufgrund des Testausgangs davon ausgehen, dass ein (nicht zufälliger) Unterschied zwischen zwei Stichproben besteht, so sprechen wir von einem signifikanten (überzufälligen) Unterschied."
  },
  {
    "objectID": "Skript_6.2.html",
    "href": "Skript_6.2.html",
    "title": "Bestimmen von Unterschieden in der Varianz mit Kreuztabellen und dem Chi-Quadrat Test",
    "section": "",
    "text": "Bestimmen von Unterschieden in der Varianz\n\n1 Kreuztabellen und Chi-Quadrat Test"
  },
  {
    "objectID": "Skript_6.3.html",
    "href": "Skript_6.3.html",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "",
    "text": "Bestimmen von Unterschieden in der zentralen Tendenz"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-unabhängigen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben",
    "text": "1.1 Das Testen von Unterschieden bei unabhängigen Stichproben\n\n1.1.1 t-Test für unabhängige Stichproben\n\n\n1.1.2 Mann-Whitney\n\n\n1.1.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n1.1.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n\nCode\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\n\nCode\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n\n1.1.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n1.1.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n\nCode\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n\nCode\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n\nCode\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n1.1.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n\nCode\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n1.1.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n\nCode\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\n\nCode\nprint(fit)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n1.1.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt.\n\n\n\n1.1.4 Kruskal Wallis\n\n\n1.1.5 mehrfaktorielle Varianzanalyse\n\n1.1.5.1 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n\nCode\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\n\nCode\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\n\nCode\nprint(fit2)\n\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n\n1.1.5.2 Post-Hoc Tests\n\n\nCode\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n1.1.5.3 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n\nCode\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\n\n\nCode\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "href": "Skript_6.3.html#das-testen-von-unterschieden-bei-verbundenen-stichproben",
    "title": "Zentrale Tendenz",
    "section": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben",
    "text": "1.2 Das Testen von Unterschieden bei verbundenen Stichproben\n\n1.2.1 t-Test für verbundene Stichproben\n\n\n1.2.2 Wilcoxon"
  },
  {
    "objectID": "Skript_6.4.html",
    "href": "Skript_6.4.html",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Die Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'rootSolve', 'lmom', 'expm', 'Exact', 'gld'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'rootSolve' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'lmom' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'expm' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Exact' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gld' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'DescTools' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nDescTools installed\n\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'multcompView', 'gmp', 'Rmpfr', 'kSamples', 'BWStest'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'multcompView' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gmp' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Rmpfr' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'kSamples' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'BWStest' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'PMCMRplus' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nPMCMRplus installed\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "Skript_7.1.html",
    "href": "Skript_7.1.html",
    "title": "Einführung",
    "section": "",
    "text": "1 Einführung in das Überprüfen von Zusammenhängen\nWillkommen zu unserem Kapitel über die statistische Überprüfung von Zusammenhängen mit Hilfe von R. In der datengetriebenen Welt von heute ist es von großer Bedeutung, statistische Zusammenhänge zu verstehen und zu validieren. In diesem Kapitel werden wir Ihnen zeigen, wie Sie mithilfe von R verschiedene statistische Tests durchführen können, um Zusammenhänge zwischen Variablen zu untersuchen.\nDer erste Schritt besteht darin, die Daten in R zu importieren und zu explorieren. Wir werden Ihnen zeigen, wie Sie die Daten visualisieren und grundlegende statistische Kennzahlen berechnen können, um einen ersten Eindruck von den vorliegenden Zusammenhängen zu bekommen. Anschließend werden wir auf verschiedene statistische Tests eingehen, darunter den Korrelationstest, den Chi-Quadrat-Test und verschiedene Formen der Regression. Sie lernen, wie Sie diese Tests in R implementieren, die Ergebnisse interpretieren und fundierte Schlussfolgerungen ziehen können.\nDarüber hinaus werden wir auf wichtige Konzepte wie Signifikanzniveau, p-Wert und Konfidenzintervalle eingehen, um Ihnen ein solides Verständnis dafür zu vermitteln, wie statistische Zusammenhänge bewertet werden können. Durch die Anwendung dieser Methoden werden Sie in der Lage sein, Ihre Daten genau zu analysieren, potenzielle Zusammenhänge zu identifizieren und deren Bedeutung zu bewerten. Tauchen Sie ein in die spannende Welt der statistischen Überprüfung von Zusammenhängen mit R und erweitern Sie Ihr analytisches Toolkit!\n(Es handelt sich bei dem Text um einen Platzhalter, erstellt von chatGDP)"
  },
  {
    "objectID": "Skript_7.2.html",
    "href": "Skript_7.2.html",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "",
    "text": "Das Überprüfen von Zusammenhängen bei zwei Variablen"
  },
  {
    "objectID": "Skript_7.2.html#korrelation",
    "href": "Skript_7.2.html#korrelation",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.1 Korrelation",
    "text": "3.1 Korrelation"
  },
  {
    "objectID": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "href": "Skript_7.2.html#analyselogik-ziel-und-einsatzgebiete-einer-regressionsanalyse",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse",
    "text": "3.2 Analyselogik, Ziel und Einsatzgebiete einer Regressionsanalyse\nIn diesem Notebook wird die einfache lineare Regression auf Grundlage der ESS-Daten vorgestellt. In der nächsten Sitzung gehen wir näher auf die Prüfung der Voraussetzungen einer Regressionsanalyse ein und lernen die multiple lineare Regression kennen.\n\n\n\nPicture generated by Midjourney\n\n\n\n3.2.1 Anwendungsbereich der linearen Regression\nDie einfache lineare Regressionsanalyse wird angewandt, wenn geprüft werden soll, ob ein (als linear vermuteter) Zusammenhang zwischen zwei metrischen Variablen besteht. Sie wird daher auch als bivariate Regression bezeichnet.\nZiel ist es, die Beziehung zwischen einer abhängigen Variable (auch erklärte Variable, Regressand oder Prognosevariable genannt) und einer oder mehreren unabhängigen Variablen (oft auch erklärende Variable, Regressor oder Prädiktorvariable) zu analysieren, um Zusammenhänge quantitativ zu beschreiben und zu erklären und/oder Werte der abhängigen Variable mit Hilfe der unabhängige Variable (des Prädiktors) zu prognostizieren. Mit Hilfe der Regressionsanalyse können drei Arten von Fragestellungen untersucht werden: 1) Ursachenanalyse: Gibt es einen Zusammenhang zwischen der unabhängigen und der abhängigen Variable? Wie stark ist dieser? 2) Wirkungsanalyse: Wie verändert sich die abhängige Variable bei einer Änderung der unabhängigen Variablen? 3) Prognose: Können die Messwerte der abhängigen Variable durch die Werte der unabhängigen Variable vorhergesagt werden?\n\n\n3.2.2 Vorbereitung und Laden der Daten\nZunächst laden wir die Pakete des tidyverse. Weiterhin laden wir das Pakete broom um die normale Ausgabe der Funktion lm (für die Berechnung linearer Modelle) in ein etwas anschaulicheres Format umwandeln zu können. Außerdem laden wir das Paket performance, dass uns später zusätzlich einige Indikatoren ausgibt.\nDen Datensatz finet ihr hier.\n\n#Laden der notwendigen Pakete\n#install.packages(\"lm.beta\") \n#install.packages(\"broom\") \n#install.packages(\"performance\")\n#install.packages(\"see\")\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lm.beta)\n\nWarning: Paket 'lm.beta' wurde unter R Version 4.3.1 erstellt\n\nlibrary(broom) # hier stecken einige Befehle zur Bereinigung der Daten und der Modelloutputs drin\n\nWarning: Paket 'broom' wurde unter R Version 4.3.1 erstellt\n\nlibrary(performance)\n\nWarning: Paket 'performance' wurde unter R Version 4.3.1 erstellt\n\nlibrary(see)\n\nWarning: Paket 'see' wurde unter R Version 4.3.1 erstellt\n\n#Daten laden und zum Datenobjekt \"daten\" zuweisen\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund der Grafiken in ggplot festlegen\ntheme_set(theme_minimal())\n\n# Anzeige der p-Werte als Zahlen mit Nachkommastellen einstellen\noptions(scipen = 999) \n\n\n\n3.2.3 Ziel der Analyse\nMit Hilfe der Regression wollen wir die Annahme prüfen, dass das Alter (agea) der Befragten einen Einfluss auf die Internetnutzung (netustm, in Minuten) hat. Beides sind metrische Variablen und erfüllen damit die Voraussetzung, dass eine Regression gerechnet werden kann. (Achtung: auch kategorische Variablen können bei der Regressionsanalyse eingesetzt werden, sie müssen dann aber durch Dummy-Coding passend gemacht werden).\n\n\n3.2.4 Data Management\nDamit der Output etwas nachvollziehbarer wird, benenne ich die Variablen mit dem rename-Befehl zunächst einmal um. Dann nutze ich den drop_na-Befehl, um alle Fällen mit fehlenden Werten zu entfernen (in der Klammer spezifiziere ich, auf welche Variablen sich der Befehl beziehen soll). Das alles weise ich einem neuen Datenobjekt zu: daten_mod\nSchließlich nutze ich den slice_sample-Befehl, um aus unseren 8432 Fällen ein Zufallssample von n=100 Fällen zu ziehen, weil mir das die visuelle Interpretation erleichtert (diesen Befehl könnten wir hier auch weglassen, dann bekommen wir unten aber sehr sehr viele Datenpunkt in unserem Streudiagramm - probieren Sie es mal aus!)\n\ndaten_mod &lt;- daten %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm) %&gt;% \n  drop_na(c(alter, internetnutzung)) %&gt;% \nslice_sample(n = 100) \ndaten_mod\n\n# A tibble: 100 × 166\n        idno cntry gndr   alter marsts   edubde1 eduade2 eduade3 nwspol netusoft\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;   \n 1       835 FR    Male      16 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 2  10006306 DE    Female    39 None of… Abitur… Diplom… Kein b…     25 Every d…\n 3  10008723 DE    Male      16 None of… (Noch)… Kein H… Kein b…     30 Most da…\n 4       231 SE    Female    80 Legally… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       270 Most da…\n 5 100003876 GB    Male      33 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 6       408 FR    Female    30 &lt;NA&gt;     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        30 Every d…\n 7  10009322 DE    Male      72 &lt;NA&gt;     Abitur… Diplom… Kein b…     90 Every d…\n 8 100003453 GB    Male      45 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n 9      1126 SE    Female    60 None of… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;       120 Every d…\n10 100001390 GB    Female    77 Widowed… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;        10 Every d…\n# ℹ 90 more rows\n# ℹ 156 more variables: internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;,\n#   pplhlp &lt;dbl&gt;, polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;,\n#   psppipla &lt;fct&gt;, cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;,\n#   trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;, trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;,\n#   vote &lt;fct&gt;, prtvede1 &lt;fct&gt;, prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;,\n#   wrkorg &lt;fct&gt;, badge &lt;fct&gt;, sgnptit &lt;fct&gt;, pbldmn &lt;fct&gt;, bctprd &lt;fct&gt;, …\n\n\n\n\n3.2.5 Prüfung der Voraussetzungen 1: Grafische Darstellung des Zusammenhangs der beiden Variablen, um die Annahme von Linearität zu prüfen\nACHTUNG! Für die Regressionsanalyse müssen noch weitere Voraussetzungen geprüft werden (insb. Homoskedastizität der Residuen; Unabhängigkeit der Residuen; Normalverteilung der Residuen; keine Ausreißer in den Daten). An dieser Stelle klammern wir die anderen Voraussetzungsprüfungen aber vorerst aus, und kommen in der nächsten Sitzung darauf zurück (das ist sonst zu viel auf einmal).\nZur visuellen Inspektion, ob der Zusammenhang zwischen unseren beiden Variablen linear ist, erstellen wir nun mit dem Befehl geom_point ein Punktdiagramm mit den Variablen Alter und Internetnutzung. Praktischerweise ist die Regressionsformel schon in ggplot integriert: Der Befehl geom_smooth erzeugt eine Trendlinie nach dem linear model (method = lm), die die Beziehung von y (=Internetznutzung) und x (=Alter) abbildet. Das Ergebnis ist eine Linie nach einer linearen Gleichung, die den Daten so eng wie möglich folgt. Mit dem Befehl ggtitle legen wir dann noch in den Klammern den Titel der Grafik fest, und mit xlab und ylab ergänzen wir die Achsenbeschriftung.\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point() + \n  geom_smooth(method = lm, formula = \"y ~ x\") + \n  ggtitle(\"Zusammenhang der Variablen Alter und Internetnutzung\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")\n\n\n\n\n\n\n3.2.6 Interpretation: Was sehen wir im Streudiagramm?\nDie grafische Darstellung legt uns einen schwachen negativen (aber linearen!) Zusammenhang zwischen Alter und Internetnutzung nahe: mit zunehmendem Alter sinkt die Nutzungsdauer. Damit scheint eine wichtige Voraussetzung der Regressionsanalyse, dass der Zusammenhang an sich linear ist, erfüllt.\nNicht wundern: Weil wir oben ein Zufallssample gezogen haben, sieht die Grafik bei Ihnen allen etwas anders aus. Sie kann dadurch auch so ausfallen, dass der lineare Zusammenhang nicht (gut) sichtbar ist – vor allem dann, wenn Ausreißer das Ergebnis massiv verzerren (z.B. wenn ein oder zwei ältere Nutzer mit [unrealistisch?] hoher Nutzungsdauer in ihrer Zufallstichprobe gelandet sind).\n\n\n3.2.7 Durchführung der einfachen linearen Regression über die Funktion lm\nOb diese Beobachtung auch statistisch belastbar ist, wollen wir jetzt mit der einfachen linearen Regressionsanalyse prüfen. Dazu nutzen wir die Funktion lm(). Die Funktion lm steht für “linear model”. In den Klammern benennen wir zunächst die abhängige Variable (hier: Internetznutzung), dann kommt eine Tilde (d.h. “wird definiert durch”) und der Bezug auf unsere unabhängige Variable (hier: Alter). Die Schreibweise y ~ x ist die Formel-Schreibweise in R; in diesem Fall besagt sie, dass y (Internetnutzung) abhängig von x (Alter) ist. Nach dem Komma folgt dann die Benennung des Datensatzes auf den die lm-Funktion angewendet werden soll.\nDer Modelloutput von lm ähnelt dem der schon behandelten Hypothesentests; enthält aber noch weitere Eckdaten wie die Effektstärke, das Signifikanzniveau oder die Erklärungsstärke des Modells.\n\n\n3.2.8 Einfache lineare Regression mit lm (=linear models)\n\nmodel &lt;- lm(internetnutzung ~ alter, data = daten_mod) \nmodel\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nCoefficients:\n(Intercept)        alter  \n    378.194       -3.982  \n\n\nDa dieser Output sehr, sehr sparsam und für uns noch wenig aussagekräftig ist, ergänzen wir ihn mit dem bekannten summary-Befehl, den wir auf unser Modell anwenden. Die dann erscheinende Ausgabe ist das “Herzstück” unserer Regressionsanalyse (insbesondere, wenn sie um die standardisierten B-Koeffizienten erweitert wird - dazu kommen wir aber unten noch). Jetzt nutzen wir erst einmal die summary-Funktion, und wir erhalten im Output einen guten Überblick über unser Regressionsmodell:\n\nsummary(model)\n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Std. Error t value     Pr(&gt;|t|)    \n(Intercept)  378.194     59.747   6.330 0.0000000074 ***\nalter         -3.982      1.198  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.9 Interpretation des Outputs: Was sehen wir in der Ausgabe?\nUnter Call wird zunächst noch einmal das Regresssionsmodell beschrieben, das wir hier berechnet haben. In diesem versuchen wir auf Basis des Datensatzes “daten_mod” die abhängige Variable “internetnutzung” durch die unabhängige Variable “alter” zu erklären.\n\n3.2.9.1 Residuen\nUnter Resdiuen erhalten wir Informationen zur Verteilung der Residuen. Diese geben die Abweichung der beobachteten Werte von den durch das Regressionsmodell erwarteten Werten an.\nCoefficients: #### Intercept Das Intercept definiert den Schnittpunkt der Regressionsgeraden mit der y-Achse (theoretischer Wert für y, wenn x den Wert 0 annimmt).\n\n\n3.2.9.2 Estimate\nDie Estimates sind die unstandardisierte b-Werte. Das sind die Werte, die zur Vorhersage in die Regressionsgleichung eingetragen werden (könnten).\n\n\n3.2.9.3 St. error\nHiermit wird der Standard-Fehler der unstandardisierten b-Werte ausgegeben.\n\n\n3.2.9.4 t-value\nDer t-value gibt den t-Wert des Modells an (Koeffizient / Standardfehler)\n\n\n3.2.9.5 p-value\nDer p-value ist für uns von besonderem Interesse - das ist der Signfikanzwert des Modells (unten mit Signfikanzsschwellen) bzw. des statistischen Zusammenhangs\n\n\n3.2.9.6 R-squared\nAuch beide R-Werte (R2, Adjusted R2) sind von zentraler Bedeutung für die Interpretation: R2 gibt uns die erklärte Gesamtvarianz des Modells der abhängigen Variable an, also die “Erklärungskraft” der unabhängigen Variable Alter auf die abhängige Internetnutzung. Zur Interpretation bietet es sich an, R2 als Prozentwert mit 100 zu multiplizieren. Wir können hier dann daraus lesen, dass das Alter (14,7) Prozent der Varianz der Internetnutzung erklärt. Das ist nicht super viel, aber auch nicht nichts. Wir können daraus aber auch ableiten, dass - neben dem Alter - noch andere Einflussfaktoren die Internetnutzung mitbestimmen müssen. Übrigens: Das R2 könnte theoretisch maximal den Wert 1 annehmen, dann hätten wir eine 100% Erklärung der abhängigen Variable durch die unabhängige Variable (=&gt; das kommt aber in der Realität aber fast nicht vor)\n\n\n3.2.9.7 Adjusted R2\nWie der Name schon sagt bezeichnet Adjusted R2 die Anpassung des Modells, wobei für die Anzahl der aufgenommenen Variablen korrigiert wird (“Strafterm für viele aufgenommene Variablen”). Das Adjusted R2 ist daher immer schlechter als R2.\n\n\n3.2.9.8 F-Statistik\nAuch die F-Statistik ist wichtig: Sie gibt uns nämlich die Signfikanz des Gesamtmodells an (nicht einzelner Variablen wie bei R2!)\n\n\n\n3.2.10 Inhaltliche Interpretation: Was bedeutet das jetzt also alles?\nDer Output zeigt uns: Das Alter hat einen negativen Einfluss auf die Internetnutzung. Je älter ein Nutzer ist, desto weniger nutzt er das Internet. Die Regressionsanalyse lässt dabei auch eine Quantifizierung dieses Zusammenhangs zu: Mit jeder Einheit, in der die unabhängige Variable Alter steigt (hier: mit jedem Jahr Alter), nimmt die unabhängige Variable Internetnutzung um (z.B. -4.642) Messeinheiten (hier: Minuten) ab. Dieser Zusammenhang ist mit p &gt; .05 statistisch signifikant.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse von Regressionsanalysen werden meistens in einer Tabelle dargestellt. Für die Angabe im Text wird folgendes gebraucht:\n✅ den R2-Wert (das Bestimmungskoeffizient)\n✅ den F-Wert (auch als F-Statistik bezeichnet)\n✅ die Freiheitsgrade in Klammern\n✅ den p-Wert\nDas Format ist normalerweise:\n\nBeispiel: Die UV beeinflusst die AV, R2 = .24, F(1, 116) = 4.71, p = .003.\n\n\n\n\n\n3.2.11 Vorhersage von Werten auf Basis des Modells\nDa bei der Regression eine Funktion geschätzt wird, können wir anhand der b-Werte und des Intercepts auch Vorhersagen für bestimmte Fälle treffen. Das ist eine der coolen Superkräfte der Regressionsanalyse :) (Das gilt aber streng genommen nur, wenn das zu Grunde liegende Sample repräsentativ ist und das Modell sowie die Parameter signifikant sind).\nZur Prognose von erwarteten Werten der abhängigen Variable (Internetnutzung) auf Basis von gegebenen Werten der unabhänigen Variable (Alter) kann man die predict.lm()-Funktion nutzen:\n\npredict.lm(model, data.frame(alter = 25))\n\n       1 \n278.6328 \n\npredict.lm(model, data.frame(alter = 75))\n\n       1 \n79.51086 \n\n\n\n\n3.2.12 Inhaltliche Interpretation\nEine Person mit einem Alter von 25 Jahren weist laut Modell eine prognostizierte Internetnutzung von (306) Minuten auf. Eine Person mit einem Alter von 75 Jahren weist laut Modell eine prognostizierte Internetnutzung von (74) Minuten auf. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\nDas geht natürlich auch kombiniert in einem Befehl, dann müssen wir aber mit c() einen combine-Befehl einfügen:\n\npredict.lm(model, data.frame(alter = c(25, 75)))\n\n        1         2 \n278.63275  79.51086 \n\n\n\n\n3.2.13 Vorhersage und Residuen berechnen\nDie Prognose-Leistung unseres Regressionsmodels können wir auch auf den gesamten Datensatz anwenden - dann bekommen wir für jeden Fall im Datensatz den prognostizierten Wert der abhängigen Variable Internetkonsum ausgegeben. Dazu können wir den Befehl fitted nutzen. Der Fall Nummer 3 hat also laut Modell (!) einen Internetkonsum von (X) Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\nfitted(model) \n\n        1         2         3         4         5         6         7         8 \n314.47470 222.87862 314.47470  59.59867 246.77325 258.72057  91.45818 198.98400 \n        9        10        11        12        13        14        15        16 \n139.24743  71.54599 163.14206 195.00156 254.73813 242.79081 306.50982 175.08937 \n       17        18        19        20        21        22        23        24 \n131.28255 131.28255 155.17718 218.89619 111.37036 202.96644 119.33524  99.42305 \n       25        26        27        28        29        30        31        32 \n242.79081 274.65032 103.40549 103.40549 155.17718  99.42305 210.93131 167.12449 \n       33        34        35        36        37        38        39        40 \n310.49226 179.07181 107.38793 131.28255 222.87862  67.56355 198.98400 302.52738 \n       41        42        43        44        45        46        47        48 \n270.66788 278.63275 187.03668  99.42305 234.82594 226.86106 246.77325 262.70300 \n       49        50        51        52        53        54        55        56 \n115.35280  71.54599 163.14206 274.65032 155.17718 163.14206 187.03668 163.14206 \n       57        58        59        60        61        62        63        64 \n234.82594 226.86106  99.42305 250.75569 115.35280 175.08937 183.05425 202.96644 \n       65        66        67        68        69        70        71        72 \n159.15962 123.31768 123.31768 143.22987 155.17718 254.73813 242.79081 278.63275 \n       73        74        75        76        77        78        79        80 \n210.93131 151.19474 270.66788 234.82594 191.01912 119.33524 310.49226 226.86106 \n       81        82        83        84        85        86        87        88 \n159.15962 206.94887 270.66788 250.75569 147.21231 238.80838 230.84350 103.40549 \n       89        90        91        92        93        94        95        96 \n147.21231 159.15962 167.12449 151.19474 302.52738 210.93131 187.03668 119.33524 \n       97        98        99       100 \n175.08937 266.68544 302.52738 198.98400 \n\n\nNun haben wir aber im Rahmen unserer Befragung die Internetnutzung der Befragten aber ja schon erhoben. Wozu dient das dann? Ganz einfach: Über die Ausgabe der prognostizierten Werte und der Residuen können wir sehen, wie hoch die Abweichung der Modellprognose ist, d.h. wie sehr die empirisch beobachteten Werte unserer Fälle im Datensatz von den laut Modell erwarteten Werten abweichen. Um die Abweichung zu quantifizieren, nutzen wir die residuals.lm-Funktion:\n\nresiduals.lm(model)\n\n          1           2           3           4           5           6 \n-194.474695 -162.878625 -269.474695  -29.598673 -126.773252 -228.720565 \n          7           8           9          10          11          12 \n -31.458176  -78.983998 -109.247430  -11.545986  -43.142057  404.998440 \n         13          14          15          16          17          18 \n-194.738127  237.209186  -96.509820  304.910630  -41.282554  -71.282554 \n         19          20          21          22          23          24 \n -35.177181  -38.896187  -51.370365 -142.966435   90.664759  -49.423051 \n         25          26          27          28          29          30 \n-240.790814   25.349683  -13.405489   16.594511 -125.177181  -84.423051 \n         31          32          33          34          35          36 \n -30.931311  132.875505 -130.492257  -89.071808   72.612073  -11.282554 \n         37          38          39          40          41          42 \n -42.878625  112.436451  401.016002   -2.527382  629.332121  921.367245 \n         43          44          45          46          47          48 \n 112.963316  -84.423051 -114.825938 -136.861063  353.226748   37.296997 \n         49          50          51          52          53          54 \n 184.647197  108.454014   16.857943  385.349683 -125.177181 -133.142057 \n         55          56          57          58          59          60 \n-172.036684  -73.142057  365.174062 -166.861063  -39.423051 -205.755690 \n         61          62          63          64          65          66 \n 184.647197 -115.089370  -63.054246  -62.966435  -99.159619  176.682322 \n         67          68          69          70          71          72 \n-108.317678 -118.229868  -35.177181  -74.738127  -32.790814   81.367245 \n         73          74          75          76          77          78 \n-180.931311  -31.194743  449.332121 -114.825938  348.980878  -29.335241 \n         79          80          81          82          83          84 \n-280.492257 -166.861063  -99.159619  -86.948873  -30.667879  -10.755690 \n         85          86          87          88          89          90 \n -27.212306 -118.808376 -200.843500  -73.405489   92.787694 -114.159619 \n         91          92          93          94          95          96 \n  12.875505  328.805257 -212.527382   29.068689 -127.036684  120.664759 \n         97          98          99         100 \n-165.089370  273.314559  -92.527382 -108.983998 \n\n\n\n\n3.2.14 Inhaltliche Interpretation\nFür unseren Fall Nummer 3 beträgt die Abweichung der Prognose von der Beobachtung (121) Minuten. Das diese Abweichung sehr groß ist, wundert uns aber nicht, denn wir wissen ja schon, dass wir eine große Spannweite haben und unser R2 mit 15 Prozent nicht besonders groß ist.\n\n\n3.2.15 Vorhersage und Residuen grafisch darstellen\nUm das grafisch gegenüberzustellen, speichern wir die obenen ausgegeben Daten zur Vorhersage (aus der predict-Funktion) und die Daten zu den Abweichungen von der Vorhersage (residuals-Funktion) jeweils als neue Variablen “vorhersage” und “residuen” ab.\n\ndaten_mod$vorhersage &lt;- predict(model) \ndaten_mod$residuen &lt;- residuals(model) \n\nUnd dann machen wir eine fancy Grafik, die uns die Abweichung der prognostizierten Werte nach oben und nach unten visuell nachvollziehen lässt:\n\nggplot(daten_mod, aes(alter, internetnutzung)) + \n  geom_point(aes(color = residuen)) + # Festlegung der Farbmarkierung für die Residuen (Punkte sind tatsächliche Werte, Linien die Residuen, Farbe gibt Größe der Abweichung an)\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + # Festlegung der Farbe für die Residuen\n  guides(color = \"none\") + # Unterdrückt eine Legende an der Seite (ist obligatorisch)\n  geom_point(aes(y = vorhersage), shape = 1) + # gibt die vorhergesagten Punkte auf der Regressionsgeraden aus\n  geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE, linewidth = 0.5, color = \"black\") + # gibt die Regressionsgerade als Linie aus \n  geom_segment(aes(xend = alter, yend = vorhersage), alpha = .2) + # zeichnet die Linie vom Punkt zur Regressionsgeraden transparent ein \n  ggtitle(\"Vorhergesagte Werte und Residuen für Alter und Internetnutzung\") + # Titel\n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\") # Achsen-Beschriftung\n\n\n\n\n\n\n3.2.16 Standardisierung der B-Koeffizienten (“beta-Koeffizienten”)\nNeben den normalen Regressionskoeffizienten b kann man auch die standardisierten Koeffizienten beta berechnen. Die standardisierten beta-Koeffizienten sind nützlich, weil sie die Skalierung der einzelnen Messwerte “herausrechnet”, wodurch unterschiedlich skalierte Variablen vergleichbar werden.\nUm uns die standardisierten beta-Koeffizienten ausgeben zu lassen, können wir auf das Paket lm.beta mit der gleichnamigen Funktion zurückgreifen, die auf ein mit lm() erzeugtes Modell angewendet werden kann. Der nun folgende Befehl ist im Prinzip wie oben, nur in den Klammern ergänzt um die Funktion lm.beta(), die dafür sorgt, dass wir im Output unten eine zusätzliche Spalte erhalten, in der die standardisierten B-Koeffizienten angezeigt werden. Diese sind als standardisierte “Regressionsgewichte” zu interpretieren - je höher der Wert, desto stärker der erklärende Beitrag der Variable.\nDas ist natürlich vor allem dann spannend, wenn ich den Einfluss mehrerer Variablen vergleichen will. Dabei haben die standardisierten beta-Werte einen wichtigen Vorteil: Sollte man mehrere Prädiktoren in einem Modell haben, die aber auf unterschiedlichen Skalen gemessen wurden (z.B. 1x 5er und 1x 7er Skala), kann man ihren relativen Erklärungsbeitrag untereinander vergleichen.\n\nsummary(lm.beta(model)) \n\n\nCall:\nlm(formula = internetnutzung ~ alter, data = daten_mod)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-280.49 -115.87  -43.01   46.13  921.37 \n\nCoefficients:\n            Estimate Standardized Std. Error t value     Pr(&gt;|t|)    \n(Intercept) 378.1937           NA    59.7466   6.330 0.0000000074 ***\nalter        -3.9824      -0.3183     1.1979  -3.324      0.00125 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 198.2 on 98 degrees of freedom\nMultiple R-squared:  0.1013,    Adjusted R-squared:  0.09218 \nF-statistic: 11.05 on 1 and 98 DF,  p-value: 0.001247\n\n\n\n\n3.2.17 Zusatzfunktionen zur schöneren Ergebnisdarstellung durch das Paket broom\n\ntidy(lm.beta(model))\n\n# A tibble: 2 × 6\n  term        estimate std_estimate std.error statistic       p.value\n  &lt;chr&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)   378.         NA         59.7       6.33 0.00000000740\n2 alter          -3.98       -0.318      1.20     -3.32 0.00125      \n\n\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.101        0.0922  198.      11.1 0.00125     1  -670. 1346. 1353.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\naugment(model)\n\n# A tibble: 100 × 8\n   internetnutzung alter .fitted .resid   .hat .sigma   .cooksd .std.resid\n             &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1             120    16   314.  -194.  0.0452   198. 0.0239       -1.00  \n 2              60    39   223.  -163.  0.0124   199. 0.00428      -0.827 \n 3              45    16   314.  -269.  0.0452   197. 0.0458       -1.39  \n 4              30    80    59.6  -29.6 0.0496   199. 0.000613     -0.153 \n 5             120    33   247.  -127.  0.0172   199. 0.00364      -0.645 \n 6              30    30   259.  -229.  0.0206   198. 0.0143       -1.17  \n 7              60    72    91.5  -31.5 0.0327   199. 0.000441     -0.161 \n 8             120    45   199.   -79.0 0.0102   199. 0.000823     -0.400 \n 9              30    60   139.  -109.  0.0161   199. 0.00253      -0.556 \n10              60    77    71.5  -11.5 0.0428   199. 0.0000792    -0.0595\n# ℹ 90 more rows\n\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link"
  },
  {
    "objectID": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "href": "Skript_7.2.html#grafische-darstellung-und-interpretation-einer-regressionsgeraden",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden",
    "text": "3.3 Grafische Darstellung und Interpretation einer Regressionsgeraden"
  },
  {
    "objectID": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "href": "Skript_7.2.html#prüfung-der-voraussetzungen",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.4 Prüfung der Voraussetzungen",
    "text": "3.4 Prüfung der Voraussetzungen"
  },
  {
    "objectID": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "href": "Skript_7.2.html#berechnung-und-interpretation-einer-einfachen-linearen-regression",
    "title": "Zusammenhänge bei zwei Variablen",
    "section": "3.5 Berechnung und Interpretation einer einfachen linearen Regression",
    "text": "3.5 Berechnung und Interpretation einer einfachen linearen Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n(Platzhalter generiert durch ChatGDP)\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "href": "Skript_6.3.html#t-test-für-unabhängige-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "2.1 t-Test für unabhängige Stichproben",
    "text": "2.1 t-Test für unabhängige Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#mann-whitney",
    "href": "Skript_6.3.html#mann-whitney",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.2 Mann-Whitney",
    "text": "2.2 Mann-Whitney"
  },
  {
    "objectID": "Skript_6.3.html#die-varianzanalyse",
    "href": "Skript_6.3.html#die-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.3 Die Varianzanalyse",
    "text": "2.3 Die Varianzanalyse\nDie Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n2.3.1 Datenmanagement\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())\n\n\n\n2.3.2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n\n2.3.3 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden.\n\n\n2.3.4 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen.\n\n\n2.3.5 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests.\n\n\n2.3.6 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.3.html#kruskal-wallis",
    "href": "Skript_6.3.html#kruskal-wallis",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.4 Kruskal Wallis",
    "text": "2.4 Kruskal Wallis"
  },
  {
    "objectID": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.3.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "2.5 Mehrfaktorielle Varianzanalyse",
    "text": "2.5 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n2.5.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n2.5.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "href": "Skript_6.3.html#t-test-für-verbundene-stichproben",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.1 t-Test für verbundene Stichproben",
    "text": "3.1 t-Test für verbundene Stichproben"
  },
  {
    "objectID": "Skript_6.3.html#wilcoxon",
    "href": "Skript_6.3.html#wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mit t-Test und Varianzanalyse",
    "section": "3.2 Wilcoxon",
    "text": "3.2 Wilcoxon"
  },
  {
    "objectID": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "href": "Skript_7.3.html#multiple-regression-mit-dummy-codierung",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.2 Multiple Regression mit Dummy-Codierung",
    "text": "1.2 Multiple Regression mit Dummy-Codierung\n\n1.2.1 Vorbereitung der Daten zum Zusammenhang von Alter, Bildung, Geschlecht und Internetnutzung\nNun wollen wir noch Geschlecht (gndr) als unabhängige Variable mit in unser Regressionsmodell aufnehmen. Bei Gender haben wir aber das Problem, dass diese Variable nicht metrisch skaliert ist! Es handelt sich vielmehr um eine kategoriale Variable. Wie Sie schon gelernt haben, können Sie diese mit einem “Trick” ebenfalls in die Regressionsanalyse einbringen - Sie müssen diese dann aber durch Dummy-Coding passend machen. Wir wollen uns hier mal anschauen, wie das funktioniert. Dazu müssen wir die Variable mittels mutate-Befehl erst einmal umcodieren.\nDurch die Dummy-Codierung wird die kategoriale Variable in zwei Gruppen übersetzt, von denen die eine mit 1 und die andere mit 0 codiert wird. Die Gruppe, der der Wert 0 zugeordnet wird, ist dann die Referenzkategorie. In unserem Beispiel Beispiel machen wir “männlich” zur Referenzkategorie (und codieren es mit 0 um). Der Regressionskoeffizient b gibt dann genau die Menge an, um die sich die Internetnutzung ändert, wenn sich das Geschlecht gegenüber der Referenzkategorie verändert.\nPS: Die Variable als numerischen Wert zu behandeln, ist in unserem Fall etwas kompliziert, weil diese eine Faktorvariable war, die wir zuerst in eine Charaktervariable umwandeln mussten.\n\n\n1.2.2 Dummy Codierung der Variable Gender\n\ndaten_mod2 &lt;- daten_mod %&gt;%\nmutate(gender_r  = recode(gender, 'Male'='0', 'Female'='1')) %&gt;% # Recodierung der Var Gender zur Dummy-Variable\n  mutate(gender_r = as.numeric(as.character(gender_r))) # Variable als numerischen Wert behandeln\ndaten_mod2\n\n# A tibble: 100 × 167\n        idno cntry gender alter marsts                   edubde1 eduade2 eduade3\n       &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;                    &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;  \n 1 100001074 GB    Female    67 &lt;NA&gt;                     &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2      1044 SE    Male      62 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 3  10007670 DE    Male      58 &lt;NA&gt;                     Fachho… Diplom… Laufba…\n 4 100002656 GB    Female    67 Legally married          &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 5      1333 SE    Male      56 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6  10002266 DE    Male      21 None of these (NEVER ma… Abitur… Kein H… Kein b…\n 7 100000680 GB    Female    31 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8  10009262 DE    Female    47 None of these (NEVER ma… Mittle… Kein H… Abgesc…\n 9 100002806 GB    Male      57 None of these (NEVER ma… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n10  10002009 DE    Female    76 &lt;NA&gt;                     Mittle… Kein H… Laufba…\n# ℹ 90 more rows\n# ℹ 159 more variables: politische_Nachrichtenrezeption &lt;dbl&gt;, netusoft &lt;fct&gt;,\n#   internetnutzung &lt;dbl&gt;, ppltrst &lt;dbl&gt;, pplfair &lt;dbl&gt;, pplhlp &lt;dbl&gt;,\n#   polintr &lt;fct&gt;, psppsgva &lt;fct&gt;, actrolga &lt;fct&gt;, psppipla &lt;fct&gt;,\n#   cptppola &lt;fct&gt;, trstprl &lt;dbl&gt;, trstlgl &lt;dbl&gt;, trstplc &lt;dbl&gt;, trstplt &lt;dbl&gt;,\n#   trstprt &lt;dbl&gt;, trstep &lt;dbl&gt;, trstun &lt;dbl&gt;, vote &lt;fct&gt;, prtvede1 &lt;fct&gt;,\n#   prtvede2 &lt;fct&gt;, contplt &lt;fct&gt;, wrkprty &lt;fct&gt;, wrkorg &lt;fct&gt;, badge &lt;fct&gt;, …\n\n\n\ntable(daten_mod2$gender_r)\n\n\n 0  1 \n56 44 \n\nsummary(daten_mod2$gender_r)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    0.44    1.00    1.00 \n\nclass(daten_mod2$gender_r)\n\n[1] \"numeric\"\n\n\n\n\n1.2.3 Regressionsmodell zum Zusammenhang von Alter, Nachrichtenrezeptiion, Geschlecht und Internetnutzung spezifizieren und anzeigen lassen\nIn die bereits bekannte Regressionsfunktion lm() fügen wir im hinteren Teil (d.h. hinter der Tilde) nun einfach die weitere unabhängige Variable gender_r ein, indem wir sie mit einem + Zeichen anhängen.\n\nmodel_m2 &lt;- lm(internetnutzung ~ alter + politische_Nachrichtenrezeption + gender_r, data = daten_mod2)\nsummary(lm.beta(model_m2))\n\n\nCall:\nlm(formula = internetnutzung ~ alter + politische_Nachrichtenrezeption + \n    gender_r, data = daten_mod2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-202.12  -94.50  -51.31   32.24  471.47 \n\nCoefficients:\n                                 Estimate Standardized Std. Error t value\n(Intercept)                     265.27637           NA   45.47227   5.834\nalter                            -1.93149     -0.20696    0.92960  -2.078\npolitische_Nachrichtenrezeption   0.26651      0.20918    0.12545   2.124\ngender_r                        -18.36732     -0.06002   30.78140  -0.597\n                                   Pr(&gt;|t|)    \n(Intercept)                     0.000000073 ***\nalter                                0.0404 *  \npolitische_Nachrichtenrezeption      0.0362 *  \ngender_r                             0.5521    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 147.5 on 96 degrees of freedom\nMultiple R-squared:  0.09507,   Adjusted R-squared:  0.06679 \nF-statistic: 3.362 on 3 and 96 DF,  p-value: 0.02189\n\n\n\n\n1.2.4 Inhaltliche Interpretation\nGender hat hier keinen signifikanten Einfluss auf den Internetkonsum. Wenn es einen hätte, wäre der Einfluss geringer als bei Alter und Ausbildungsjahren (ersichtlich an der Größe des standardisierten beta-Koeffizienten). Wichtig: Gender ist als eine Dummy- Variable in 0=männlich und 1=weiblich codiert, deshalb ist der Estimate hier etwas schwieriger zu lesen. Die Frauen sind als 1 codiert und stellen hier die Vergleichsgruppe zur Referenzgruppe der Männer (=0) dar.\nFrauen haben eine um (9,8) Minuten geringeren Internetkonsum als Männer (wobei dieser Befund statistisch ja (nicht) signifikant ist). (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.5 Vorhersage des multivariaten Modells für die tägliche Internetnutzung durch Alter, Nachrichtenrezeption und Geschlecht\n\npredict.lm(model_m2, data.frame(alter = c(25, 75), gender_r = c(0,1), politische_Nachrichtenrezeption = c(5, 10)))\n\n       1        2 \n218.3217 104.7124 \n\n\n\n\n1.2.6 Inhaltliche Interpretation\nEine männliche Person, die 25 Jahre alt ist und 5 Minuten pro Tag politische Nachrichten rezipiert hat, einen prognostizierten Internetkonsum von 293 Minuten. Eine weibliche Person, die 75 Jahre alt ist und ebenfalls 5 Minuten pro Tag politische Nachrichten rezipiert, hat einen prognostizierten Internetkonsum von 41 Minuten. (Nicht wundern, wenn Ihr Ergebnis leicht anders ausfällt: Der Wert variiert entsprechend der gezogenen Zufallsstichprobe.)\n\n\n1.2.7 Zusätzliche Voraussetzungsprüfung bei der multiplen linearen Regression: Liegt Multikollinearität vor?\nDie multiple lineare Regression erfordert alle Voraussetzungen, die für die einfache Regression auch verlangt sind - wie Sie diese ausführen, haben Sie ja heute zu Anfang der Sitzung gelernt (siehe oben). Zusätzlich müssen Sie bei einer multiplen Regresssion noch prüfen, ob Multikollinearität vorliegt. Multikollinearität bedeutet, dass mindestens einer unserer Prädiktoren durch einen oder mehrere der anderen Prädiktoren vorhergesagt werden kann. Die Prädiktoren wären in diesem Fall nicht unabängig voneinander, sondern würden hoch miteinander korrelieren und hätten damit sozusagen keine selbstständige Erklärungskraft im Modell.\nOb Multikollinearität vorliegt, können wir durch den VIF-Wert (variance inflation factor) ermitteln. Dieser darf nicht über 10 liegen, idealerweise auch nicht über 5. Um dies zu prüfen, nutzen wir den check_collinearity-Befehl aus dem Performance package:\n\ncheck_collinearity(model_m2)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                            Term  VIF    VIF 95% CI Increased SE Tolerance\n                           alter 1.05 [1.00,  3.50]         1.03      0.95\n politische_Nachrichtenrezeption 1.03 [1.00, 26.71]         1.01      0.97\n                        gender_r 1.07 [1.00,  2.29]         1.04      0.93\n Tolerance 95% CI\n     [0.29, 1.00]\n     [0.04, 1.00]\n     [0.44, 1.00]\n\n\n\n\n1.2.8 Inhaltliche Interpretation\nDie VIF-Werte liegen zwischen 0 und 5; wir können daher davon ausgehen, dass keine Multikollinearität vorliegt (grün = “Low Correlation”)."
  },
  {
    "objectID": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "href": "Skript_7.3.html#add-on-weitere-plots-zur-vergleichenden-regressionsanalyse",
    "title": "Zusammenhänge bei mehr als zwei Variablen mit der multiplen Regression",
    "section": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse",
    "text": "1.3 Add-On: Weitere Plots zur (vergleichenden) Regressionsanalyse\n\ndaten_mod3 &lt;- daten %&gt;% \n  select(agea, netustm, cntry) %&gt;% \n  rename(alter = agea,\n         internetnutzung = netustm,\n         land = cntry) %&gt;% \n  filter(land %in% c(\"DE\", \"FR\", \"IS\", \"PL\")) %&gt;% \n  drop_na() %&gt;% \n  group_by(land) %&gt;% \n  slice_sample(n = 100) %&gt;% \n  ungroup()\ndaten_mod3\n\n# A tibble: 200 × 3\n   alter internetnutzung land \n   &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;\n 1    59             240 DE   \n 2    35              60 DE   \n 3    38             240 DE   \n 4    20             240 DE   \n 5    51              60 DE   \n 6    64             150 DE   \n 7    60              30 DE   \n 8    28             540 DE   \n 9    21             300 DE   \n10    53              45 DE   \n# ℹ 190 more rows\n\n\n\nggplot(daten_mod3, aes(alter, internetnutzung, colour = land)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(method = lm, formula = \"y ~ x\", se = FALSE) + \n  scale_colour_brewer(palette = \"Set1\") + \n  ggtitle(\"Lineare Regression für Alter und Internetnutzung (vier Länder)\") + \n  xlab(\"Alter\") + ylab(\"tägliche Internetnutzung (Minuten)\")"
  },
  {
    "objectID": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "href": "Skript_7.2.html#literatur-und-beispiele-aus-der-praxis",
    "title": "Zusammenhänge bei zwei Variablen mittels Korrelation und Regression",
    "section": "3.3 Literatur und Beispiele aus der Praxis",
    "text": "3.3 Literatur und Beispiele aus der Praxis\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Weeks, B. E., & Holbert, R. L. (2013). Predicting dissemination of news content in social media: A focus on reception, friending, and partisanship. Journalism & Mass Communication Quarterly, 90(2), 212-232. Link\n\n:::"
  },
  {
    "objectID": "Home.html",
    "href": "Home.html",
    "title": "Willkommen zur Einführung in quantitative Forschungsdesigns und Datenanalyse",
    "section": "",
    "text": "Bild generiert von Midjourney\n\n\nWillkommen auf der Startseite unserer Webseite, die Ihnen einen umfassenden Einführungskurs in quantitative Forschungsdesigns mit R bietet. Dieser Kurs ist Teil der Lehrveranstaltung des Bachelors in Kommunikations- und Medienwissenschaften an der Universität Bremen und wird durch das renommierte Zentrum für Medien, Kommunikations- und Informationsforschung ausgerichtet. Wir freuen uns, Ihnen dieses Wissen und diese Fähigkeiten im Rahmen des SKILL-Projekts der Universität präsentieren zu können.\nIn diesem Kurs werden wir Sie schrittweise durch die Grundlagen der quantitativen Forschungsdesigns führen und Ihnen zeigen, wie Sie R, eine leistungsstarke Programmiersprache und Umgebung für statistische Berechnungen und Datenanalyse, effektiv einsetzen können. Egal, ob Sie ein angehender Forscher, ein Kommunikations- oder Medienwissenschaftler oder einfach nur daran interessiert sind, quantitative Forschungsmethoden zu erlernen, dieser Kurs bietet Ihnen das nötige Wissen, um Ihre analytischen Fähigkeiten zu erweitern.\nDank der großzügigen Förderung durch das SKILL-Projekt der Universität können wir Ihnen diesen Kurs kostenlos zur Verfügung stellen. Sie haben Zugang zu umfangreichen Lernmaterialien, interaktiven Übungen und praktischen Beispielen, die Ihnen helfen werden, quantitative Forschungsdesigns zu verstehen und diese mit Hilfe von R umzusetzen. Beginnen Sie noch heute und entdecken Sie die faszinierende Welt der quantitativen Forschungsdesigns mit R. Wir freuen uns darauf, Sie auf Ihrer Lernreise zu begleiten!\nIn den kommenden Abschnitten werden wir Ihnen die Grundlagen der quantitativen Forschung vermitteln und Ihnen dabei helfen, ein solides Verständnis für verschiedene statistische Konzepte und Methoden zu entwickeln. Egal, ob Sie in den Bereichen Wissenschaft, Wirtschaft oder Gesundheitswesen tätig sind, das Erlernen von R und statistischer Datenanalyse wird Ihnen helfen, Ihre Daten effektiv zu analysieren, aussagekräftige Erkenntnisse zu gewinnen und fundierte Entscheidungen zu treffen.\nUnser Ziel ist es, Ihnen das nötige Wissen und die praktischen Fähigkeiten zu vermitteln, um quantitative Forschung durchzuführen und Daten mit R zu analysieren. Wir werden Ihnen Schritt für Schritt erklären, wie Sie Daten in R importieren, explorieren, bereinigen und visualisieren können. Darüber hinaus werden wir auf verschiedene statistische Techniken eingehen, darunter Hypothesentests, lineare Regression, multivariate Analyse und vieles mehr. Beginnen Sie noch heute und entdecken Sie die aufregende Welt der quantitativen Forschung und Datenanalyse mit R!\n(Platzhalter generiert durch ChatGDP)\n\nLicense\nCreative Commons License This online work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International. Visit here for more information about the license.\n\n\nAcknowledements\nVielen Dank an die #rstats-Community!\nThis website is built with Quarto, the lovely icons by icons8 & Emojipedia, and none of this would be possible without the tidyverse."
  },
  {
    "objectID": "Autoren.html",
    "href": "Autoren.html",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Wir möchten uns einmal kurz vorstellen…"
  },
  {
    "objectID": "Autoren.html#patrick-zerrer",
    "href": "Autoren.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "Patrick Zerrer",
    "text": "Patrick Zerrer\nVita\n\n\n\n\n\nPatrick Zerrer\n\n\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Er schloss den „B.A. Governance and Public Policy - Staatswissenschaften” an der Universität Passau sowie den „M.A. Öffentliche Kommunikation” an der Friedrich-Schiller-Universität ab. Seine Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation, der Nachrichten- und Mediennutzung sowie der politischen Partizipation mit einem Fokus auf (digitalen) Klimaprotest und -bewegungen. Für seine Forschung nutzer er unterschiedliche Methoden, u.a. digitales (mobiles) Tracking, digitale Spurdaten, Umfragen, (automatisierter) Inhaltsanalysen und Interviews.\nForschungsschwerpunkte\n\nPolitische Kommunikation\n(mobile) Mediennutzungsforschung\nKlimaprotestforschung\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Hintergrund.html",
    "href": "Hintergrund.html",
    "title": "Lernen durch Praxis - Der Weg zu fundierten statistischen Kenntnissen",
    "section": "",
    "text": "Picture generated by Midjourney\n\n\nUnser Leitbild ist es, Ihnen einen Kurs anzubieten, der auf der Verschränkung von praktischem Lernen, grundlegenden Kenntnissen und eigenständigem Forschen basiert. Unser Ziel ist es, Ihnen die Werkzeuge und das Verständnis zu vermitteln, um quantitative Forschungsdesigns mit R erfolgreich umzusetzen.\nWir beginnen mit einer soliden Grundlage, in der wir Ihnen die wesentlichen Konzepte und Techniken der quantitativen Forschung vermitteln. Wir legen Wert darauf, dass Sie die grundlegenden Prinzipien verstehen, bevor wir Sie in die Praxis entlassen. Sie werden lernen, wie Sie Daten in R importieren, explorieren und visualisieren können, um ein tieferes Verständnis für Ihre Forschungsfragen zu gewinnen.\nNachdem Sie diese Grundlagen erworben haben, gehen wir einen Schritt weiter und bieten Ihnen die Möglichkeit, Ihr eigenes Forschungsprojekt durchzuführen. Unter Anleitung unserer erfahrenen Tutoren werden Sie ein eigenständiges Projekt entwickeln, bei dem Sie Ihre neu erlernten statistischen Kenntnisse anwenden können. Sie werden Schritt für Schritt lernen, wie Sie Hypothesen aufstellen, Daten sammeln, analysieren und interpretieren. Durch dieses praktische Erleben vertiefen Sie nicht nur Ihr Verständnis, sondern gewinnen auch wertvolle Erfahrungen im Bereich der quantitativen Forschung.\nUnser Kurs legt großen Wert darauf, dass Sie nicht nur theoretisches Wissen erlangen, sondern dieses Wissen in die Praxis umsetzen können. Wir sind der festen Überzeugung, dass das eigenständige Durchführen eines Forschungsprojekts Ihnen nicht nur ein tieferes Verständnis für statistische Methoden gibt, sondern auch Ihre analytischen und Problemlösungsfähigkeiten stärkt. Unsere Tutoren stehen Ihnen dabei zur Seite und bieten Ihnen individuelle Unterstützung, um sicherzustellen, dass Sie Ihr volles Potenzial entfalten können.\nWillkommen zu einer spannenden Lernreise, bei der Sie nicht nur statistische Kenntnisse erwerben, sondern auch die Fähigkeit entwickeln, Ihr Wissen auf praktische Weise anzuwenden. Wir sind davon überzeugt, dass Sie durch dieses ganzheitliche Lernkonzept Ihre Ziele erreichen und in der Welt der quantitativen Forschung erfolgreich sein werden.\n(Platzhalter generiert durch ChatGDP)"
  },
  {
    "objectID": "Hintergrund.html#patrick-zerrer",
    "href": "Hintergrund.html#patrick-zerrer",
    "title": "Das Team stellt sich vor",
    "section": "",
    "text": "Patrick Zerrer\n\n\nVita\nPatrick Zerrer ist wissenschaftlicher Mitarbeiter am Zentrum für Medien-, Kommunikations- und Informationsforschung an der Universität Bremen. Von September 2013 bis 2017 studierte er an der Universität Passau „B.A. Governance and Public Policy – Staatswissenschaften\". Das Masterstudium „Öffentliche Kommunikation\" an der Friedrich-Schiller-Universität schloss er von Oktober 2017 bis März 2020 ab. Seine Masterarbeit legte er in Form einer Gruppenarbeit zum Thema „Eine methodische Triangulation zur Identifizierung der Motivationen der öffentlichen Meinungsäußerung nach dem Social Identity Model of Collective Action\" mit einer Teilstudie von Online-Kommentaren auf deutschen Nachrichtenwebsites zum Thema der „Klimapolitik\" mittels einer quantitativen Inhaltsanalyse ab. Die Forschungsinteressen liegen insbesondere im Bereich der politischen Online-Kommunikation.\nForschungsschwerpunkte\nPolitische Kommunikation, Online-Kommunikation, Digitale Mediennutzungsforschung\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_5.2.html#laden-der-nötigen-pakete",
    "href": "Skript_5.2.html#laden-der-nötigen-pakete",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "library(psych)\n\nWarning: Paket 'psych' wurde unter R Version 4.3.1 erstellt\n\nlibrary(psy)\n\n\nAttache Paket: 'psy'\n\n\nDas folgende Objekt ist maskiert 'package:psych':\n\n    wkappa\n\nlibrary(nFactors)\n\nWarning: Paket 'nFactors' wurde unter R Version 4.3.1 erstellt\n\n\nLade nötiges Paket: lattice\n\n\n\nAttache Paket: 'nFactors'\n\n\nDas folgende Objekt ist maskiert 'package:lattice':\n\n    parallel\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(haven)"
  },
  {
    "objectID": "Skript_5.2.html#laden-des-datensatzes",
    "href": "Skript_5.2.html#laden-des-datensatzes",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "daten &lt;- haven::read_dta(\"Datensatz/ESS8.dta\")"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Wir nehmen an, dass wir einen Index für die Erfassung politischer Kompetenz- und Einflusserwartungen (politische Selbstwirksamkeit) erstellen möchten.\nMessinstrument mit den Items: https://zis.gesis.org/skala/Beierlein-Kemper-Kovaleva-Rammstedt-Political-Efficacy-Kurzskala-(PEKS)# Ausführliche Erläuterung: https://www.gesis.org/fileadmin/kurzskalen/working_papers/PEKS_Workingpaper.pdf\nHierfür haben wir aufgrundlage von Theorie die folgenden Indikatoren ausgewählt:\npsppsgva - Political system allows people to have a say in what government does actrolga - Able to take active role in political group psppipla - Political system allows people to have influence on politics cptppola - Confident in own ability to participate in politics\n\ness_wirksamkeit &lt;- daten %&gt;%\n  select(psppsgva,\n         actrolga,\n         psppipla,\n         cptppola) %&gt;% \n  mutate(psppsgva = as.numeric(psppsgva),\n         actrolga = as.numeric(actrolga),\n         psppipla = as.numeric(psppipla),\n         cptppola = as.numeric(cptppola)) %&gt;% \n  na.omit()\nhead(ess_wirksamkeit)\n\n# A tibble: 6 × 4\n  psppsgva actrolga psppipla cptppola\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        2        2        2        3\n2        1        3        2        3\n3        2        2        2        2\n4        3        2        3        4\n5        3        3        4        1\n6        2        2        2        2"
  },
  {
    "objectID": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "href": "Skript_5.2.html#deskriptive-statistik-für-den-teildatensatz",
    "title": "Die Faktorenanalyse",
    "section": "1.2 Deskriptive Statistik für den Teildatensatz",
    "text": "1.2 Deskriptive Statistik für den Teildatensatz\nWir werfen einen kurzen Blick in die deskriptive Statistik für unseren Teildatensatz, um ein besseres Verständnis für die Daten zu erhalten.\n\n1summary(allbus_vertrauen)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n Ver_Gesundheitswesen   Ver_BVerfG    Ver_Bundestag   Ver_Verwaltung \n Min.   :1.000        Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:4.000        1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000  \n Median :5.000        Median :6.000   Median :4.000   Median :5.000  \n Mean   :4.939        Mean   :5.255   Mean   :4.058   Mean   :4.482  \n 3rd Qu.:6.000        3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000        Max.   :7.000   Max.   :7.000   Max.   :7.000  \n Ver_kath_Kirche Ver_evan_Kirche   Ver_Justiz        Ver_TV     \n Min.   :1.000   Min.   :1.00    Min.   :1.000   Min.   :1.000  \n 1st Qu.:1.000   1st Qu.:1.00    1st Qu.:4.000   1st Qu.:3.000  \n Median :2.000   Median :3.00    Median :5.000   Median :4.000  \n Mean   :2.331   Mean   :3.04    Mean   :4.581   Mean   :3.577  \n 3rd Qu.:3.000   3rd Qu.:4.00    3rd Qu.:6.000   3rd Qu.:4.000  \n Max.   :7.000   Max.   :7.00    Max.   :7.000   Max.   :7.000  \n  Ver_Zeitung       Ver_Uni      Ver_Regierung    Ver_Polizei     Ver_Parteien \n Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.000   1st Qu.:4.000   1st Qu.:2.00  \n Median :4.000   Median :5.000   Median :4.000   Median :5.000   Median :3.00  \n Mean   :4.012   Mean   :5.203   Mean   :4.054   Mean   :4.948   Mean   :3.19  \n 3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:5.000   3rd Qu.:6.000   3rd Qu.:4.00  \n Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.000   Max.   :7.00  \n   Ver_Kom_EU      Ver_EU_Par   \n Min.   :1.000   Min.   :1.000  \n 1st Qu.:2.000   1st Qu.:2.000  \n Median :4.000   Median :4.000  \n Mean   :3.515   Mean   :3.556  \n 3rd Qu.:5.000   3rd Qu.:5.000  \n Max.   :7.000   Max.   :7.000"
  },
  {
    "objectID": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "href": "Skript_5.2.html#andere-methoden-der-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Hauptachsen-Analyse fa() Funktion. Anwendung wie oben beschrieben.\nHauptkomponentenanalyse principal() Funktion. Eigentlich keine Faktorenanalyse, beide Methoden sind sich aber sehr ähnlich. Anwendung wie oben beschrieben."
  },
  {
    "objectID": "Skript_5.2.html#quellen-für-das-script",
    "href": "Skript_5.2.html#quellen-für-das-script",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Stephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.3.html#laden-der-nötigen-pakete",
    "href": "Skript_5.3.html#laden-der-nötigen-pakete",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Laden der nötigen Pakete",
    "text": "1.1 Laden der nötigen Pakete\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\n\nWarning: Paket 'pacman' wurde unter R Version 4.3.1 erstellt\n\np_load(tidyverse, haven, psych, dplyr, htmlTable)\n\nUnd laden im Anschluss den notwendigen Datensatz."
  },
  {
    "objectID": "Skript_5.3.html#laden-des-datensatzes",
    "href": "Skript_5.3.html#laden-des-datensatzes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Laden des Datensatzes",
    "text": "1.2 Laden des Datensatzes\n\ndaten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")"
  },
  {
    "objectID": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "href": "Skript_5.3.html#teildatensatz-mit-den-benötigten-index-variablen",
    "title": "Reliabilität von Skalen",
    "section": "1.1 Teildatensatz mit den benötigten Index-Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Index-Variablen\nWir greifen natürlich auf die gleiche Datengrundlage zurück, welche wir auch für die Faktorenanalyse verwendet haben. Was in unserem Fall bedeutet, dass wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nWir bereiten die Daten entsprechend vor, indem wir die fehlenden Werte entfernen und die Variablen in numerische umwandeln.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nWir haben nun alle Daten geladen und die Variablen entsprechend vorbereitet. Wir können eigentlich mit der Indexbidlung beginnen, müssen uns allerdings davor noch entscheiden, welche Art von Index wir bilden möchten."
  },
  {
    "objectID": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "href": "Skript_5.3.html#berechnen-eines-ungewichteten-summenindex",
    "title": "Reliabilität von Skalen",
    "section": "1.3 Berechnen eines Ungewichteten Summenindex",
    "text": "1.3 Berechnen eines Ungewichteten Summenindex\nWir haben bereits in Kapitel 5.2 mittels der explorativen Faktorenanalyse statistisch getestet, ob wir einen Index aus den genannten Variablen bilden können. Dies ist der Fall. Wir berechnen nur die einfachste Form eines Index, den ungewichteten Summenindex. Das bedeutet, dass wir die Werte pro befragter Person für die genannten Variablen aufsummieren und KEINE Gewichtungen einbauen. Eine Gewichtung wäre bspw. wenn wir eine Variable doppelt zählen würden.\nWir erstellen eine neue Variable vertrauen_ges_inst und summieren die Werte aller Indikatoren pro Fall (befragte Person) auf, bevor wir diese durch die Anzahl der Indikatoren teilen. Auf diese Art und Weise erhalten wir die selben Werteausprägungen, wie in den Indikatoren was uns die Interpretation erleichtert.\n\nindex_vertrauen = allbus_vertrauen %&gt;% \n1  mutate(vertrauen_ges_inst = (Ver_Gesundheitswesen + Ver_BVerfG + Ver_Bundestag + Ver_Verwaltung + Ver_kath_Kirche + Ver_evan_Kirche + Ver_Justiz+ Ver_TV +  Ver_Zeitung + Ver_Uni + Ver_Regierung + Ver_Polizei + Ver_Parteien + Ver_Kom_EU + Ver_EU_Par) / 15)\n2htmlTable(head(index_vertrauen))\n\n\n1\n\nWir bilden mit Hilfe von mutate die neue Variable vertrauen_ges_inst, welche sich aus der Summe der Indikatoren geteilt durch die Anzahl der Indikatoren zusammensetzt.\n\n2\n\nDie htmlTable Funktion ermöglicht uns eine schönere Darstellung der Tabelle. Mit head wählen wir die ersten paar Fälle aus dem Datensatz index_vertrauen aus\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\nvertrauen_ges_inst\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n4.6\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n4.8\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n4.2\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n4.66666666666667\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n4\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n2.8\n\n\n\n\n\nWir können uns noch die deskriptive Statistik für den Index anschauen, diese ist wichtig um den berechneten Index korrekt zu interpretieren.\n\n1summary(index_vertrauen$vertrauen_ges_inst)\n\n\n1\n\nMit dem summary Befehl können wir uns die deskritpive Statistik ausgeben lassen\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.400   4.133   4.049   4.733   7.000"
  },
  {
    "objectID": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "href": "Skript_5.3.html#reliabilität-des-indizes-berechnen",
    "title": "Reliabilität von Skalen",
    "section": "1.4 Reliabilität des Indizes berechnen",
    "text": "1.4 Reliabilität des Indizes berechnen\nBevor wir diesen Index einsetzen können, müssen wir zunächst noch checken, ob die Variablen auch inhaltlich zusammenpassen. Dazu ermitteln wir Cronbach’s Alpha als Maß der Skalenreliabilität:\n\nindex_vertrauen %&gt;%\n1  select(Ver_Gesundheitswesen:Ver_EU_Par) %&gt;%\n2  psych::alpha(check.keys=TRUE)\n\n\n1\n\nWir wählen mit select alle Variablen zwischen Ver_Gesundheitswesen und Ver_EU_Par aus\n\n2\n\nHier rufen wir das Paket psych auf, nutzen aus diesem die Funktion alpha, um Cronbach’s Alpha zu berechnen\n\n\n\n\n\nReliability analysis   \nCall: psych::alpha(x = ., check.keys = TRUE)\n\n  raw_alpha std.alpha G6(smc) average_r S/N    ase mean sd median_r\n      0.92      0.92    0.94      0.43  11 0.0021    4  1     0.42\n\n    95% confidence boundaries \n         lower alpha upper\nFeldt     0.91  0.92  0.92\nDuhachek  0.91  0.92  0.92\n\n Reliability if an item is dropped:\n                     raw_alpha std.alpha G6(smc) average_r  S/N alpha se var.r\nVer_Gesundheitswesen      0.92      0.92    0.94      0.44 11.0   0.0021 0.024\nVer_BVerfG                0.91      0.91    0.94      0.43 10.5   0.0022 0.023\nVer_Bundestag             0.91      0.91    0.93      0.41  9.9   0.0024 0.020\nVer_Verwaltung            0.91      0.91    0.94      0.43 10.6   0.0022 0.025\nVer_kath_Kirche           0.92      0.92    0.94      0.45 11.7   0.0020 0.018\nVer_evan_Kirche           0.92      0.92    0.94      0.45 11.3   0.0020 0.021\nVer_Justiz                0.91      0.91    0.93      0.42 10.3   0.0023 0.023\nVer_TV                    0.91      0.91    0.93      0.43 10.7   0.0022 0.023\nVer_Zeitung               0.91      0.91    0.93      0.43 10.5   0.0022 0.023\nVer_Uni                   0.91      0.92    0.94      0.44 10.8   0.0022 0.024\nVer_Regierung             0.91      0.91    0.93      0.41  9.8   0.0024 0.020\nVer_Polizei               0.91      0.91    0.94      0.43 10.7   0.0022 0.024\nVer_Parteien              0.91      0.91    0.93      0.42 10.1   0.0023 0.022\nVer_Kom_EU                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\nVer_EU_Par                0.91      0.91    0.93      0.42 10.0   0.0023 0.020\n                     med.r\nVer_Gesundheitswesen  0.43\nVer_BVerfG            0.41\nVer_Bundestag         0.41\nVer_Verwaltung        0.42\nVer_kath_Kirche       0.43\nVer_evan_Kirche       0.43\nVer_Justiz            0.41\nVer_TV                0.43\nVer_Zeitung           0.41\nVer_Uni               0.42\nVer_Regierung         0.41\nVer_Polizei           0.43\nVer_Parteien          0.41\nVer_Kom_EU            0.41\nVer_EU_Par            0.41\n\n Item statistics \n                        n raw.r std.r r.cor r.drop mean  sd\nVer_Gesundheitswesen 3238  0.58  0.58  0.53   0.51  4.9 1.4\nVer_BVerfG           3238  0.70  0.69  0.67   0.64  5.3 1.5\nVer_Bundestag        3238  0.83  0.82  0.82   0.79  4.1 1.6\nVer_Verwaltung       3238  0.66  0.66  0.62   0.60  4.5 1.3\nVer_kath_Kirche      3238  0.47  0.46  0.42   0.38  2.3 1.5\nVer_evan_Kirche      3238  0.54  0.52  0.49   0.45  3.0 1.7\nVer_Justiz           3238  0.73  0.73  0.70   0.67  4.6 1.5\nVer_TV               3238  0.64  0.65  0.62   0.58  3.6 1.3\nVer_Zeitung          3238  0.67  0.68  0.66   0.62  4.0 1.3\nVer_Uni              3238  0.61  0.63  0.58   0.56  5.2 1.2\nVer_Regierung        3238  0.83  0.83  0.83   0.80  4.1 1.6\nVer_Polizei          3238  0.63  0.64  0.60   0.57  4.9 1.4\nVer_Parteien         3238  0.78  0.77  0.76   0.74  3.2 1.3\nVer_Kom_EU           3238  0.80  0.79  0.81   0.75  3.5 1.5\nVer_EU_Par           3238  0.80  0.79  0.81   0.76  3.6 1.6\n\nNon missing response frequency for each item\n                        1    2    3    4    5    6    7 miss\nVer_Gesundheitswesen 0.02 0.04 0.10 0.17 0.28 0.28 0.11    0\nVer_BVerfG           0.02 0.04 0.07 0.15 0.19 0.28 0.24    0\nVer_Bundestag        0.08 0.10 0.16 0.25 0.24 0.14 0.04    0\nVer_Verwaltung       0.03 0.05 0.13 0.26 0.31 0.18 0.04    0\nVer_kath_Kirche      0.42 0.20 0.16 0.13 0.05 0.03 0.02    0\nVer_evan_Kirche      0.26 0.16 0.19 0.19 0.11 0.07 0.02    0\nVer_Justiz           0.04 0.06 0.13 0.21 0.25 0.24 0.07    0\nVer_TV               0.08 0.13 0.21 0.33 0.18 0.05 0.01    0\nVer_Zeitung          0.05 0.09 0.18 0.30 0.26 0.11 0.01    0\nVer_Uni              0.01 0.02 0.05 0.17 0.29 0.34 0.12    0\nVer_Regierung        0.10 0.10 0.14 0.22 0.24 0.16 0.04    0\nVer_Polizei          0.02 0.04 0.08 0.18 0.28 0.30 0.10    0\nVer_Parteien         0.13 0.18 0.25 0.28 0.13 0.03 0.00    0\nVer_Kom_EU           0.13 0.14 0.19 0.26 0.19 0.08 0.02    0\nVer_EU_Par           0.13 0.14 0.18 0.25 0.19 0.09 0.02    0"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-crombachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "",
    "text": "Zur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "href": "Skript_5.3.html#interpretation-des-wirksamkeit-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.6 Interpretation des Wirksamkeit-Indizes",
    "text": "1.6 Interpretation des Wirksamkeit-Indizes\nDie Werte sind ein gutes Ergebnis. Die Items zeigen eine gute Inter-Item-Korrelation.\nWir können noch nachschauen, ob wir die Skalen-Reliabilität verbessern können, indem wir einzelne Items herauswerfen. Denn der Output von Cronbachs Alpha gibt uns auch hilfreiche Aufschlüsse darüber, welche Items man evtl. ausschließen kann, um Cronbachs Alpha bei ungenügender Höhe noch auf ein mindestens akzeptables Maß zu heben. Diese Information findet sich im Bereich “Reliability if an item is dropped”:. In unserem Fall wird die reliabitlitä aber noch schlechter - wir können nichts mehr verbessern.\nEntsprechend haben wir erfolgreich einen Index für die latente Variable Vertrauen in gesellschaftliche Institutionen gebildet. Wir haben eine theoretische Grundlage gefunden, diese empirisch anhand der Daten des Allbus mittels explorativer Faktorenanalyse geprüft und einen Summenindex berechnet, dessen Qualität wir mittels Cronbachs Alpha zeigen konnten.\n\n\n\n\n\n\nWie gebe ich die Ergebnisse korrekt an?\n\n\n\nDie Ergebnisse der Indexbildung werden meistens direkt im Text angegeben:\n✅ die Art des gebildeten Index (Summenindex, etc.)\n✅ Cronbachs Alpha\n✅ Enthaltene Indikatoren\nDas Format ist normalerweise:\n\nBeispiel: Der Summenindex individuelle Identität umfasst fünf Indikatoren (Ziele und Befriedigung, Regeln und Verantwortung, Gefühle oder Emotionen, Verständnis der Welt, individuelle Identität im Allgemeinen; α = 0,84)."
  },
  {
    "objectID": "Skript_5.3.html#quellen-für-das-script",
    "href": "Skript_5.3.html#quellen-für-das-script",
    "title": "Reliabilität von Skalen",
    "section": "1.8 Quellen für das Script",
    "text": "1.8 Quellen für das Script\nStephanie Geise Daniela Keller https://www.youtube.com/watch?v=NFPGQcq1fO8 Field, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811"
  },
  {
    "objectID": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "href": "Skript_5.2.html#analyselogik-ziel-und-einsatzgebiete-einer-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Video\nWir beginnen zunächst mit der Vorbereitung und Laden die notwendigen Pakete und Daten."
  },
  {
    "objectID": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "href": "Skript_5.2.html#suche-nach-der-zugrunde-liegenden-variable---die-faktorenanalyse",
    "title": "Die Faktorenanalyse",
    "section": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse",
    "text": "1.3 Suche nach der zugrunde liegenden Variable - Die Faktorenanalyse\nDie Faktorenanalyse bringt, wie jedes statistische Verfahren, eine Reihe von Vorraussetzungen mit. Diese Vorraussetzungen sollten wir kennen und bei der Anwendung der Faktorenanalyse beachten. Viele der Vorraussetzungen beziehen sich auf Pearson-Korrelationskoeffizienten, welcher die statistsiche Grundlage für die Berechnung der Faktoren bildet.\n\nVarianz: Wir sollten sichergehen, dass die Daten aus unserer Stichprobe ausreichend varrieren. Wir werfen hierfür ein Blick in die Daten.\n\n\n1colors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n1\n\nVisuelle Überprüfung mit einem Histogram für die erste Variable Ver_Gesundheitswesen. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nLinearität: Der Pearson-Korrelationskoeffizient (r) misst die lineare Beziehung zwischen zwei Variablen. Wenn die tatsächliche Beziehung nicht linear ist, dann verringert sich der Wert von r. Wir können auf Linearität u.a. visuell durch das Betrachten der Daten mittel Streudiagramm prüfen.\n\n\n1ggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen, y = Ver_BVerfG)) +\n  geom_point() +\n  geom_abline(linetype = \"dashed\", color = \"darkgreen\", size = 1)\n\n\n1\n\nVisuelle Überprüfung mit einem Streudiagramm für die erste Variable Ver_Gesundheitswesen & Ver_BVerfG. Die restlichen Variablen sollten auch überprüft werden.\n\n\n\n\n\n\n\n\nNormalverteilung: Der Pearson-Korrelationskoeffizient setzt eine Normalverteilung voraus. Allerdings finden sich in der Realität fast nie perfekt normalverteilte Daten. Schiefe und Kurtosis sind besonders einflussreich die Ergebnisse der Faktorenanalyse und können im Extremfall artefaktische Ergenbnisse erzeugen.\n\n\ncolors = c(rep(\"darkgreen\", 1))\n\nggplot(allbus_vertrauen, aes(x = Ver_Gesundheitswesen)) +\n  geom_histogram(binwidth = 0.5, fill = colors)\n\n\n2\n\nStatistische Überprüfung mittels Shapiro Wilk Test für die erste Variable Ver_Gesundheitswesen. Ein p-Wert unter 0.05 = keine Normalverteilung und ein p-Wert über 0.05 = Normalverteilung\n\n\n\n\n\n\n2shapiro.test(allbus_vertrauen$Ver_Gesundheitswesen)\n\n\n    Shapiro-Wilk normality test\n\ndata:  allbus_vertrauen$Ver_Gesundheitswesen\nW = 0.92081, p-value &lt; 2.2e-16\n\n\n\nLevel der Messung: Bei Pearson-Korrelationen wird davon ausgegangen, dass normalverteilte Variablen auf Intervall- oder Verhältnisskalen gemessen werden, d. h. es handelt sich um kontinuierliche Daten mit gleichen Intervallen. Diese Eigenschaften treffen nicht auf ordinale (bspw. Kategorien) oder dochotome (bspw. Wahr-Falsch-Items) Variablen zu, was sich negativ auf Pearson-Korrelationskoeffizieten auswirkt und zu verzerrten Ergebnissen führen kann. Allerdings ist ein beträchtlicher Teil der Daten, mit denen wir zu tun haben, ordinal oder dichotom skaliert, um auch mit diesen Daten arbeiten zu können nutzen wir die polychorische Korrelation, welche robuster Nicht-Normalverteilung ist.\nFehlende Werte: In jeder Studie sollten wir die Anzahl und die Art der fehlenden Werte sowie die Gründe und die Methoden für den Umgang mit diesen Daten angegeben werden.\n\n\nallbus_vertrauen = allbus_vertrauen %&gt;%   \n1  mutate(across(Ver_Gesundheitswesen:Ver_EU_Par, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n2  na.omit()\n\n\n1\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter. Während %in% angibt, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n2\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz"
  },
  {
    "objectID": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "href": "Skript_5.2.html#teildatensatz-mit-den-benötigten-variablen",
    "title": "Die Faktorenanalyse",
    "section": "1.1 Teildatensatz mit den benötigten Variablen",
    "text": "1.1 Teildatensatz mit den benötigten Variablen\nDie Variablen werden aufgrund ihrer Nützlichkeit als Indikatoren für die zu untersuchende latente Variable ausgewählt. Entsprechend ist es wichtig, dass die Variablen inhaltliche, diskriminante und konvergente Validität aufweisen. Etwas vereinfacht ausgedrückt sollten die Indikatoren über eine inhaltliche Passung zur latenten Variable verfügen, möglichst gut von anderen latenten Variablen abgrenzbar und mit mehreren unterschiedlichen Arten der Messung nachweisbar sein.\nIn unserem Fall möchten wir das Vertrauen in gesellschaftliche Institutionen untersuchen, entsprechend sollten wir Variablen bzw. Indikatoren auswählen, die die unterschiedlichen Bestandteile der latenten Variable abdecken.\nGanz konkret wählen wir Variablen aus, die das…\n\nVertrauen in das Gesundheitswesen (pt01)\nVertrauen in das Bundesverfassungsgericht (pt02)\nVertrauen in den Bundestag (pt03)\nVertrauen in die Stadt- oder Gemeindeverwaltung (pt04)\nVertrauen in die Katholische Kirche (pt06)\nVertrauen in die Evangelische Kirche (pt07)\nVertrauen in die Justiz (pt08)\nVertrauen in das Fernsehen (pt09)\nVertrauen in das Zeitungswesen (pt10)\nVertrauen in die Hochschulen (pt11)\nVertrauen in die Bundesregierung (pt12)\nVertrauen in die Polizei (pt14)\nVertrauen in die Politischen Parteien (pt15)\nVertrauen in die Kommission der EU (pt19)\nVertrauen in das Europäische Parlament (pt20)\n\n… erfassen.\nFür die statistische Identifizierung einer latenten Variablen bzw. eines Faktors werden mindestens drei gemessene Variablen benötigt, obwohl mehr Indikatoren vorzuziehen sind. Es werden beispielsweise auch vier bis sechs Indikatoren pro Faktor empfohlen. Im Allgemeinen funktioniert die EFA besser, wenn jeder Faktor überdeterminiert ist (d. h. es werden mehrere gemessene Variablen von der zu entdeckenden latenten Variable bzw. Faktor beeinflusst). Unabhängig von der Anzahl sollten Variablen, die voneinander abhängig sind, nicht in eine EFA einbezogen werden.\n\n1allbus_vertrauen = daten %&gt;%\n2  select(pt01:pt20) %&gt;%\n3  mutate(across(pt01:pt20, ~ as.numeric(.))) %&gt;%\n4  mutate(across(pt01:pt20, ~ ifelse(.%in% c(-9, -11, -42), NA,.))) %&gt;%\n5  na.omit()\n\n\n1\n\nWir erstellen ein neues Objekt basierend auf dem Datensatz daten\n\n2\n\nMit dem Doppelpunkt wählen wir alle Variablen zwischen pt01 bis pt20 aus\n\n3\n\nDie Kombination aus mutate und across ermöglicht es uns die Funktion as.numeric in einer Zeile auf alle zuvor ausgewählten Variablen anzuwenden. Die ~ gibt den Start der anzuwenden Funktion (hier as.numeric an), der Punkt innerhalb der Klammer der as.numeric Funktion dient als eine Art Platzhalter für die zuvor ausgewählten Variablen\n\n4\n\nWir codieren die unterschiedlichen fehlenden Werte um (aus der Allbus-Dokumentation entnommen). Hierfür greifen wir auf die Funktion if_elsezurück. Dabei handelt es sich um ein Entweder-Oder-Befehl. Konkret wird zunächst eine Bedingung geprüft und dann auf dieser Grundlage entweder Option 1 oder Option 2 ausgeführt. Der Punkt vor %in% dient wiederum als Platzhalter, mit dem Ausdruck %in% prüfen wir, ob die nachfolgenden Werte (also c(-9, -11, -42)) in der jeweiligen Variable vorkommen. Hierbei handelt es sich um unsere Bedingung für den Entweder-Oder-Befehl. Wenn einer der Werte aus der geprüften Variable einem der spezifizierten Werte entspricht - also unsere Bedingung erfüllt - wird dieser durch NA ersetzt. Ist die Bedingung nicht erfüllt, wird die andere Option ausgeführt und der bereits existierende Wert wird beibehalten.\n\n5\n\nWir entfernen mit der Funktion na.omit fehlende Werte aus dem Datensatz\n\n\n\n\nWir bennen die ausgewählten Indikatoren um, damit die Bezeichnungen der Indikatoren für uns leichter zu merken sind.\n\nallbus_vertrauen = allbus_vertrauen %&gt;% \n  \n1  rename(Ver_Gesundheitswesen = pt01,\n         Ver_BVerfG = pt02,\n         Ver_Bundestag = pt03,\n         Ver_Verwaltung = pt04,\n         Ver_kath_Kirche = pt06,\n         Ver_evan_Kirche = pt07,\n         Ver_Justiz = pt08,\n         Ver_TV = pt09,\n         Ver_Zeitung = pt10,\n         Ver_Uni = pt11,\n         Ver_Regierung = pt12,\n         Ver_Polizei = pt14,\n         Ver_Parteien = pt15,\n         Ver_Kom_EU = pt19,\n         Ver_EU_Par = pt20)\n\n2htmlTable(head(allbus_vertrauen))\n\n\n1\n\nMit dem rename Befehl können wir die Variablen umbennen\n\n2\n\nWir überprüfen kurz, ob die Umbenennung geklappt hat und lassen uns die ersten Zeilen des Datensatzes anzeigen. Hierfür nutzen wir htmlTable für eine schönere Darstellung der Tabelle sowie head, um uns die ersten paar Zeilen des Datensatzes allbus_vertrauen anzeigen zu lassen\n\n\n\n\n\n\n\n\nVer_Gesundheitswesen\nVer_BVerfG\nVer_Bundestag\nVer_Verwaltung\nVer_kath_Kirche\nVer_evan_Kirche\nVer_Justiz\nVer_TV\nVer_Zeitung\nVer_Uni\nVer_Regierung\nVer_Polizei\nVer_Parteien\nVer_Kom_EU\nVer_EU_Par\n\n\n\n\n1\n7\n7\n4\n2\n3\n7\n4\n2\n4\n7\n6\n3\n3\n5\n5\n\n\n2\n4\n7\n4\n4\n2\n5\n5\n5\n5\n5\n6\n5\n4\n5\n6\n\n\n3\n3\n4\n5\n6\n1\n3\n6\n4\n4\n4\n5\n6\n4\n4\n4\n\n\n4\n5\n5\n4\n5\n3\n5\n4\n5\n6\n6\n4\n5\n3\n5\n5\n\n\n5\n4\n7\n5\n3\n2\n2\n5\n3\n4\n5\n5\n6\n3\n3\n3\n\n\n6\n3\n3\n3\n3\n1\n2\n3\n3\n3\n4\n4\n4\n2\n2\n2\n\n\n\n\n\nNeben der Auswahl der Variablen bzw. Indikatoren müssen auch die Fälle (in unserem Fall die Anzahl der befragten Personen) festgelegt werden. Hier sollten wir uns zunächst fragen, ob die Stichprobe der Teilnehmer:innen in Bezug auf die gemessenen Indikatoren sinnvoll ist? Handelt es sich um eine repräsentative Stichprobe? Bei dem Allbus ist das der Fall und entsprechend können wir davon ausgehen, dass wir eine passende Stichprobe für die Durchführung eine EFA vorliegen haben."
  },
  {
    "objectID": "Skript_5.2.html#referenzen",
    "href": "Skript_5.2.html#referenzen",
    "title": "Die Faktorenanalyse",
    "section": "",
    "text": "Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. https://doi.org/10.1177/0095798418771807"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-würdigung",
    "href": "Skript_5.2.html#danksagung-und-würdigung",
    "title": "Die Faktorenanalyse",
    "section": "1.4 Danksagung und Würdigung",
    "text": "1.4 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-würdigung",
    "href": "Skript_5.3.html#danksagung-und-würdigung",
    "title": "Reliabilität von Skalen",
    "section": "1.9 Danksagung und Würdigung",
    "text": "1.9 Danksagung und Würdigung\nBei der Struktur und dem Inhalt dieser Seite haben mich die folgenden herausragenden Arbeiten inspieriert. Ich möchte mich bei den Autor:innen für ihre Arbeit sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "href": "Skript_5.3.html#verschiedene-arten-von-indizes",
    "title": "Reliabilität von Skalen",
    "section": "1.2 Verschiedene Arten von Indizes",
    "text": "1.2 Verschiedene Arten von Indizes\nEs gibt eine ganze Reihe von möglichen Arten von Indizes, welche wir theoretisch berechnen könnten.\n\n\n\n\n\n\n\n\nArt des Index\nBildung (Beispiel)\nBeschreibung\n\n\n\n\nUngewichteter additivier Index\nIndex = Indikator_1 + Indikator_2 + Indikator_3\nDie Ausprägungen der Indikatorvariablen werden addiert bzw. zu gemittelt.\n\n\nUngewichteter multiplikativer Index\nIndex = Indikator_1 * Indikator_2 * Indikator_3\nWenn ein Index Mindestausprägungen auf allen Indikatorvariablen voraussetzt sollte multiplikativ zu einem Gesamtindex verknüpft werden.\n\n\nGewichteter additivier Index\nIndex = (2*Indikator_1) + Indikator_2 + Indikator_3\nGewichtete additive Indizes ermöglichen eine differenzierte Behandlung der einzelnen Indikatoren.\n\n\n\nDie Entscheidung, welche Art der Indexbildung gewählt wird sollte vor dem Hintergrund der Daten, sowie der latenten Variable und deren Eigenschaften erfolgen. Beispielsweise würde es für ein Index, welcher die Zufriedenheit mit einer Bahnreise widerspiegelt und aus den Inidkatoren Reisedauer, Service während der Reise, Komfort während der Fahrt gebildet wird, Sinn ergeben einen Ungewichteten multiplikativen Index zu bilden, da bei einer Reisedauer von Null keine Reise stattgefunden hat und somit auch die anderen beiden Indikatoren nicht von Bedeutung sind."
  },
  {
    "objectID": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "href": "Skript_5.3.html#interpretation-von-cronbachs-alpha",
    "title": "Reliabilität von Skalen",
    "section": "1.5 Interpretation von Cronbach’s Alpha",
    "text": "1.5 Interpretation von Cronbach’s Alpha\nZur Einordnung der erhaltenen Skalen-Reliabilität, gehen wir von folgenden Cronbachs Alpha-Bereichen und ihrer Beurteilung aus:\n\nWerte &lt;0,6: nicht akzeptabel\n0,6 bis 0,7: akzeptabel, teilweise auch schon als grenzwertig klassifiziert\n0,7 bis 0,8: gut, stellenweise auch nur als akzeptabel klassifiziert\n0,8 bis 0,9: sehr gut"
  },
  {
    "objectID": "Skript_5.2.html#danksagung-und-literatur",
    "href": "Skript_5.2.html#danksagung-und-literatur",
    "title": "Die Faktorenanalyse",
    "section": "2.1 Danksagung und Literatur",
    "text": "2.1 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\nBrown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Brown T. A. (2015). Confirmatory factor analysis for applied research (2nd ed.). New York, NY: Guilford Press. Watkins, M. W. (2018). Exploratory Factor Analysis: A Guide to Best Practice. Journal of Black Psychology, 44(3), 219–246. Link\n\n\n📖 Fabrigar, L. R., & Wegener, D. T. (2012). Exploratory factor analysis. Oxford University Press.\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Koirala, B. P., Araghi, Y., Kroesen, M., Ghorbani, A., Hakvoort, R. A., & Herder, P. M. (2018). Trust, awareness, and independence: Insights from a socio-psychological factor analysis of citizen knowledge and participation in community energy systems. Energy research & social science, 38, 33-40. Link"
  },
  {
    "objectID": "Skript_5.3.html#danksagung-und-literatur",
    "href": "Skript_5.3.html#danksagung-und-literatur",
    "title": "Reliabilität von Skalen",
    "section": "1.7 Danksagung und Literatur",
    "text": "1.7 Danksagung und Literatur\nDie Struktur und Inhalt dieser Seite orientiert sich an den folgenden Arbeiten. Ich möchte mich bei den Autor:innen sehr bedanken!\n\nDaniela Keller Link\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using R. 749-811\n\n\n\n\n\n\n\nLiteratur und Beispiele aus der Praxis\n\n\n\nWir empfehlen euch folgende Lehrbücher, falls ihr weiterführende Informationen braucht.\n\n📖 Field, Z., Miles, J., & Field, A. (2012). Discovering statistics using R. Discovering statistics using r, 1-992. Link\n\n\n📖 Döring, N., & Bortz, J. (2016). Forschungsmethoden und evaluation. Wiesbaden: Springerverlag. Link\n\nHier findet ihr ein Beispiel aus der Forschungspraxis:\n\n🔬 Zerrer, P., & Engelmann, I. (2022). Users’ Political Motivations in Comment Sections on News Sites. International Journal of Communication, 16, 23. Link"
  },
  {
    "objectID": "Skript_1.1.html#installation-von-r-und-rstudio",
    "href": "Skript_1.1.html#installation-von-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "3 Installation von R und RStudio",
    "text": "3 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "href": "Skript_1.1.html#projekte-und-ordnerstrukturen",
    "title": "Einführung in R und RStudio",
    "section": "4 Projekte und Ordnerstrukturen",
    "text": "4 Projekte und Ordnerstrukturen\nEinführung in die Logik von R, R Studio Server und das R Environment Einführung in Markdown (Quarto?) Basics der Befehlssyntax in Base R und Tidyverse (im Vergleich), ab dann aber alles in dplyr Laden von Daten und Importieren von anderen Datenformaten Einführung in das Datenmanagement: Projekte und Ordnerstrukturen auf dem PC Öffnen von Datensätzen, Laden von Daten, Importieren von anderen Datensätzen Speichern von Daten aus R in verschiedenen Formaten (Rda, csv etc.)"
  },
  {
    "objectID": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "href": "Skript_1.1.html#pakete-als-erweiterung-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Pakete als Erweiterung von R",
    "text": "5 Pakete als Erweiterung von R\nInstallieren von Paketen und Laden von Librarys (p_load) Vorschau: Datentypen und deren Charakteristika."
  },
  {
    "objectID": "Skript_1.3.html#r-markdown",
    "href": "Skript_1.3.html#r-markdown",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown",
    "text": "1 R Markdown\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen. Markdown bzw. Quarto-Dokumente bestehen aus drei Bestandteilen\n\n1.1 YAML-Header\n\n\n1.2 Text\n\n\n1.3 Code Chunks\n💡"
  },
  {
    "objectID": "Skript_1.3.html#quarto",
    "href": "Skript_1.3.html#quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Quarto",
    "text": "2 Quarto\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen)."
  },
  {
    "objectID": "Skript_1.3.html#literatur",
    "href": "Skript_1.3.html#literatur",
    "title": "Die Logik von Markdown und Quarto",
    "section": "7 Literatur",
    "text": "7 Literatur\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link\n\n\n📖 Xie, Y., Allaire, J. J., & Grolemund, G. (2020). R markdown: The definitive guide. Chapman; Hall/CRC Link\n\n\n📖 Xie, Y., Dervieux, C., & Riederer, E. (2020). R markdown cookbook. Chapman and Hall/CRC Link\n\n\n📖 Allaire, J. J., Xie, Y., McPherson, J., Luraschi, J., Ushey, K., Atkins, A., Wickham, H., Cheng, J., Chang, W., & Iannone, R. (2020a). rmarkdown: Dynamic documents for r. Link"
  },
  {
    "objectID": "Autoren.html#katharina-maubach",
    "href": "Autoren.html#katharina-maubach",
    "title": "Das Team stellt sich vor",
    "section": "Katharina Maubach",
    "text": "Katharina Maubach\nVita\n\n\n\n\n\nKatharina Maubach, © B. Köhler\n\n\nKatharina Maubach ist seit Dezember 2022 als wissenschaftliche Mitarbeiterin am Zentrum für Medien-, Kommunikations- und Informationsforschung der Universität Bremen im Lab „Politische Kommunikation und Innovative Methoden“ tätig. Sie forscht im Rahmen des DFG-geförderten Projektes „Remixing Political News Reception“ unter der Leitung von Prof. Stephanie Geise zur Rezeption multimodaler Medieninhalte. Innerhalb des Projektes war sie seit Februar 2021 an der Universität Münster beschäftigt, wo Sie zudem von Januar 2019 bis Januar 2021 im Arbeitsbereich von Prof. Volker Gehrau forschte und lehrte. Ihr Studium der Kommunikationswissenschaft absolvierte sie ebenfalls an der Universität Münster; in ihrer Masterarbeit forschte sie zum Einfluss politischer Nachrichtensatire auf politisches Interesse und Informiertheit.\nForschungsschwerpunkte\n\nPolitische Kommunikation\nRezeptions- und Wirkungsforschung\nKlimakommunikation\nStatistische Methoden\n\n\n\n\n\n\n\nKontakt\n\n\n\n\n\n🏠 Universität Bremen; ZeMKI, Zentrum für Medien-, Kommunikations- und Informationsforschung; Linzer Str. 4; 28359 Bremen\n🎓 Webseite\n#️⃣ Twitter"
  },
  {
    "objectID": "Skript_1.4.html",
    "href": "Skript_1.4.html",
    "title": "Vorbereiten der R-Umgebung",
    "section": "",
    "text": "Bild generiert von Midjourney"
  },
  {
    "objectID": "Skript_1.4.html#section",
    "href": "Skript_1.4.html#section",
    "title": "Die Kunst vom Umgang mit Daten",
    "section": "1.1 ",
    "text": "1.1"
  },
  {
    "objectID": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "href": "Skript_1.4.html#das-anlegen-eines-r-projekts",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.1 Das Anlegen eines R-Projekts",
    "text": "1.1 Das Anlegen eines R-Projekts\nStarten wir zunächst mit dem Anlegen eines sogenannten R-Projekts. RStudio-Projekte ermöglichen es uns alle mit einem Projekt verbundenen Dateien an einem Ort zu speichern. Das umfasst Datensätze, R-Skripte, Ergebnisse, Abbildungen, Berichte usw. Projekte sind bereits in RStudio integriert.\n Das gerade aktive RProjekt sehen wir in der rechts oberen Ecke des Nutzer:innen Interfaces von RStudio. Hier können wir durch durch den Button New Project ein neues Projekt anlegen, hierfür folgen wir einfach dem Menüverlauf. Für einen guten, reproduzierbaren Arbeitsablauf sollten alle Analyse Projekte mit der Erstellung eines Projekts beginnen."
  },
  {
    "objectID": "Skript_1.4.html#das-working-dictonary-wd",
    "href": "Skript_1.4.html#das-working-dictonary-wd",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.2 Das Working-Dictonary (wd)",
    "text": "1.2 Das Working-Dictonary (wd)\nIm Allgemeinen ist das Arbeitsverzeichnis (das Working-Dictonary, wd) der Ort, an dem R nach Dateien (vor allem nach Datensätzen) sucht. Wenn ihr kein Projekt verwenden, müsst ihr wahrscheinlich durch die Funktion setwd oder das Interface (siehe Screenshot) ein Working-Dictonary setzen, bevor ihr euren R-Code ausführen könnt.\n Wenn ihr beispielsweise den Code verwenden möchtet, den wir euch in diesem Kurs zu Verfügung stellen, müsst ihr darauf achten, dass ihr die Dateipfade auf euer Working-Dictonary und Ordnerstruktur anpasst.\n\n1daten = haven::read_dta(\"Datensatz/Allbus_2021.dta\")\n\n\n1\n\nDer Bereich in Anführungszeichen gibt den Dateipfad an. Damit sagt ihr R, wo sich die zu ladende Datei befindet.\n\n\n\n\nIn dem Beispiel ist im Working-Dictonary des Projekts ein Unterordner mit der zu ladenden Datei. Der Dateipfad befindet sich im R-Code in den Anführungszeichen, also \"Datensatz/Allbus_2021.dta\". Der erste Teil \"Datensatz/\" gibt an, dass sich die Datei in einem Ordner befindet, der Datensatz heißt. Der zweite Teil Allbus_2021.dta\" gibt den eigentlichen Dateinamen mit der entsprechend Dateiendung an (also .dta). Wahrscheinlich müsst ihr den R-Code entsprechend anpassen."
  },
  {
    "objectID": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "href": "Skript_1.4.html#das-zugreifen-auf-dateien-und-datensätze",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.3 Das Zugreifen auf Dateien und Datensätze",
    "text": "1.3 Das Zugreifen auf Dateien und Datensätze\nBevor wir allerdings auf Dateien und Datensätze zugreifen können, müssen wir diese zunächst in unseren Projekt-Ordner (also in das Working-Dictonary) verschieben. Falls ihr für diesen Kurs die Desktop-Version von RStudio nutzt, könnt ihr hier einfach die entsprechenden Datein auf eurem lokalen Rechner in das zuvor von euch festgelegte Working-Dictonary kopieren. Wenn ihr die Cloud-Version von RStudio nutzt, müsst ihr hier die Dateien und Datensätze zunächst in die Cloud laden. Hierfür könnt ihr auf den Upload-Button in der unteren rechten Ecke des Interfaces gehen und in dem sich öffnenden Fenster die entsprechende Datei auswählen.\n\n\n\nScreenshot RStudio\n\n\nSobald ihr durch das Klicken von OK bestätigt habt, wird die Datei hochgeladen und erscheint in dem unteren rechten Fenster im Files-Reiter."
  },
  {
    "objectID": "Skript_1.4.html#die-installation-von-paketen",
    "href": "Skript_1.4.html#die-installation-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Installation von Paketen",
    "text": "1.4 Die Installation von Paketen\nWir möchten für unser Analyse-Projekt nicht nur die Standartausführung von R verwenden, welche auch als base R bezeichnet wird, sondnern einige Pakte installieren. R-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Wir können Pakete über CRAN (Comprehensive R Archive Network) - das ist ein zentrales Software-Repository - installieren. R hat den Vorteil, dass es über eine große Zahl an frei verfügbaren und einfach zu installierenden Paketen verfügt. Das hat unter anderem dazu geführt, dass die Programmiersprache im Data Science Bereich verbreitet ist.\nWenn wir ein R-Paket installieren möchten können wir hierfür die Funktion install.packages verwenden, in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen.\n\ninstall.packages(\"tidyverse\", repos = \"http://cran.us.r-project.org\")\n\nPaket 'tidyverse' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpyghIHy\\downloaded_packages\n\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und Bestätigt.\n Einige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden."
  },
  {
    "objectID": "Skript_1.4.html#das-laden-von-paketen",
    "href": "Skript_1.4.html#das-laden-von-paketen",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Das Laden von Paketen",
    "text": "1.5 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet.\nDies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen).\n\nlibrary(tidyr)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden.\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio"
  },
  {
    "objectID": "Skript_1.4.html#die-session",
    "href": "Skript_1.4.html#die-session",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.4 Die Session",
    "text": "1.4 Die Session\nDie sogenannte Session wurde bereits mehrmals kurz erwähnt. Bei der Session handelt es sich um eine laufende Instanz des R-Programms, die ihr beenden und neustarten könnt. Das macht insbesondere dann Sinn, wenn ihr die falsche Version eines Pakets installiert habt oder ihr einen sauberen Start für euren R Code haben möchtet. Durch einen Neustart der Session könnt ihr euch sicher sein, dass ihr nicht ausversehen irgendwelche zuvor verwendeten Pakete oder Berechnungen mitnehmt.\nIhr könnt die Session mit der .rs.restartR Funktion in der Konsole neustarten. Alternativ könnt ihr im R Studio Interface auf Sessionund Restart R gehen.\n\n\n\nScreenshot RStudio\n\n\nEin kurze Mitteilung in der Konsole gibt euch an, dass die Session neu gestartet wurde."
  },
  {
    "objectID": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "href": "Skript_1.4.html#hilfe-zur-selbsthilfe",
    "title": "Vorbereiten der R-Umgebung",
    "section": "1.5 Hilfe zur Selbsthilfe",
    "text": "1.5 Hilfe zur Selbsthilfe\nIhr werdet im Laufe des Kurses immer wieder mit Fehlermeldungen zu tun haben. Hier ist es wichtig einen kühlen Kopf zu bewahren und möglichst systematisch zu versuchen den Fehler zu finden und zu beheben. Der Umgang mit Fehlern ist ein großer und wichtiger Teil bei jeglicher Programmierung und es gibt hier ein paar Tipps, wie man damit am besten umgeht.\n\nAtmen: Bleib ruhig und atme ein paar mal tief durch, gerade wenn es nicht der erste Fehler ist der dir heute begegnet.\nLesen: Lies aufmerksam die Fehlermeldung.\nCode überprüfen: Schaue nach, ob sich ein Warnzeichen am Rand deines R Codes befindet. Dieses Warnzeichen weist dich auf einen Syntaxfehler (bspw. eine vergessene Klammer oder Komma) hin.\nVariablen überprüfen: Überprüfe kurz, ob du die richtigen Variablen verwendest oder sich ein Tippfehler eingeschlichen hat.\nInformationen suchen: Rufe eine Vignette (mit dem Befehl vignette) oder rufe die Hilfeseite mit einem der Funktion vorgelagerten Fragezeichen auf (mit bspw. ?dpylr()) auf und lies sie dir durch.\nSich im Netz Hilfe suchen: Suche auf Stack Overflow nach deinem Problem, meistens hat jemand auf der Welt das oder ein ähnliches Problem schon einmal gelöst.\nDie KI fragen: Nutze ein KI Chatbot, um dir die Fehlermeldung erklären zu lassen und Lösungsvorschläge zu unterbreiten. Beachte hierbei bitte, dass die Antworten nicht immer korrekt sind und meist Kontextwissen von dir benötigt wird, um die Korrektheit der Antworten zu prüfen Link.\nFrag uns: Frag dein:e Dozent:in :)"
  },
  {
    "objectID": "Skript_1.1.html#was-ist-r",
    "href": "Skript_1.1.html#was-ist-r",
    "title": "Einführung in R und RStudio",
    "section": "1 Was ist R?",
    "text": "1 Was ist R?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten."
  },
  {
    "objectID": "Skript_1.1.html#was-ist-rstudio",
    "href": "Skript_1.1.html#was-ist-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "2 Was ist RStudio?",
    "text": "2 Was ist RStudio?\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten."
  },
  {
    "objectID": "Skript_1.1.html#das-interface-von-rstudio",
    "href": "Skript_1.1.html#das-interface-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Das Interface von RStudio",
    "text": "4 Das Interface von RStudio\nGrundsätzliches zur Oberfläche\nSchaltflächen\nMenüleiste\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "href": "Skript_1.1.html#die-programmoberfläche-von-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "4 Die Programmoberfläche von RStudio",
    "text": "4 Die Programmoberfläche von RStudio\n\n4.1 Schaltflächen\nInnerhalb von RStudio unterscheiden wir 4 Schaltflächen welche sich beliebig via Drag and Drop verschieben oder auch minimieren lassen.\n\n1 Ist der Bereich in welchen wir Skripte, Markdown und Quarto-Dokumente bearbeiten und ausführen können (siehe auch Markdown und Quarto)\n2 Ist die Konsole. Dies ist der Bereich in welchem wir weiterhin oldschool R angezeigt bekommen. Dieser Bereich kann sehr hilfreich sein, wenn man kurz Befehle benötigt, welche nicht im Skript auftauchen sollen (beispielsweise eine kurze Hilfe zu Funktionen mittels ?)\n3 In diesem Bereich findet sich alles zu den innerhalb von R geladenen Datensätzen. Unter Environment finden sich die Datensätze (Data), die Historie der genutzten Befehle (History), eine Schnittstelle zu Datenbanken (Connection) sowie R-interne Tutorials (Tutorial).\n4 In diesem Bereich finden sich verschiedene Reiter, welche die Organisation der Arbeit mit R erleichtern. Unter File befinden sich alle innerhalb des Ordners oder Projektes befindlichen Dateien. Unter Plots kann man sich die in R erstellten Grafiken anzeigen lassen. Packages zeigt alle in R installierten Pakete an. Hier lassen sich mittels install auch neue Pakete installieren oder über update die aktuellen Pakete updaten. Bei Help können wir eine Hilfeseite aufrufen. Es kann wahlweise direkt innerhalb der Seite nach Hilfen gesucht werden oder mit dem Befehl ?Name der Funktion bzw. ??Name des Packages innerhalb der Konsole. Möchte man nach einem Befehl aus einem Paket suchen nutzt man den Suchbefehle?Name des Packages::Name der Funktion. Unter Viewer finden sich gerenderte Dokumente (beispielsweise ein gerendertes Markdown oder Quarto Dokument) und unter Presentation gerenderte Shiny-Dokumente.\n\n\n4.2 Menüleiste\nZusätzlich zu den Schaltflächen findet sich oben links eine Menüleiste:\n\n\nUnter File können neue Dateien erstellt, geöffnet und gespeichert werden.\nEdit bietet Möglichkeiten der Dateibearbeitung (bspw. Kopieren, Ausschneiden, Rückgängig etc.) falls ihr keine Short-Cuts nutzen wollt.\nCode gibt eine Übersicht über Funktionen innerhalb des Markdown-Dokumentes (bspw. Codechunks einfügen).\nUnter View können die einzelnen Schaltflächen und deren Aufteilung geändert werden. Wahlweise geht dies auch via Drag & Drop.\nPlots vereinfacht den Umgang mit in r erstellten Grafiken. Wahlweise könnt ihr hier auch den Reiter Plots in Schaltfläche 4 nutzen.\nSession hier kann eine R-Session neu gestartet oder beendet werden (siehe auch 1.6 Die Session).\nBuild, Debug und Profile beinhaltet Sonderanwendungen, wie beispielsweise das Debugging von Funktionen oder Fragen nach der Speed-Optimierung von R-Code.\nTools hat viele hilfreiche Funktionen. Hier können unter anderem Pakete installiert und hilfreiche Keyboard Shortcuts ausgegeben werden. Am bedeutsamsten ist hier jedoch der Bereich Global Options, in welchem unter anderem grundlegende Einstellungen zum Speicherort von R und den Paketen, zur Aufteilung und Aussehen von RStudio und zur Funktionalität von R Markdown getroffen werden können.\n\nAuch der Reiter Help kann sehr hilfreich sein. Hier finden sich eine Hilfeseite (siehe auch Reiter Help in Schaltfläche 4), Möglichkeiten der besseren Zugänglichkeit (Accessibility), Cheat Sheets für die Arbeit mit R und erneut eine Übersicht über Shortcuts für die Arbeit mit R."
  },
  {
    "objectID": "Skript_1.1.html#die-logik-von-r",
    "href": "Skript_1.1.html#die-logik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Logik von R",
    "text": "5 Die Logik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen. Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#die-grundlogik-von-r",
    "href": "Skript_1.1.html#die-grundlogik-von-r",
    "title": "Einführung in R und RStudio",
    "section": "5 Die Grundlogik von R",
    "text": "5 Die Grundlogik von R\nWie bereits zuvor erwähnt ist R eine Programmiersprache. Das bedeutet, dass alle Schritte in R, vom Datenmanagement bis zu komplizierteren statistischen Analysen, mit Befehlen bzw. Funktionen erfolgen. Grundsätzlich funktionieren Befehle so, dass zunächst der Befehl erfolgt und in Klammern anschließend worauf sich dieser Befehl bezieht.\nDabei können sich manche Befehle auf den gesamten Datensatz beziehen (etwa der Befehl str der die Struktur des Datensatzes anzeigt) oder auch jeweils nur auf einzelne Variablen (beispielsweise wenn wir den Mittelwert einer einzelnen Variablen des Datensatzes ermitteln wollen).\nFür den Befehl str() sieht die Code-Zeile bei einem Datensatz names data wie folgt aus:\n\nstr(data)\n\nFür die Berechnung des Mittelwertes mit der Funktion mean() müssen wir R ebenfalls den Datensatz nennen, auf welchen wir uns beziehen wollen (💡 Fun-Fact, R “denkt nicht mit”, also auch wenn für euch klar ist, dass ihr doch die gesamte Zeit mit dem selben Datensatz arbeitet, muss R das immer wieder gesagt bekommen). Zusätzlich müssen wir die Variable angeben, von welcher der Mittelwert berechnet werden soll. Datensatz und Variable können wir dabei mit einem Dollarzeichen $ verbinden, dass sagt R, dass es sich um die Variable aus dem jeweiligen Datensatz handelt. Möchten wir nun den Mittelwert der Variablen Alter aus dem Datensatz data berechnen, sieht der Code wie folgt aus:\n\nmean(data$Alter)\n\nDabei haben die meisten Funktionen noch weitere Zusatzoptionen, welche wir nutzen können. Bei dem Befehl mean können wir beispielsweise angeben, ob fehlende Werte in die Berechnung mit einfließen sollen oder nicht. Dies geschieht mit den Zusatz na.rm = TRUE bzw. na.rm = FALSE. Na.rm steht in diesem Fall für NA (=not available, fehlende Fälle) und remove (also entfernen), fragt demnach ob fehlende Fälle aus der Berechnung ausgeschlossen werden sollen. Dabei ist die default-Option, also die Option die in dem Befehl voreingestellt ist, dass fehlende Werte nicht aus der Berechnung ausgeschlossen werden (na.rm=F). Die meisten Befehle haben bestimmte defaults, da dies diese den Normalfall der Nutzung beschreiben und uns beim Programmieren Schreibaufwand ersparen (möchten wir die defaults nutzen, müssen wir immerhin nichts zusätzliches in der Funktion angeben). Allerdings können wir hier auch immer die anderen Optionen nutzen, wir müssen dies nur in unserem Befehl angeben:\n\nmean(data$Alter, na.rm=T)\n\nHier haben wir auch bereits eine weitere Funktionalität von R kennengelernt, nämlich die Operationalisierung einzelner Parameter über T (TRUE) und F (FALSE). Doch woher weiß ich nun als neuer Nutzer, welche Optionen mit bei einzelnen Befehlen zur Verfügung stehen?\nHier hilf ein Blick in die Hilfeseite, welche wir beispielsweise für den Mittelwert mit dem Befehl ?mean() aufrufen können. Für jede Funktion stehen - wie oben bereits erwähnt - Hilfeseiten zur Verfügung. Diese beinhalten zunächst eine kurze Beschreibung Description, anschließend einen Überblick zur Nutzung Usage sowie dem Default des Befehles. Anschließend finden sich die Arguments, dies sind die Möglichkeiten, wie wir die Funktion nutzen können und welche zusätzlichen Optionen zur Verfügung stehen. Oftmals finden sich zudem weitere Erklärungen und Beispiele der Nutzung.\n\n5.1 Kurz-Exkurs: das tidyverse\nIn R selbst findet sich eine Vielzahl von Befehlen. Zusätzlich wird R von den Nutzern immer weiter entwickelt und es kommen neue Funktionen in Form von Paketen hinzu. Eines der meist genutzten Pakete(-universen) stellt dabei das tidyverse dar. Mit den Befehlen und Funktionen dieses Paketes kommt eine etwas andere Programmiersprache, welche uns jedoch die Arbeit mit R erleichtert. Gerade auch bei Fragen des Datenmanagementes ist das tidyverse hilfreich, denn hier kommt ein zweiter Fun-Fact über R:💡 R kann nicht nur manchmal etwas dumm sein (wir erinnern uns, es denkt nicht mit), es ist auch recht vergesslich. Wir haben oben bereits gelernt, dass wir in Befehlen immer den Datensatz und die Variable spezifizieren müssen. Dies stellt kein Problem bei einzelnen Befehlen dar, ist jedoch bei einer Vielzahl von Befehlen etwas nervig. Hier kommt uns die tidyverse Logik zu Nutze, in welcher einmal zu Beginn des Dokumentes der Datensatz spezifziert wird und anschließend alle weiteren Schritte durch eine Pipe %&gt;% verbunden werden. Die Pipe (Shortcut Windows: Ctrl + Shift + M; MAC: Cmd + Shift + M) bedeutet so viel wie “und dann”. Also im Prinzip sagen wir R, nehme diesen Datensatz und dann mache die folgenden Dinge, wobei wir so viele Schritte wie wir möchten jeweils mit Pipes verbinden können. Wir nutzen in unseren Skripten hauptsächlich die tidyverse-Logik, erklären diese daher grundlegender in Kapitel 3.1, wenn wir uns mit den ersten Schritten des Datenmanagements beschäftigen."
  },
  {
    "objectID": "Skript_1.1.html#pakete",
    "href": "Skript_1.1.html#pakete",
    "title": "Einführung in R und RStudio",
    "section": "6 Pakete",
    "text": "6 Pakete\nR-Pakete sind Erweiterungen, die Funktionen, Daten, Code und dessen Dokumentation enthalten und uns damit unsere Arbeit deutlich erleichtern. Diese erweiteren die Funktionen, die bereits in der Standardausführung von R (bekannt als base R) gegeben sind.\n\n6.1 Die Installation von Paketen\nPakete können wahlweise R-intern über CRAN (das steht für Comprehensive R Archive Network und ist das zentrale Software-Repository) oder direkt von GitHub (für sehr neue Pakete, welche noch nicht auf CRAN sind) installiert werden. Im Normalfall installieren wir jedoch direkt von CRAN, da hier eine Vielzahl von Paketen und Funktionen vorhanden sind und die Installation sehr simpel ist.\nWenn wir ein R-Paket von CRAN installieren möchten nutzen wir die Funktion install.packages(), in deren Klammer wir in Anführungszeichen den Namen des zu installierenden Pakets setzen. Für das Paket tidyverse wäre der Befehl wie folgt:\n\ninstall.packages(\"tidyverse\")\n\nBeim Ausführen des Codes gibt R in der Konsole Auskunft über die verschiedenen Installationsschritte sowie über den (Mis-)Erfolg der Installation. Alternativ könnt ihr auch über das RStudio Interface Pakete installieren. Hierfür geht ihr in der rechten unteren Ecke auf den Reiter Packages und wählt den Buttion Install aus. In dem sich öffnenen Fenster gebt ihr den Namen des von euch gewünschten Pakets ein und bestätigt die Anwendung wiederum mit Install.\n\n\n\nScreenshot RStudio\n\n\nEinige der Pakete, die wir im Rahmen des Kurses verwenden sind bereits in der R Cloud vorinstalliert und müssen nicht mehr von euch eigenhändig installiert werden. Grundsätzlich ist es jedoch immer ratsam, einmal zu checken, ob die Pakete in der aktuellen Version installiert sind, da ansonsten die Funktionen der Pakete nicht funktionieren können.\n\n\n6.2 Das Laden von Paketen\nIm vorherigen Schritt haben wir bereits die von uns benötigten Pakete installiert, jetzt müssen wir R nur noch sagen, dass wir diese Pakete für die aktuelle Session nutzen möchten. Ihr könnt euch das so vorstellen, dass ihr die benötigten Pakete aktiv schaltet. Dies geschieht mit dem library Befehl (Achtung, hier benötigen wir im Gegensatz zum install.packages Befehl keine Anführungszeichen):\n\nlibrary(tidyverse)\n\nWir nutzen in diesem Kurs wiederum pacman, um automatisch mehrere Pakete installieren und laden zu können. Dafür installieren und laden wir einmalig das Paket pacman und können anschließend mit der Funktion p_load die von euch in der Klammer angegebenen Pakete installieren und laden:\n\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\np_load(tidyverse, ggplot2, haven, psych, psy, nFactors, htmlTable) \n\nAnsonsten habt ihr noch die Möglichkeit die Pakete händisch über das R Studio Interface zu aktiveren, was ich persönlich ab einer gewissen Anzahl von benötigten Paketen eher nervig finde. Nichtsdestotrotz könnt ihr hierfür in der rechten unteren Ecke des Interfaces auf den Reiter Packages gehen und in der unteren Liste das gewünschte Paket durch das Setzen eines Hakens auswählen.\n\n\n\nScreenshot RStudio\n\n\n\n\n\n\n\n\nWeiterführende Literatur\n\n\n\n\n📖 Gehrau, V., Maubach, K., & Fujarski, S. (2022). Einfache Datenauswertung mit R. Link"
  },
  {
    "objectID": "Skript_1.1.html#was-sind-r-und-rstudio",
    "href": "Skript_1.1.html#was-sind-r-und-rstudio",
    "title": "Einführung in R und RStudio",
    "section": "1 Was sind R und RStudio?",
    "text": "1 Was sind R und RStudio?\nR ist eine freie Programmiersprache die auf allen gängigen Betriebssystemen läuft. In seiner Grundfunktion ist R zunächst eine Konsole, in welcher wir zeilenweise Code eingeben und ausführen können. Dabei kann es sich um einfache Rechnungen oder auch komplexe Modelle handeln. Die meisten arbeiten dabei nicht mit R als solches sondern nutzen verschiedene Umgebungen, etwa RStudio oder Jupyter Hub um mit R zu arbeiten.\nRStudio ist eine Erweiterung von R und bietete den Nutzer*innen eine benutzerfreundlichere Programmoberfläche. So ermöglicht RStudio einen direkten Überblick über die geladenen Pakete, Datensätze und im Arbeitsverzeichnis gespeicherten Dateien. Zudem ermöglicht RStudio die Arbeit mit Skripten, Markdown und Quarto Dokumenten.\n\n1.1 Installation von R und RStudio\nMöchtet ihr das Programm lokal auf eurem Rechner nutzen, müsst ihr zunächst die Programme R, RStudio und gegebenfalls RTools installieren. Alle Programme sind kostenlos online verfügbar und lassen sich auf allen gängigen Betriebssystemen installieren. Installiert zunächst R und anschließend RStudio und achtet darauf regelmäßig auf die aktuelle Version zu aktualisieren.\nInnerhalb unseres Kurse arbeiten wir mit R und RStudio, nutzen die Programme allerdings in der Umgebung von Jupyter."
  },
  {
    "objectID": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "href": "Skript_1.3.html#getting-started-mit-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Getting Started mit Markdown und Quarto",
    "text": "3 Getting Started mit Markdown und Quarto\n\n3.1 Installation\n\n\n3.2 Dokument laden\n\n\n3.3 Neues Dokument anlegen"
  },
  {
    "objectID": "Skript_1.3.html#r-markdown-und-quarto",
    "href": "Skript_1.3.html#r-markdown-und-quarto",
    "title": "Die Logik von Markdown und Quarto",
    "section": "1 R Markdown und Quarto",
    "text": "1 R Markdown und Quarto\nIn diesem Kurs verwenden wir R Markdown bzw. R Quarto Dokumente. Diese haben den Vorteil, dass wir innerhalb eines Dokumentes Codeteile (sogenannte Code Chunks) und Text kombinieren können. Dies erlaubt die Dokumentation und Reproduzierbarkeit statistischer Auswertungen.\nBei Quarto handelt es sich im Prinzip um die neuere Variante von RMarkdown Dokumenten. Diese beinhalten alle Funktionalitäten von Markdown-Dokumenten (sind genauso aufgebaut und lassen sich normal rendern), aber bieten zusätzlich die Möglichkeit weitere Programmiersprachen (wie Python, Julia und Javascript) und interaktive Elemente (Widgets und Shiny-Anwendungen). Markdown-Dokumente weisen die Endung .rmd auf wohingegen Quarto-Dokumente den Endung .qmd haben. Beide Dokumenttypen können für unseren Kurszweck gleichwertig genutzt werden. Lediglich bei der Arbeit mit interaktiven, multimedialen oder mehrsprachigen Dokumenten ist Quarto besser geeignet als Markdown."
  },
  {
    "objectID": "Skript_1.3.html#installation-der-programme",
    "href": "Skript_1.3.html#installation-der-programme",
    "title": "Die Logik von Markdown und Quarto",
    "section": "2 Installation der Programme",
    "text": "2 Installation der Programme\nMarkdown kann innerhalb von R mit dem Befehl install.packages(\"Rmarkdown\") installiert und anschließend geladen werden. Da es sich bei Quarto nicht um ein Package, sondern ein eigenes Interface handelt, muss das Programm extern heruntergeladen und auf dem Rechner installiert werden."
  },
  {
    "objectID": "Skript_1.3.html#bestehende-dokument-laden",
    "href": "Skript_1.3.html#bestehende-dokument-laden",
    "title": "Die Logik von Markdown und Quarto",
    "section": "3 Bestehende Dokument laden",
    "text": "3 Bestehende Dokument laden\nZum Laden eines Quarto- oder Markdown Dokumentes könnt ihr dieses einfach auf eurem Rechner doppelklicken. Wahlweise könnt ihr auch über File -&gt; Open File (oder Strg + O) innerhalb von RStudio ein Dokument öffnen."
  },
  {
    "objectID": "Skript_1.3.html#neue-dokumente-anlegen",
    "href": "Skript_1.3.html#neue-dokumente-anlegen",
    "title": "Die Logik von Markdown und Quarto",
    "section": "4 Neue Dokumente anlegen",
    "text": "4 Neue Dokumente anlegen\nUm ein neues Dokument anzulegen, könnt ihr einfach über File -&gt; New File das gewünschte Dokument anlegen. Sowohl bei Markdown, als auch bei Quarto öffnet sich dann das Folgende Fenster:\n\n\n\nNeues Dokument anlegen\n\n\nDort könnt ihr euer Dokument benennen (Title), die Autoren festlegen (Author) sowie das Output-Format (HTML, PDF oder WORD) festlegen. Zudem könnt ihr bei Quarto angeben, wie ist das Dokument gerendert werden soll (Knitr vs. Jupyter). Hier könnt ihr die Auswahl auf Knitr belassen. Zuletzt könnt ihr festlegen, ob ihr den visual markdown editor oder den source editor nutzen möchtet. All diese Punkte könnt ihr jedoch auch noch nachfolgend im YAML-Header oder im Menü) ändern."
  },
  {
    "objectID": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "href": "Skript_1.3.html#überblick-über-die-dokumentkomponenten",
    "title": "Die Logik von Markdown und Quarto",
    "section": "6 Überblick über die Dokumentkomponenten",
    "text": "6 Überblick über die Dokumentkomponenten\nSowohl Markdown, als auch Quarto-Dokumente bestehen aus drei Bestandteilen: dem YAML-Header, Textbereichen und Codebereichen.\n\n6.1 YAML-Header\nInnerhalb des YAML Headers, welcher jeweils von --- umgeben ist, legen wir die Dokumentstruktur fest.\n\n\n\nYAML Header in einem Quarto Dokument\n\n\nDies beinhaltet beispielsweise den Titel des Dokumentes title:, die Autoren author, sowie Spezifikationen zur Dokumentstruktur, wie beispielsweise das Outputformat format: oder auch in Quarto Spezifikationen zum Umgang mit den Codechunks execute: echo : true auf Gesamtdokumentebene.\n\n\n6.2 Text\nIn Markdown und Quarto-Dokumenten können wir Text einbinden und diesen beliebig formatieren. Dazu können wir wahlweise die Source-Variante oder die Visual-Variante nutzen. In der Source-Variante variieren wir Text mittels Syntax. Typische Syntaxbefehle sind:\n\n*kursiv*: jeweils einen Stern vor und nach einem Wort um dieses kursiv zu schreiben\n**fett**: jeweils zwei Sterne vor und nach einem Wort um dieses fett zu schreiben\n#: Rauten für Überschriften, wobei eine Raute die erste Überschrift signalisiert, zwei Rauten die zweite usw.\n![Bildunterschrift](Link des Bildes): um Bilder einzufügen\n[Linktext](url): Um Links einzufügen\n\nMöchten wir übrigens die oben genutzten Symbole im Text nutzen, so können wir mit einem  vor dem jeweiligen Symbol die Formatierung umgehen.\nWahlweise können wir auch den Visual-Modus nutzen, indem wir oben in der Dokumentleiste von Sourceauf Visual umstellen. In diesem Modus erhalten wir ein Word-ähnliches Interface und können Formatoptionen durch Klicken auf die jeweilige Formatierung umsetzen:\n\n\n\nFormatierungsoptionen im Visual Modus\n\n\n\n\n6.3 Code Chunks\nInnerhalb von Markdown und Quarto können wir Codebefehle direkt in unser Dokument innerhalb von sogenannten Codechunks integrieren. Hier können wir alle Arten von Code schreiben sowie diese mit Hilfe von # direkt kommentieren (alles hinter einer Raute wird dabei nicht ausgeführt). Codechunks beginnen mit drei Backticks und einem r in geschweiften Klammern und enden wieder mit drei Backticks:\n\n```{r}\n# Dies ist ein Code Chunk\n```\n\nUm die Codechunks zu erzeugen können wir einfach auf Code -&gt; Insert Codechunk gehen, auf das +C-Symbol in der Dokumentmenüleiste oder den Shortcut Alt + Strg + I nutzen. Innerhalb der Chunks können wir Code schreiben und ausführen. Dies geschieht für eine einzelne Codezeile mit dem Shortcut Strg + Enter und für den gesamten Codechunk mit dem Shortcut Strg + Shift + Enter. Wahlweise könnt ihr auch den kleinen grünen Pfeil in der rechten oberen Ecke des Chunks, Code -&gt; Run Selected Lines oder den Punkt Run in der Quarto-Dokumentleiste auswählen.\nZusätzlich können wir hinter dem {r} angeben, wie R mit dem Code des Chunks umgehen soll. Wir können beispielsweise auswählen, ob R den Code ausführen soll (eval = T/F) ob der Codebereich in unserem Enddokument aufgeführt sein soll (echo = T) oder wir lediglich die Ergebnisse angezeigt wollen (echo = F) oder ob wir beispielsweise Warnungen (warnings = T/F) oder Messages (message = T/F) in unserem Output-Dokument wünschen:\n\n```{r, echo = T}\n# Dies ist ein Code Chunk\n```\n\nWahlweise können wir diese Optionen auch für das Gesamtdokument im YAML-Header festlegen. Dafür nutzen wir den Zusatz execute: und geben anschließend alle unsere Dokumentoptionen (für einen Überblick siehe hier) an.\n\n\n\nCodeoptionen im Quarto Header\n\n\nAchtung: wir nutzen hier : statt = und schreiben true und false statt TRUE/T und FALSE/F."
  },
  {
    "objectID": "Skript_1.3.html#übergreifende-menüpunkte",
    "href": "Skript_1.3.html#übergreifende-menüpunkte",
    "title": "Die Logik von Markdown und Quarto",
    "section": "5 Übergreifende Menüpunkte",
    "text": "5 Übergreifende Menüpunkte\nUnterhalb der allgemeinen RStudio-Menüleiste findet ihr eine eigene Markdown und Quarto-Dokumentleiste:\n\n\n\nDokumentmenüleistet\n\n\nHier könnt ihr euer Dokument speichern (Diskettensymbol), in eurem Dokument suchen und ersetzen (Lupensymbol) oder euer Dokument Rendern (Quarto) oder Knitten (Markdown). Hiermit rendert ihr euer Dokument von R in euer gewünschtes Outputformat (Word, Pdf, oder Html). Zusätzlich könnt ihr mit dem Zahnrad Optionen für den Umgang mit dem gerenderten Dokument und den Codechunks festlegen. Zusätzlich könnt ihr hier neue Codechunks anlegen (+C-Symbol auf der rechten Seite) oder die bestehenden Code-Chunks ausführen (Run-Symbol)"
  },
  {
    "objectID": "Skript_6.4.html#die-varianzanalyse",
    "href": "Skript_6.4.html#die-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "",
    "text": "Die Varianzanalyse weitet den Grundgedanken des t-Tests aus, indem sie den simultanen Vergleich von Gruppenmittelwerten über mehr als zwei Gruppen erlaubt. Die einfaktorielle Varianzanalyse definiert die Gruppen dabei anhand eines Faktors (einer unabhängigen Variable), die mehrfaktorielle Varianzanalyse erlaubt mehrere unabhängige kategoriale Variablen im Modell, ist aber nicht mit der multivariaten Varianzanalyse (MANOVA) zu verwechseln, die auch mehrere metrische abhängige Variablen gleichzeitig zulässt. Wenn die Gruppierungsvariable (also die unabhängige Faktor-Variable) mehr als zwei Gruppen unterscheidet, müssen nach der Anova Post-Hoc Tests durchgeführt werden. Denn sollte die Varianzanalyse insgesamt signifikante Werte liefern, wurde zwar festgestellt, dass es überzufällige Unterschiede zwischen den durch die Faktorstufen definierten Gruppen gibt, bei drei und mehr Faktorstufen bleibt aber noch unklar, auf welchen Gruppenunterschieden dieses Ergebnis beruht. Post hoc-Tests liefern spezifischere Informationen dazu, welche Gruppenmittelwerte signifikant voneinander abweichen.\n\n\nIn R ist es zwingend notwendig, neben dem Programm als solches auch die Daten zu laden. Nachfolgend findet sich der load-Befehl. Dieser lädt die R-Daten. Dafür ist es wichtig, das diese im selben Ordner wie dieses Skript gespeichert sind. Zudem müssen wir die Pakete laden. Hier nutzen wir den Paketmanagaer pacman. Diese muss einmal installiert und geladen werden und anschließend können mit dem Befehl p_load alle benötigten Pakete gleichzeitig installiert und geladen werden.\n\n#Pakete laden\nif(!require(\"pacman\")) {install.packages(\"pacman\");library(pacman)}\n\nLade nötiges Paket: pacman\n\np_load(knitr,car, tidyverse, ggplot2, DescTools, dplyr,afex, emmeans, PMCMRplus)\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'rootSolve', 'lmom', 'expm', 'Exact', 'gld'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'rootSolve' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'lmom' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'expm' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Exact' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gld' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'DescTools' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nDescTools installed\n\n\nInstalliere Paket nach 'C:/Users/Katharina Maubach/AppData/Local/R/win-library/4.2'\n(da 'lib' nicht spezifiziert)\n\n\ninstalliere auch Abhängigkeiten 'multcompView', 'gmp', 'Rmpfr', 'kSamples', 'BWStest'\n\n\nWarning: kann nicht auf den Index für das Repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2 zugreifen:\n  kann URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.2/PACKAGES' nicht öffnen\n\n\nPaket 'multcompView' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'gmp' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'Rmpfr' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'kSamples' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'BWStest' erfolgreich ausgepackt und MD5 Summen abgeglichen\nPaket 'PMCMRplus' erfolgreich ausgepackt und MD5 Summen abgeglichen\n\nDie heruntergeladenen Binärpakete sind in \n    C:\\Users\\Katharina Maubach\\AppData\\Local\\Temp\\RtmpQLcYin\\downloaded_packages\n\n\n\nPMCMRplus installed\n\n#Daten laden\ndaten &lt;- read_rds(\"Datensatz/ESS8_vier_laender.rds\")\n\n#Visualisierungshintergrund festlegen\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#voraussetzungsprüfung-für-einfaktorielle-und-mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse",
    "text": "2 Voraussetzungsprüfung für einfaktorielle und mehrfaktorielle Varianzanalyse\nDie Varianzanalyse ist ein statistisches Verfahren, dass bestimmte Voraussetzungen bezüglich der abhängigen und unabhängigen Variablen voraussetzt. Dies sind im Überblick:\n\nDatenniveau der AV (metrisch) und UV (Faktor)\nNormalverteilung der abhängiven Variablen\nHomogenität der Fehlervarianzen der unabhängigen Variablen\n\nDie meisten Voraussetzungen, die für die Durchführung und Interpretation einer Varianzanalyse erfüllt sein müssen, lassen sich bereits im Vorfeld der eigentlichen Analyse überprüfen. So kann das Messniveau unmitttelbar festgestellt werden. Es gilt für die abhängige (y-)Variable stets, dass sie intervallskaliert sein muss, in R also als ein Vektor vorliegen muss. Die Faktoren bzw. unabhängige(n) (x-) Variable(n) hingegen müssen zwingend nominalskaliert sein und in R demnach als Faktoren vorliegen.\n\n2.1 Überprüfung der Normalverteilung\nEine weitere wichtige, leicht vorab zu prüfende Bedingung, die für eine Varianzanalyse erfüllt sein muss, ist die Normalverteilung der abhängigen Variable. Diese können wir graphisch überprüfen:\nIn R nutzen wir das Paket ggplot2 von Wickham et al. um ein Histogramm auszugeben. Zunächst müssen wir hier das Paket ggplot2 mit dem Befehl ggplot() aufrufen. Anschließend spezifizieren wir innerhalb der Klammer unseren Datensatz (hier Varianzanalyse) und unter aes unsere Variable (hier Angst). Mit einem Plus-Zeichen legen wir die nächste Ebene fest und geben hier mit geom_histogram an, dass wir ein Histogramm wünschen. Die Spezifizierungen innerhalb der Klammer geben an, dass dieses auf den Zahlen unseres Datensatzes beruhen soll (aes(y = ..count..)), wir die Außenumrandung schwarz color = black und die Füllfarbe grau wünschen (fill = grey). Diese Spezifikationen sind optional, sorgen jedoch für ein schöneres Aussehen unserer Grafik. Zuletzt legen wir innerhalb der Klammer die Breite unserer Säulen fest. Mit binwidth = 1 verweisen wir hier auf eine Breite der Balken 1. Nun haben wir ein vollständiges Histogramm. Wir können jedoch für ein verschönertes Aussehen unseres Graphen mit labs zusätzlich die Achsen beschriften.\n\n#Histogramm ausgeben\nggplot(daten, aes(happy)) +\n  geom_histogram(aes(y = ..count..), \n                 color = \"black\", fill = \"grey\", \n                 binwidth = 1) +\n  labs(x = \"Zufriedenheit\", \n         y = \"\")\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Removed 15 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWir sehen an der Grafik, dass die Variable Zufriedenheit rechtssteil ist, also die Teilnehmer der Befragung eher eine höhere Zufriedenheit angegeben haben.\nZusätzliche Gewissheit beuüglich des Vorliegens der Normalverteilung bietet der Kolmogorov-Smirnov-Test oder der Shapiro-Wilk-Test (bei diesem wird jedoch eine Stichprobengröße zwischen 3 und 5000 vorausgesetzt, welche wir hier überschreiten, daher rechnen wir nur den Kolmogorov-Smirnov-Test). In R erhalten wir diese Tests mit dem Befehl LillieTest() aus dem Paket DescTools() bzw. shapiro.test(). Beide Tests testen auf Abweichung von der Normalverteilung, demnach sollte diese nicht signifikant ausfallen (da ein signifikanter Test aussagt, dass eine Abweichung von der Normalverteilung besteht, was wir nicht möchten).\n\n#Lilliefors Kolmogorov-Smirnov-Test\nLillieTest(daten$happy)\n\n\n    Lilliefors (Kolmogorov-Smirnov) normality test\n\ndata:  daten$happy\nD = 0.2124, p-value &lt; 2.2e-16\n\n#Shapiro-Wilk Test\n#shapiro.test(daten$happy)\n\nIm vorliegenden Beispiel ist der Test signifikant. Allerdings reagieren beide Tests insbesondere bei großen Stichproben sehr sensibel, sodass bereits leichte Abweichungen von der Normalverteilung (etwa durch Ausreißer) die Tests signifikant werden lassen; in diesen Fällen ist es sinnvoll eher auf die graphische Überprüfung (siehe oben) zu schauen. Sowohl der Kolmogorov-Smirnov-Test als auch die Grafik lassen jedoch nicht auf eine perfekte Normalverteilung unserer abhängigen Variablen schließen. In diesem Fall kann die Varianzanalyse dennoch gerechnet werden, da solange die Verteilung nicht extrem steil ist, das Verfahren einigermaßen robust ist. Bei einer sehr starken Verletzung der Normalverteilung müsste ggf. auf nicht parametrische Tests ausgewichen werden."
  },
  {
    "objectID": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "href": "Skript_6.4.html#überprüfung-der-homogenität-der-fehlervarianzen",
    "title": "Die Varianzanalyse",
    "section": "3 Überprüfung der Homogenität der Fehlervarianzen",
    "text": "3 Überprüfung der Homogenität der Fehlervarianzen\nDie letzte Voraussetzung, die für eine Varianzanalyse erfüllt sein muss, ist die Homogenität der Fehlervarianzen. Um diese zu testen, nutzen wir den Levene-Test auf Varianzhomogenität. Hierfür nutzen wir die Funktion leveneTest()aus dem Paket car. Innerhalb der Klammer müssen wir zunächst unsere abhängige Variable angeben. Danach folgt eine Tilde (~). Im Anschluss müssen wir unsere unabhängige(n) Variablen angeben. Die Tilde sagt quasi, dass unsere abhängige Variable durch unsere unabhängigen Variablen bestimmt wird. Haben wir nur eine abhängige Variable, so geben wir diese an. Haben wir mehrere so können wir diese mit einem * verbinden. Hier wird der Test für beide Variablen sowie den Interaktionsterm ausgeben.\n\n#Levene-Test für einfaktorielle Varianzanalyse\ndaten %&gt;% \n  leveneTest(happy~cntry, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    3  15.242 6.883e-10 ***\n      8413                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Levene-Test für mehrfaktorielle Varianzanalyse ausgeben\ndaten %&gt;% \n  leveneTest(happy ~ cntry*gndr, data = ., center = mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n        Df F value    Pr(&gt;F)    \ngroup    7  7.1507 1.497e-08 ***\n      8409                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWenn der Levene-Test statistisch signifikant ausfällt, sollte die Hypothese homogener Varianzen abgelehnt werden. Falls der Test wie im vorliegenden Fall signifikant ausfällt (da der Pr(&gt;F-Wert) kleiner als 0.05 ist) wurde die Voraussetzung der Homogenität der Fehlervarianzen verletzt. In einem solchen Fall können wir wahlweise auf nicht-parametrische Tests ausweichen, oder die Varianzanalyse dennoch berechnen, wenn wahlweise die deskriptiven Kennwerte keine allzu große Streuung aufweisen, oder wir einen alternativen Posthoc-Test (etwa Tamhame T2) wählen."
  },
  {
    "objectID": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "href": "Skript_6.4.html#einfaktorielle-varianzanalyse-ohne-messwiederholung",
    "title": "Die Varianzanalyse",
    "section": "4 Einfaktorielle Varianzanalyse (ohne Messwiederholung)",
    "text": "4 Einfaktorielle Varianzanalyse (ohne Messwiederholung)\nNachdem wir die Voraussetzungen geprüft haben, schauen wir uns die einfaktorielle Varianzanalyse an. Im vorliegenden Beispiel möchten wir gerne überprüfen, inwiefern sich die Nationalität bzw. Länderzugehörigkeit (Variable cntry; Deutschland, Schweden, Frankreich und Großbritannien) auf die Lebenszufriedenheit (Variable happy) auswirkt.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\ndaten %&gt;% \n  group_by(cntry) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = sd(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nM\nSD\n\n\n\n\nDE\n7.76\n1.73\n\n\nFR\n7.21\n1.79\n\n\nGB\n7.64\n1.81\n\n\nSE\n7.85\n1.63\n\n\n\n\n\nAnhand der deskriptiven Statistiken sehen wir, dass die Zufriedenheit in Frankreich am geringsten ist und in Schweden am höchsten. Zudem erkennen wir, dass die Standardabweichungen der Ländern sich nicht drastisch unterscheiden, wir demnach trotz der Verletzung der Varianzhomogenität die ANOVA rechnen können.\nAnschließend können wir die einfaktorielle ANOVA berechnen.Dafür nutzen wir die Funktion aov_car aus dem afex-Package, in welcher wir zunächst die abhängige Variable (happy) angeben müssen und nach einer Tilde die unabhängige Variabel (cntry) Zudem müssen wir den Zusatz + Error() nutzen un in der Klammer die Fallid (hier idno) angeben. Da die Variable idno doppelte Fälle hatte müssen wir in dem vorliegenden Beispiel diese ausschließen, da andernfalls der Befehl nicht funktioniert. Dies geschieht mit dem Befehl distinct welcher aus der Variablen idno alle doppelten Fälle ausschließt. Der Zusatz .keep_all = T bedeutet, dass wir alle Variablen des Datensatzes behalten wollen und nur die doppelten Fälle ausgeschlossen werden sollen. Alles speichern wir als Objekt fit welches wir anschließend mit print aufrufen.\n\n#Einfaktorielle ANOVA\nfit &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = T) %&gt;% \naov_car(happy ~ cntry + Error(idno), data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry  .\n# 530   703    SE NA\n# 602   785    FR NA\n# 719   931    SE NA\n# 959  1248    FR NA\n# 1800 2534    SE NA\n# 2280 3277    SE NA\n\n\nContrasts set to contr.sum for the following variables: cntry\n\nprint(fit)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n  Effect      df  MSE         F  pes p.value\n1  cntry 3, 7642 3.13 45.96 *** .018   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nAls Output erhalten wir eine Tabelle mit den folgenden Parametern:\n\nEffect: unabhängige Variable\ndf: Freiheitsgrade (degrees of freedom)\nMSE: Fehlervarianz, mittlere quadratische Abweichung (mean squared errors)\nF: F-Werte\npes: parties Eta-Quadrat (partial eta-quared)\np.value: Signifikanz\n\nEffect zeigt die unabhängige Variable des Modells, in diesem Fall die Ländervariable cntry. Die drei bedeutet, dass hier insgesamt eine Gruppe mit 3 anderen (= vier Ausprägungen) verglichen wurde. mean-squared errors\nÜber die Werte der zweiten und vierten Spalte, die Freiheitsgrade (df) und die F-Werte (F) ließe sich, wenn man es wollte, der emprische F-Wert in der F-Tabelle, mit dem kritischen Wert (theoretischen F-Wert) vergleichen, um zu prüfen, ob die Nullhypothese, dass keine Unterschiede zwischen den durch die jeweilige Variable definierten Gruppen bestehen, verworfen werden darf. Diesen Aufwand kann man sich allerdings sparen, da R in der Spalte p.value die umgekehrte Aussage macht, dass die Nullhypothese mit der dort berichteten Fehlerwahrscheinlichkeit verworfen werden kann. Beim per Konvention in den Sozialwissenschaften mindestens geltenden Konfidenz-Niveau von 95%, dürfen im Umkehrschluss also für alle Modellterme signifikante Unterschiede in der Grundgesamtheit angenommen werden, die hier einen Wert &lt;.05 aufweisen. Die Spalte ist damit die wichtigste der gesamten Tabelle! Im vorliegenden Datenbeispiel ist demnach ein signifikanter Unterschied der Länder im Bezug auf die Lebenszufriedenheit ersichtlich.\nDie mittlere quadratische Abweichung (MSE) oder Fehlervarianz ist die Summe der Abweichungsquadrate aller Werte vom jeweiligen Gruppenmittelwert. Berechnet wird diese durch die Quadratsummer der Fehlerresiduen geteilt durch die Freiheitsgrade. Sie gibt damit die Varianz innerhalb der einzelnen Gruppen (=nicht erklärte Varianz) wieder.\nDie Spalte pes steht für das partielle Eta-Quadrat und gibt die Erklärungskraft der einzelnen Faktoren im Hinblick auf die anhängige Variable an – partiell ist das Eta2, da es um die Einflüsse der übrigen Modellgrößen bereinigt ist (für unifaktorielle Analysen wie im vorliegenden Fall ist dies nicht relevant, allerdings für die multifaktorielle ANOVA). Im Datenbeispiel hat demnach die Länderzugehörigkeit (und damit auch unser Gesamtmodell) eine Erklärkraft von 1.8 Prozent. Allerdings wissen wir lediglich, dass sich unsere Gruppen signifikant unterscheiden, nicht jedoch, ob sich alle Gruppen unterscheiden, oder lediglich einzelne. Daher benötigen wir die Posthoc-Tests."
  },
  {
    "objectID": "Skript_6.4.html#posthoctests",
    "href": "Skript_6.4.html#posthoctests",
    "title": "Die Varianzanalyse",
    "section": "5 PostHocTests",
    "text": "5 PostHocTests\nZuletzt müssen wir die Posthoc-Tests berechnen, welche uns Aufschluss darüber geben, welche unserer Gruppen sich unterscheiden. Es gibt verschiedene Posthoc-Tests. Grundsätzlich ist der Tukex-Post-Hoc Test zu empfehlen, welche wir über die Funktion emmeansaus dem emmeans-Package aufrufen. Bei fehlender Varianzhomogenität können wir zudem den tamhane-T2 Test nutzen. Dieser basiert allerdings auf einem aov-Objekt, daher geben wir hier mit aov die ANOVA erneut aus. Innerhalb von emmeans können wir hingegen einfach auf unser zuvor spezifiziertes Modell fit verweisen, müssen allerdings noch mit specs= angeben, auf Basis welcher Variablen der Gruppenvergleich durchgeführt werden soll.\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit, specs = \"cntry\") %&gt;% \n  pairs()\n\n contrast estimate     SE   df t.ratio p.value\n DE - FR     0.543 0.0511 7642  10.618  &lt;.0001\n DE - GB     0.120 0.0519 7642   2.312  0.0955\n DE - SE    -0.102 0.0718 7642  -1.423  0.4847\n FR - GB    -0.423 0.0558 7642  -7.576  &lt;.0001\n FR - SE    -0.645 0.0747 7642  -8.639  &lt;.0001\n GB - SE    -0.222 0.0752 7642  -2.955  0.0166\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n#Tamhame T2 Test\ndaten %&gt;% \naov(happy ~ cntry, data = .) %&gt;% \n  tamhaneT2Test(.)\n\n\n    Pairwise comparisons using Tamhane's T2-test for unequal variances\n\n\ndata: happy by cntry\n\n\n   DE     FR     GB    \nFR &lt;2e-16 -      -     \nGB 0.1233 7e-13  -     \nSE 0.3992 &lt;2e-16 0.0017\n\n\n\nP value adjustment method: T2 (Sidak)\n\n\nalternative hypothesis: two.sided\n\n\nHier interessiert uns jeweils der p-value (für den Tamhame T2 Test wird uns nur dieser angezeigt). Werte unter .05 bedeuten, dass zwischen diesen Gruppen ein signifikanter Mittelwertunterschied besteht. In unserem Beispiel sehen wir Unterschiede zwischen Frankreich und allen weiteren Ländern, sowie einen Unterschied zwischen Großbritannien und Schweden. Wenn wir auf die deskriptiven Statistiken schauen, sehen wir demnach, dass in Frankreich die Lebenszufriedenheit signifikant geringer als in Deutschland, Schweden und Großbritannien ist, sowie die Lebenszufriedenheit in Großbritannien signifikant geringer als in Schweden ausfällt."
  },
  {
    "objectID": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "href": "Skript_6.4.html#exkurs-kruskal-wallis-test",
    "title": "Die Varianzanalyse",
    "section": "6 Exkurs: Kruskal Wallis Test",
    "text": "6 Exkurs: Kruskal Wallis Test"
  },
  {
    "objectID": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "href": "Skript_6.4.html#mehrfaktorielle-varianzanalyse",
    "title": "Die Varianzanalyse",
    "section": "7 Mehrfaktorielle Varianzanalyse",
    "text": "7 Mehrfaktorielle Varianzanalyse\nIn der mehrfaktoriellen Varianzanalyse können wir unser Modell aus der einfachen Varianzananlyse erweitern. In diesem Beispiel nutzen wir neben der Ländervariablen (cntry) zusätzlich die Variable Geschlecht (gndr) um die Lebenszufriedenheit (happy) vorherzusagen.\nZunächst möchten wir uns anhand von deskriptiven Statistiken einen Überblick über unsere Daten verschaffen. Dafür nutzen wir die group_by-Funktion in Kombination mit der summarise-Funktion. Im nachfolgenden Schritt haben wir mit Hilfe des Befehls kable() aus dem Paket knitr das Aussehen unserer Tabelle verschönert. Dieser Schritt ist optional.\n\n#Deskriptive Statistiken ausgeben\ndaten %&gt;% \n  group_by(cntry, gndr) %&gt;% \n  summarise(Mittelwert = mean(happy, na.rm = T), \n            Standardabweichung = SD(happy, na.rm = T)) %&gt;% \n  kable(digits = 2, col.names = c(\"Land\", \"Geschlecht\", \"M\", \"SD\"), caption = \"Descriptives Lebenszufriedenheit\")\n\n`summarise()` has grouped output by 'cntry'. You can override using the\n`.groups` argument.\n\n\n\nDescriptives Lebenszufriedenheit\n\n\nLand\nGeschlecht\nM\nSD\n\n\n\n\nDE\nMale\n7.71\n1.72\n\n\nDE\nFemale\n7.80\n1.75\n\n\nFR\nMale\n7.30\n1.73\n\n\nFR\nFemale\n7.14\n1.83\n\n\nGB\nMale\n7.61\n1.78\n\n\nGB\nFemale\n7.66\n1.84\n\n\nSE\nMale\n7.85\n1.60\n\n\nSE\nFemale\n7.84\n1.66\n\n\nSE\nNo answer\nNaN\nNA\n\n\n\n\n\nZunächst können wir unsere deskriptiven Statistiken betrachten. Hier interessieren uns insbesondere die Mittelwerte für die einzelnen Gruppen. Diese Mittelwerte im Fließtext kurz zu erwähnen, gehört zum „guten Ton” bei der Auswertung einer Varianzanalyse und sollte daher nicht vergessen werden.\nNun können wir unsere ANOVA aufstellen. Es gibt verschiedene Möglichkeiten eine ANOVA zu berechnen, namentlich Type I, II und III. Die einzelnen Typen unterscheiden sich darin, wie die Parameter (insbesondere die Quadratsumme) berechnet wird. Typ I sollte vor allem für ausgeglichene Daten verwendet werden, also Daten bei der für jede Gruppe die gleiche Anzahl an Fällen vorliegen. Ist dies nicht der Fall, sollte Typ II oder Typ III (beispielsweise die Option der Anova in SPSS) verwendet werden. Der typische Befehl für eine Anova in R ist der Befehl aov(). Dieser ist jedoch nur für die Typ I Anova ausgelegt, daher nutzen wir hier erneut den Befehl aov_car() (sowie den distict-Befehl) aus dem afex-Paket welcher standardmäßig die Anova nach Typ III berechnet. Im Prinzip nutzen wir die selbe Syntax wie bei der unifaktoriellen Anova. Wir ergänzen allerdings unsere zweitere unabhängige Variable, beziehungsweise verbinden die beiden unabhängigen Variablen mit einem *. Dadurch erhalten wir sowohl die Werte für die einzelnen Variablen als auch für den Interaktionsterm, also das Zusammenspiel der Variablen.\n\nfit2 &lt;- daten %&gt;% \ndplyr::distinct(idno, .keep_all = TRUE) %&gt;% \n  afex::aov_car(happy ~ cntry * gndr + Error(idno),\n                                     data = ., anova_table = \"pes\")\n\nWarning: Missing values for 10 ID(s), which were removed before analysis:\n703, 785, 931, 1248, 2534, 3277, 10000220, 10001297, 16000091, 100002326, ... [showing first 10 only]\nBelow the first few rows (in wide format) of the removed cases with missing data.\n       idno cntry   gndr  .\n# 530   703    SE Female NA\n# 602   785    FR   Male NA\n# 719   931    SE Female NA\n# 959  1248    FR Female NA\n# 1800 2534    SE   Male NA\n# 2280 3277    SE   Male NA\n\n\nContrasts set to contr.sum for the following variables: cntry, gndr\n\nprint(fit2)\n\nAnova Table (Type 3 tests)\n\nResponse: happy\n      Effect      df  MSE         F   pes p.value\n1      cntry 3, 7638 3.13 44.82 ***  .017   &lt;.001\n2       gndr 1, 7638 3.13      0.01 &lt;.001    .910\n3 cntry:gndr 3, 7638 3.13    2.33 + &lt;.001    .072\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nDie Erläuterungen der einzelnen Parameter sind gleich zu den Erläuterungen der unifaktoriellen ANOVA, daher werden diese nicht wiederholt. Es zeigt sich, dass das Land erneut einen signifikanten Einfluss auf die Zufriedenheit hat (welcher im vorliegenden Modell etwas geringer mit 1.7 Prozent erklärter Varianz ausfällt). Keinen signfikanten Einfluss haben hingegen das Geschlecht der Befragten sowie das Zusammenspiel aus Geschlecht und Länderzugehörigkeit (p.value über .05).\nIm Anschluss müssen wir, wie in der univariaten ANOVA, die Posthoc-Tests berechnen (im vorliegenden Fall können wir lediglich den Tukey-Test berechnen, das der Tamhame-Test nur für einfaktorielle Designs funktioniert).\n\n7.1 Post-Hoc Tests\n\n#Tukey Post-Hoc Test\nemmeans::emmeans(fit2, specs = c(\"cntry\", \"gndr\")) %&gt;% \n  pairs()\n\n contrast              estimate     SE   df t.ratio p.value\n DE Male - FR Male       0.4100 0.0732 7638   5.600  &lt;.0001\n DE Male - GB Male       0.1073 0.0753 7638   1.425  0.8457\n DE Male - SE Male      -0.1191 0.1023 7638  -1.164  0.9419\n DE Male - DE Female    -0.0886 0.0664 7638  -1.335  0.8857\n DE Male - FR Female     0.5785 0.0699 7638   8.281  &lt;.0001\n DE Male - GB Female     0.0551 0.0704 7638   0.782  0.9940\n DE Male - SE Female    -0.1672 0.0997 7638  -1.678  0.7018\n FR Male - GB Male      -0.3027 0.0829 7638  -3.650  0.0064\n FR Male - SE Male      -0.5292 0.1080 7638  -4.898  &lt;.0001\n FR Male - DE Female    -0.4987 0.0749 7638  -6.653  &lt;.0001\n FR Male - FR Female     0.1684 0.0780 7638   2.158  0.3775\n FR Male - GB Female    -0.3550 0.0785 7638  -4.521  0.0002\n FR Male - SE Female    -0.5773 0.1056 7638  -5.468  &lt;.0001\n GB Male - SE Male      -0.2264 0.1094 7638  -2.069  0.4354\n GB Male - DE Female    -0.1959 0.0770 7638  -2.546  0.1765\n GB Male - FR Female     0.4711 0.0800 7638   5.892  &lt;.0001\n GB Male - GB Female    -0.0523 0.0804 7638  -0.650  0.9981\n GB Male - SE Female    -0.2746 0.1070 7638  -2.566  0.1688\n SE Male - DE Female     0.0305 0.1035 7638   0.295  1.0000\n SE Male - FR Female     0.6976 0.1058 7638   6.594  &lt;.0001\n SE Male - GB Female     0.1742 0.1061 7638   1.641  0.7253\n SE Male - SE Female    -0.0481 0.1275 7638  -0.378  0.9999\n DE Female - FR Female   0.6671 0.0717 7638   9.310  &lt;.0001\n DE Female - GB Female   0.1437 0.0722 7638   1.991  0.4883\n DE Female - SE Female  -0.0786 0.1010 7638  -0.779  0.9942\n FR Female - GB Female  -0.5234 0.0754 7638  -6.944  &lt;.0001\n FR Female - SE Female  -0.7457 0.1033 7638  -7.221  &lt;.0001\n GB Female - SE Female  -0.2223 0.1036 7638  -2.145  0.3858\n\nP value adjustment: tukey method for comparing a family of 8 estimates \n\n\n\n\n7.2 Interaktionen visualisieren\nUm einen möglichen Interaktionseffekt (auch wenn in unseren Fall der Interkationseffekt nicht signifikant war) auch anschaulich vermitteln bzw. oft auch verstehen zu können, empfiehlt es sich, diesen als Diagramm darzustellen. Hierzu nutzen wir die Pakete emmeans und ggplot2.\n\n#Interaktionsplot ausgeben\nemmip(fit2, cntry ~ gndr) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Geschlecht\")\n\n\n\nemmip(fit2, gndr ~ cntry) + \n  labs(title = \"Geschätzes Randmittel von Lebenszufriedenheit\", \n       y = \"Geschätzte Randmittel\",\n       x = \"Land\")"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests-mann-whitney-u-wilcoxon",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon",
    "text": "3.2 Exkurs nicht parametrische Tests: Mann-Whitney-U & Wilcoxon"
  },
  {
    "objectID": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "href": "Skript_6.3.html#exkurs-nicht-parametrische-tests",
    "title": "Die Bestimmung der Zentrale Tendenz mittels t-Tests",
    "section": "3.2 Exkurs: nicht parametrische Tests",
    "text": "3.2 Exkurs: nicht parametrische Tests\nMann-Whitney-U & Wilcoxon"
  }
]